\documentclass[12pt]{report}

% lacunae etc indicated by FIX
% places which need to be checked for current 
% truth are indicated by UPDATE

%\usepackage{latexsym}
%\usepackage{times}
% use the time fonts for better results with
% pdf (CM fonts don't index correctly)
%\usepackage[dvips]{hyperref}

%- comment out epsf box for TeXtures
\input epsf
%\newcommand{\epsfbox}[1]{}
%\def\epsfxsize{}
\input thedefs1
%\input thedefs

\setlength{\evensidemargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{9in}

\newcommand{\itsdb}{{\sf \lbrack incr tsdb()\rbrack}}

\newcommand{\lispcommand}[1]{\noindent\rm #1}%
\newcommand{\ldescnl}{\\}
\newenvironment{error}%
{\begin{quote}
\tt
}%
{\end{quote}
}
\newenvironment{warning}%
{\begin{quote}
\tt
}%
{\end{quote}
}

% commands for figures
\newcommand{\figtype}[1]{{\bf \strut #1}}
\newcommand{\figfeat}[1]{{\sc \strut #1}}
\newcommand{\nodedot}{\circle*{5}}

%\newcommand{\semnest}[1]{AVM omitted because of semantic nest size}
\newcommand{\semnest}[1]{#1}

\begin{document}
\chapter{Typed feature structures made simple}
\label{easy}


If you have worked through the previous chapter, you should have
gained an intuitive idea of the components of a typed feature structure
grammar, how to write feature structure descriptions for lexical entries
etc, and how grammar rules work.  In this chapter we will go over the
underlying concepts in more detail, to give you a more precise (though 
still relatively informal)
description of typed feature structures.\\
{\bf 
Cautionary note:}  there are several slightly different variants of
typed feature structure formalisms.  Here we are describing the 
version used in the LKB system, and since this is an informal
introduction, we will not discuss the differences with other approaches
in this chapter.  Some of these differences are mentioned in Chapter~\ref{formal},
which gives more formal definitions.  If you have some familiarity
with typed feature structure formalisms, you are strongly urged to 
read Chapter~\ref{formal} instead of this one.  In the case
of any apparent conflict between this chapter and Chapter~\ref{formal},
Chapter~\ref{formal} should be taken as definitive.  
The terminology in the literature is also confused, to say the least.  
Rather than trying to explain the alternatives in the main
text, we have given a glossary in \S\ref{glossary} (glossary
not finished yet).
% FIX
(This chapter is a first draft: comments are very welcome, especially
from members of the target audience.)

Some sections are followed by
exercises.  Please attempt them, at least those which aren't marked as 
optional.  They shouldn't take more than a few minutes 
and sometimes we will use them to introduce material which is 
referred to later on.

First we are going to introduce a really tiny grammar, both in order
to make the concept of encoding a grammar in feature structures clearer,
and also to give us concrete examples that can be used in the discussion
of the typed feature structures themselves.  


\section{A really really simple grammar}

We expect that readers have some familiarity with
grammars defined in terms of atomic symbols.  For instance,
Figure~\ref{smplatomic} shows a very simple grammar and lexicon:
\begin{figure}
{\small
\begin{minipage}[t]{3in}
\begin{verbatim}
Root symbol: S

Rules:

S -> NP VP
NP -> Det N
\end{verbatim}
\end{minipage}
\begin{minipage}[t]{3in}
\begin{verbatim}
Lexicon:

dog : N
dogs: N
this : Det
these: Det
sleeps : VP
sleep: VP

\end{verbatim}
\end{minipage}}
\caption{A tiny grammar encoded using atomic symbols}
\label{smplatomic}
\end{figure}
Note that the specification of the {\it root} 
symbol as {\tt S} means that the grammar will
accept as complete strings ``these dogs sleep'' and ``this dog sleeps'',
but not ``sleep'' and ``this dog'', because these cannot be assigned
the category S.

Grammars in general can be viewed abstractly as mappings between a 
set of descriptive structures and a set of
strings of characters.  
For a very simple grammar like
that in Figure~\ref{smplatomic}, 
we can use a labelled bracket notation as
the description.  For instance:
\begin{verbatim}
(S (NP (Det this) (N dog)) (VP sleeps))
\end{verbatim}
This labelled bracketing can equivalently 
be drawn as a tree, as shown in Figure~\ref{trivtree}:
\begin{figure}
\begin{center}
\setlength{\unitlength}{0.5in}
\begin{picture}(3,3.25)
\thicklines
\put(2,3){\makebox(0,0){\strut S}}
\put(1.9,2.9){\line(-1, -1){0.7}}
\put(2.1,2.9){\line(1, -1){0.7}}
%
\put(3,2){\makebox(0,0){\strut VP}}
%
\put(1,2){\makebox(0,0){\strut NP}}
\put(0.9,1.9){\line(-1, -1){0.7}}
\put(1.1,1.9){\line(1, -1){0.7}}
%
\put(0,1){\makebox(0,0){\strut Det}}
\put(2,1){\makebox(0,0){\strut N}}
%
\put(0,0.5){\makebox(0,0){\strut this}}
\put(2,0.5){\makebox(0,0){\strut dog}}
\put(3,0.5){\makebox(0,0){\strut sleeps}}
\end{picture}
\end{center}
\caption{Tree for {\it this dog sleeps} with the 
tiny grammar using atomic symbols}
\label{trivtree}
\end{figure}
Note that in the previous chapter we talked about a
tree being an abbreviation for a much larger structure,
but here the tree is a complete description of the structure,
since the grammar's symbols are all atomic.

Grammars can also be viewed as specifications that allow
parsing and generation of strings given partial information.
For instance, given the input string:
\begin{verbatim}
these dogs sleep
\end{verbatim}
we could derive the single labelled bracketing:
\begin{verbatim}
(S (NP (Det these) (N dogs)) (VP sleep))
\end{verbatim}
Given the partial bracketed structure:
\begin{verbatim}
(S (NP (Det ?) (N ?)) (VP sleeps))
\end{verbatim}
(where we are just using {\tt ?}
as a placeholder) the corresponding strings are:
\begin{verbatim}
this dog sleeps
these dog sleeps
this dogs sleeps
these dogs sleeps
\end{verbatim}

As this example shows, this grammar allows
sentences which are not grammatical English, such as:
\begin{ex}
these dog sleeps
\end{ex}
The problem is that the grammar does not include any specification of
agreement.  It could be fixed by increasing the number of atomic symbols,
for instance as shown in Figure~\ref{agratomic}.
\begin{figure}
{\small
\begin{minipage}[t]{3in}
\begin{verbatim}
Root symbol: S

Rules:

S -> NP-sg VP-sg
S -> NP-pl VP-pl
NP-sg -> Det-sg N-sg
NP-pl -> Det-pl N-pl

\end{verbatim}
\end{minipage}
\begin{minipage}[t]{3in}
\begin{verbatim}
Lexicon:

dog : N-sg
dogs: N-pl
this : Det-sg
these: Det-pl
sleeps : VP-sg
sleep: VP-pl
\end{verbatim}
\end{minipage}}
\caption{An atomic grammar encoding subject verb agreement}
\label{agratomic}
\end{figure}
But this approach
would soon become very tedious as we expanded the grammar.

Feature structures are one way of allowing extra information
to be encoded to
deal with agreement and more complex phenomena.  
Some approaches to representation use such feature structures
to augment a backbone of rules expressed using atomic symbols.
However, in other approaches, including that implemented in the
LKB, we instead make everything be a 
(typed) feature structure, including lexical entries
and grammar rules.  As we'll see, this also means that the
idea of a mapping between a string and its linguistic description
can actually be encoded within a feature structure.

Figure~\ref{lkbgram1} shows how a grammar equivalent to the very
simple grammar shown above can be written using
typed feature structures.
%
\begin{figure}
{\small
\begin{minipage}[t]{3in}
\begin{verbatim}
;;; Types
string := *top*.

*list* := *top*.

*ne-list* := *list* &
 [ FIRST *top*,
   REST *list* ].

*null* := *list*.

synsem-struc := *top* &
[ CATEGORY cat ].

cat := *top*.

s := cat.

np := cat.

vp := cat.

det := cat.

n := cat.

phrase := synsem-struc &
[ ARGS *list* ].

lexeme := synsem-struc &
[ ORTH string ].

root := phrase &
[ CATEGORY s ].
\end{verbatim}
\end{minipage}
\begin{minipage}[t]{3in}
\begin{verbatim}
;;; Lexicon
this := lexeme &
[ ORTH "this",
  CATEGORY det ].

these := lexeme &
[ ORTH "these",
  CATEGORY det ].

sleep := lexeme &
[ ORTH "sleep",
  CATEGORY vp ].

sleeps := lexeme &
[ ORTH "sleeps",
  CATEGORY vp ].

dog := lexeme &
[ ORTH "dog",
  CATEGORY n ].

dogs := lexeme &
[ ORTH "dogs",
  CATEGORY n ].

;;; Rules
s_rule := phrase &
[ CATEGORY s,
  ARGS [ FIRST [ CATEGORY np ],
         REST [ FIRST [ CATEGORY vp ],
                REST *null* ]]] .

np_rule := phrase &
[ CATEGORY np,
  ARGS [ FIRST [ CATEGORY det ],
         REST [ FIRST [ CATEGORY n ],
                REST *null* ]]] .
\end{verbatim}
\end{minipage}
}
\caption{Tiny typed feature structure grammar}
\label{lkbgram1}
\end{figure}
%
This is the grammar that is in the directory 
{\tt tiniest/grammar1} distributed with the LKB.
You should load this grammar into the LKB system in order to
experiment.
Figure~\ref{lkbgram1} looks much more complex than the grammar given 
in Figure~\ref{smplatomic}, but 
the 
additional initial complexity of feature structure grammars pays off 
with more complex grammars.\footnote{In fact this grammar is a bit
more complex than it needed to be 
to simply simulate the starting grammar in the LKB system, but we've
done this in order to be consistent with the other grammars distributed 
with the LKB, such as the toy grammar discussed in the previous
chapter.  As we'll see in a lot of detail later on,
although the LKB system can process any grammar that
can be expressed in this variant of the typed feature structure 
formalism, there are a few parameters which control the interface 
between the grammar and the system which have to be declared.  
For simplicity, we've kept most of those parameters the same in
the sample grammars supplied with the system.}
Please note that this isn't intended to be in any way a linguistically
interesting grammar
(unlike the toy grammar used in the last
chapter, which does have theoretically motivated fundamentals,
but which is somewhat too complex to use for our current
purposes).  

We will not go through the operation
of the grammar here, but rely on the intuitive correspondence
with the grammar in Figure~\ref{smplatomic}.
The details will emerge as we discuss the
formalism.
For now, you should just note that 
the basic components of the grammar are:
\begin{description}
\item[The type system]  The type system acts as the defining
framework for the rest of the grammar.  For instance,
it determines which structures
are mutually compatible and which features can occur, and it sets
up an inheritance system which allows generalisations to be 
expressed.  For this particular grammar, for example,
all the basic linguistic entities
are of type {\bf synsem-struc} which the type system defines
to have the feature {\sc category}.
The information which was conveyed by the simple atomic categories
in the grammar in Figure~\ref{smplatomic}
is kept in this feature.
The categories themselves, {\bf s}, {\bf np} and so
on, are also represented as types. 
Note that all types must be explicitly defined, except for
string types such as {\tt "these"}.
\item[Lexical entries]  These are typed feature structures
which the grammar writer constructs in order to encode the words
of the language.  Lexical entries define a relationship 
between a string representing the characters in a word
and some linguistic description of the word (or, more precisely,
of a particular sense of a word).
In this grammar,
the {\sc orth} (orthography)
feature is used to encode the string in lexical 
entries.  
\item[Grammar rules]
Grammar rules are typed feature structures that
describe how to combine lexical entries and phrases
to make further phrases.  In the atomic category grammar, we expressed 
the idea of a rule in a conventional way by specifying 
a category for a phrase
to the left of an arrow, and the daughter categories, in their
expected linear order, to the right of the arrow.
For this feature structure grammar
we have made the mother structure actually
include the daughters.  Thus, in this particular
grammar, the rules are actually partial descriptions
of phrases.  The daughters of the phrase are
contained in the feature {\sc args}.  This takes as its value a list
where the order of the list elements
corresponds
to the linear order of the daughters.  
Note that we will sometimes collectively refer to lexical entries
and grammar rules as {\it entries}.
\item[Root symbol]
The root symbol is represented by the type {\bf root}.
\end{description}

For this grammar, we could still derive the same labelled
bracketing for {\it this dog sleeps} as we saw before,
by simply taking the values of the category feature
from the lexical entries and from each phrase.
But a complete representation for the derivation is
given by the feature structure for the sentence, since
it contains all of the daughters.  This structure
is shown in Figure~\ref{phraseavm}.  
\begin{figure}
\semnest{
\begin{center}
{\tiny
$\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{s}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{lexeme}\\                                                
\attvaltyp{ORTH}{"this"}\\                                                      
\attvaltyp{CATEGORY}{det}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{lexeme}\\
\attvaltyp{ORTH}{"dog"}\\   
\attvaltyp{CATEGORY}{n}}}\\
\attvaltyp{REST}{*null*}}}}}}}\\
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{lexeme}\\
\attvaltyp{ORTH}{"sleeps"}\\                                                      
\attvaltyp{CATEGORY}{vp}}}\\
\attvaltyp{REST}{*null*}}}}}}$}
\end{center}}
\caption{AVM for {\it this dog sleeps} from
the tiny grammar using feature structures}
\label{phraseavm}
\end{figure}
Note that this
structure directly encodes the relationship between
the string and the linguistic representation.

In the next few sections, we will use examples from this grammar, and some
slightly more complex variants of it, in order to explain 
typed feature structures in detail.

\section{The type hierarchy}

Each node in a typed feature structure has a type and we'll start
off this detailed explanation by discussing {\it type system}s,
which form the backbone of grammars. 
A type system consists of a hierarchy of types, which
indicates specialisation, plus a set of constraints
on types which determined which typed feature structures are well-formed.
The constraints on types are themselves
typed feature structures. 
In the type hierarchy, there is a
unique most general type, which we will refer to as the top type,
{\bf *top*}.  
The way that
type files are written in the LKB
means that the
descriptions of  types contain a specification of their parents together
with the constraint (see Figure~\ref{lkbgram1}, where, for instance,
{\bf *ne-list*} is defined as having the parent {\bf *list*}).
It is the specification of the parents that defines the type hierarchy.
Figure~\ref{th}
shows the type hierarchy corresponding to our simple grammar.
Note that {\bf *top*} is not
defined in the grammar specification (the name ``*top*''
is actually set in the globals file).  
\begin{figure}
\begin{center}
\setlength{\unitlength}{0.95in}
\begin{picture}(6.5,1.7)(0,2.4) 
\thicklines
\put(3,4){\makebox(0,0){\figtype{*top*}}}
\put(2.9,3.9){\line(-2, -1){0.6}}
\put(3.1,3.9){\line(2, -1){0.6}}
\put(2.2,3.5){\makebox(0,0){\figtype{string}}}
\put(3.8,3.5){\makebox(0,0){\figtype{synsem-struc}}}
\put(2.8,3.9){\line(-5, -1){1.6}}
\put(3.2,3.9){\line(5, -1){1.6}}
\put(1,3.5){\makebox(0,0){\figtype{*list*}}}
\put(5,3.5){\makebox(0,0){\figtype{cat}}}
%
\put(4.9,3.4){\line(-1, -1){0.3}}
\put(4.5,3.0){\makebox(0,0){\figtype{s}}}
\put(5.0,3.4){\line(0, -1){0.3}}
\put(5,3.0){\makebox(0,0){\figtype{vp}}}
\put(5.1,3.4){\line(1, -1){0.3}}
\put(5.5,3.0){\makebox(0,0){\figtype{np}}}
\put(5.15,3.4){\line(5, -2){0.8}}
\put(6.0,3.0){\makebox(0,0){\figtype{n}}}
\put(5.2,3.4){\line(4, -1){1.2}}
\put(6.5,3.0){\makebox(0,0){\figtype{det}}}
%
\put(0.9,3.4){\line(-2, -1){0.6}}
\put(1.1,3.4){\line(2, -1){0.6}}
\put(0.2,3.0){\makebox(0,0){\figtype{*ne-list*}}}
\put(1.8,3.0){\makebox(0,0){\figtype{*null*}}}
%
\put(3.8,3.4){\line(0, -1){0.3}}
\put(3.8,3.0){\makebox(0,0){\figtype{phrase}}}
\put(3.8,2.9){\line(0, -1){0.3}}
\put(3.8,2.5){\makebox(0,0){\figtype{root}}}
\put(3.7,3.4){\line(-2, -1){0.6}}
\put(3,3.0){\makebox(0,0){\figtype{lexeme}}}
\end{picture}
\end{center}
\caption{Type hierarchy for the tiny grammar}
\label{th}
\end{figure}
Although this inheritance hierarchy is a tree, it is possible
for a type to have more than one parent: this is usually
referred to as {\it multiple inheritance} and we'll see some examples
of it shortly.

The type hierarchy must obey
the following properties:
\begin{enumerate}
\item There is a single hierarchy containing all the types.
\item There are no cycles in the hierarchy.  
\item Any two types in the hierarchy must either be incompatible,
in which case they will not share any descendants, or they must
have a unique highest common descendant (referred to as 
the unique {\it greatest lower bound}).
\end{enumerate}

In order to make the last
restriction a bit clearer, we will use an artificial example of
a hierarchy (Figure~\ref{th2}). 
\begin{figure}
\begin{center}
\setlength{\unitlength}{1in}
\begin{picture}(6,3.2)(0.7,1.9) 
\thicklines
\put(3,4){\makebox(0,0){\figtype{*top*}}}
\put(3,3.9){\line(0, -1){0.3}}
\put(3,3.5){\makebox(0,0){\figtype{animal}}}
\put(2.9,3.4){\line(-2, -1){0.6}}
\put(3.1,3.4){\line(2, -1){0.6}}
\put(2.2,3){\makebox(0,0){\figtype{swimmer}}}
\put(3.8,3){\makebox(0,0){\figtype{invertebrate}}}
\put(2.8,3.4){\line(-5, -1){1.6}}
\put(3.2,3.4){\line(5, -1){1.6}}
\put(1,3){\makebox(0,0){\figtype{flyer}}}
\put(5,3){\makebox(0,0){\figtype{vertebrate}}}
\put(3.7,2.9){\line(-5, -1){1.6}}
\put(1.1,2.9){\line(2, -1){0.6}}
\put(1.9,2.5){\makebox(0,0){\figtype{bee}}}
%
\put(2.3,2.9){\line(5, -1){1.6}}
\put(4.9,2.9){\line(-2, -1){0.6}}
\put(4.1,2.5){\makebox(0,0){\figtype{fish}}}
\put(4.0,2.4){\line(-1, -1){0.3}}
\put(3.6,2.0){\makebox(0,0){\figtype{cod}}}
\put(4.2,2.4){\line(1, -1){0.3}}
\put(4.7,2.0){\makebox(0,0){\figtype{guppy}}}
\end{picture}
\end{center}
\caption{Valid hierarchy with multiple inheritance}
\label{th2}
\end{figure}
This is a formally valid hierarchy, since if we look at any pair
of types, the relationship between them falls one of the
following categories:
\begin{enumerate}
\item The types have no descendents in common (e.g., 
{\bf vertebrate} and {\bf invertebrate})
\item The types have a hierarchical
relationship (e.g., {\bf animal} and {\bf bee}), in which case
the unique greatest common descendant is trivially the lower
in the hierarchy.
\item There is a third type which is
a unique greatest common 
descendant.  For instance, {\bf vertebrate} and {\bf swimmer}
have {\bf fish} as a common descendant: {\bf cod} and {\bf guppy}
are also common descendants, but {\bf fish} is above both of them
in the hierarchy.  
\end{enumerate}
Crucially, the assumption is made that 
all the types that exist are specified in the hierarchy (sometimes referred
to as a {\it closed world} assumption)\footnote{Strings
are an exception, see below.} 
and that, if two types are compatible, there must
be a single type which represents their combination.
So if we know something is of type 
{\bf vertebrate} and also of type {\bf swimmer},
given this hierarchy, we can conclude it is of type {\bf fish}
(though we don't know whether it is a {\bf cod} or a {\bf guppy}).

An example of an invalid hierarchy is given in Figure~\ref{th3},
where we have added {\bf mammal} under {\bf vertebrate} and
{\bf whale} as inheriting from {\bf mammal}
and {\bf swimmer}.  Now {\bf vertebrate} and {\bf swimmer} 
have multiple mutual descendants which don't stand 
in an inheritance relationship: {\bf fish} and {\bf whale}.
Note that it is irrelevant that the node {\bf mammal} intervenes
between {\bf vertebrate} and 
{\bf whale} while 
{\bf fish} is a direct daughter of {\bf vertebrate}: 
{\bf whale} is
not a descendant of {\bf fish}.  In fact the number of
nodes in a type hierarchy is never relevant formally.
\begin{figure}
\begin{center}
\setlength{\unitlength}{1in}
\begin{picture}(6,3.2)(0.7,1.9) 
\thicklines
\put(3,4){\makebox(0,0){\figtype{*top*}}}
\put(3,3.9){\line(0, -1){0.3}}
\put(3,3.5){\makebox(0,0){\figtype{animal}}}
\put(2.9,3.4){\line(-2, -1){0.6}}
\put(3.1,3.4){\line(2, -1){0.6}}
\put(2.2,3){\makebox(0,0){\figtype{swimmer}}}
\put(3.8,3){\makebox(0,0){\figtype{invertebrate}}}
\put(2.8,3.4){\line(-5, -1){1.6}}
\put(3.2,3.4){\line(5, -1){1.6}}
\put(1,3){\makebox(0,0){\figtype{flyer}}}
\put(5,3){\makebox(0,0){\figtype{vertebrate}}}
\put(3.7,2.9){\line(-5, -1){1.6}}
\put(1.1,2.9){\line(2, -1){0.6}}
\put(1.9,2.5){\makebox(0,0){\figtype{bee}}}
%
\put(2.3,2.9){\line(5, -1){1.6}}
\put(4.9,2.9){\line(-2, -1){0.6}}
\put(4.1,2.5){\makebox(0,0){\figtype{fish}}}
\put(4.0,2.4){\line(-1, -1){0.3}}
\put(3.6,2.0){\makebox(0,0){\figtype{cod}}}
\put(4.2,2.4){\line(1, -1){0.3}}
\put(4.7,2.0){\makebox(0,0){\figtype{guppy}}}
%
\put(5.1,2.9){\line(2, -1){0.6}}
\put(5.9,2.5){\makebox(0,0){\figtype{mammal}}}
\put(5.8,2.4){\line(-1, -1){0.3}}
\put(5.4,2.0){\makebox(0,0){\figtype{whale}}}
\put(6.0,2.4){\line(1, -1){0.3}}
\put(6.4,2.0){\makebox(0,0){\figtype{dog}}}
\put(2.2,2.9){\line(4, -1){3.1}}
\end{picture}
\end{center}
\caption{Invalid hierarchy}
\label{th3}
\end{figure}

This hierarchy can be fixed straightforwardly, if somewhat
uninterestingly, by adding a new node {\bf vertebrate-swimmer}.
This is shown in Figure~\ref{th4}.
\begin{figure}
\begin{center}
\setlength{\unitlength}{1in}
\begin{picture}(6,3.7)(0.7,1.4) 
\thicklines
\put(3,4){\makebox(0,0){\figtype{*top*}}}
\put(3,3.9){\line(0, -1){0.3}}
\put(3,3.5){\makebox(0,0){\figtype{animal}}}
\put(2.9,3.4){\line(-2, -1){0.6}}
\put(3.1,3.4){\line(2, -1){0.6}}
\put(2.2,3){\makebox(0,0){\figtype{swimmer}}}
\put(3.8,3){\makebox(0,0){\figtype{invertebrate}}}
\put(2.8,3.4){\line(-5, -1){1.6}}
\put(3.2,3.4){\line(5, -1){1.6}}
\put(1,3){\makebox(0,0){\figtype{flyer}}}
\put(5,3){\makebox(0,0){\figtype{vertebrate}}}
\put(3.7,2.9){\line(-5, -1){1.6}}
\put(1.1,2.9){\line(2, -1){0.6}}
\put(1.9,2.5){\makebox(0,0){\figtype{bee}}}
%
\put(2.3,2.9){\line(5, -1){1.6}}
\put(4.9,2.9){\line(-2, -1){0.6}}
\put(4.1,2.5){\makebox(0,0){\figtype{vertebrate-swimmer}}}
\put(4.1,2.4){\line(0, -1){0.3}}
\put(4.2,2.4){\line(4,-1){1.1}}
%
\put(5.1,2.9){\line(2, -1){0.6}}
\put(5.9,2.5){\makebox(0,0){\figtype{mammal}}}
\put(5.8,2.4){\line(-1, -1){0.3}}
\put(5.4,2.0){\makebox(0,0){\figtype{whale}}}
\put(6.0,2.4){\line(1, -1){0.3}}
\put(6.4,2.0){\makebox(0,0){\figtype{dog}}}
%
\put(4.1,2.0){\makebox(0,0){\figtype{fish}}}
\put(4.0,1.9){\line(-1, -1){0.3}}
\put(3.6,1.5){\makebox(0,0){\figtype{cod}}}
\put(4.2,1.9){\line(1, -1){0.3}}
\put(4.7,1.5){\makebox(0,0){\figtype{guppy}}}
\end{picture}
\end{center}
\caption{Corrected hierarchy}
\label{th4}
\end{figure}
In the LKB, if a type hierarchy does not conform to
the greatest lower bound condition, the system will create
types in order to satisfy the condition ({\it glbtypes}).
For more details of this process, see \ref{glbgen}.
However in the text from now on, we will assume that
all type hierarchies obey the glb condition.

As we mentioned above, strings such as {\tt "these"}
are an exception to the requirement
that all types have to be defined by the user.  Any arbitrary 
string (i.e. sequence of characters starting and ending with
{\tt "})
is conventionally considered to be a subtype of the
type {\bf string}.  No strings have subtypes, which
means that different strings are incompatible.

You should note that, in the LKB system, despite the requirement
that all types must be defined, it is valid to specify that
a type has exactly one daughter, as in the case of {\bf root}
which is the unique daughter of {\bf phrase} in the
sample grammar.  The system does not infer that something of type
{\bf phrase} is also of type {\bf root}.  This is one point of
fundamental difference between typed feature structure formalisms,
but it would be far too much of a digression to discuss it here.
The LKB system's form of the closed world assumption
just requires the grammar writer to always specify, 
for any pair of types, whether or
not they are compatible, and if they are compatible, what the result
of combining them is.  As we'll see below,
this requirement makes it straightforward to define combination of
typed feature structures, that is, {\it unification}.  

\subsection{Exercises}
\begin{enumerate}
\item Draw the hierarchy corresponding to 
the following type file (note that, in the description language
used to define types,
\verb+x := a & b.+ means
that {\bf x} has parents {\bf a} and {\bf b}):
\begin{verbatim}
a := *top*.
b := *top*.
x := a & b.
y := a & b & c.
z := a & b & c.
\end{verbatim}
What's wrong with it?  How could you fix it?
\item (Optional) Biological taxonomies classify organisms
according to categories such as phylum, class, order, family, genus, species and
so on.  In what respects are type hierarchies similar to and different
from biological taxonomies?  Could a representation of a taxonomy
be implemented using a type hierarchy?
\end{enumerate}

\subsection{Answers}
\begin{enumerate}
\item The problems are:
\begin{enumerate}
\item Lack of connectivity: {\bf c} is not defined.
\item Multiple greatest lower bounds
\end{enumerate}
In order to fix the lack of connectivity, we can define {\bf c} to
inherit from {\bf *top*}.  In order to fix the multiple
greatest lower bound problem, we have to introduce new types: a minimal
solution is:
\begin{verbatim}
a := *top*.
b := *top*.
c := *top*.
ab := a & b.
abc := ab & c.
x := ab.
y := abc.
z := abc.
\end{verbatim}
If you came up with another solution, and want to check it in the 
LKB system, replace the type file in the directory {\tt answers1}
with your own type file and load the script in that directory as usual.
Note that there is, in general, no unique way of adding types
to fix a greatest lower bound problem.
\item Taxonomies and type hierarchies both encode a notion
of specificity: for instance, the phylum {\it Chordata} contains
the classes {\it Mammalia} and {\it Reptilia} (among others).
They also share the property that the specificity relationship
is transitive and cannot be overridden: there is no way to say that, for instance
{\bf bird} is a subtype of {\bf vertebrate} and that
{\bf penguin} is a subtype of {\bf bird} but to cancel
the inference that {\bf penguin} is a subtype of {\bf vertebrate}.
But there is no cross-classification, so taxonomies are trees.
There are distinct levels of classification,
although there are some categories which may not be
instantiated for a particular organism, such as sub-species.
So a taxonomy could be partially represented in a type hierarchy:
the specialisation relationships could be captured, but
the categories of classification, such as genus,
cannot be represented, since
there is nothing in the formalism which allows one to
talk about the levels in the hierarchy, or to restrict the
hierarchy to a fixed number of levels.

There's also something of a difference with respect to the
notion of completeness: although organisms are discovered which
can, for example, be classified with respect to genus but not
to a particular species, a biologist is unlikely to be content to leave them
underspecified for long.  So it might be reasonable 
to assume that if there was only
one species in a given genus, that if an organism was classified as belonging
to that genus, it automatically also belonged to that species.
An LKB description of a taxonomy would be deficient in this respect.
\end{enumerate}

\section{Typed feature structures}

We now move on to talking about typed feature structures.
We'll make a distinction between
typed feature structures in general, which we'll discuss in this section,
and the subset of typed feature structures
which are {\it well-formed} with respect to a set of type constraints,
which are described in \S\ref{wf}.  

All typed feature structures can be thought of as graphs, which have exactly
one type on each node, and which have labelled arcs connecting nodes
(except for the case of the simplest feature structures, which
consist of a single node with a type but with no arcs). 
The labels are referred to as features.  Arcs are regarded as having
a direction, conventionally regarded as
pointing into the structure.  Any typed feature structure
must have a unique root node:  apart from the root, all nodes have
one or more parent nodes.  
Any node may have zero or more arcs leading out of it, but the
label on each must be unique.  No node may have an arc that points
back to one of its ancestors: i.e., there may be no cycles in the
structure.\footnote{Formally, in fact, both type hierarchies
and typed feature structures are {\it directed acyclic graphs} (DAGs).
However, directional arcs in typed
feature structures do not encode specificity in any way.
It would not make much sense for type hierarchies to contain cycles,
since intuitively it cannot be the case that x is more specific than
y and that y is also more specific than x.  But since the arcs in
typed feature structures don't have this sort of interpretation,
cycles are not intuitively ruled out, and some variants of the
typed feature structure formalism allow them.}
All the types on nodes must be in the type hierarchy, as discussed
in the previous section.
 
The graph shown in Figure~\ref{introfs}
represents the structure for the rule {\tt np\_rule}
from the grammar shown in Figure~\ref{lkbgram1}, where the first daughter
position ({\sc args first}) has been unified with the feature structure
corresponding to {\it these}.  
\begin{figure}
\begin{center}
\setlength{\unitlength}{0.3mm}
\begin{picture}(400,300)(0,0)
\thicklines
\put(0,310){\makebox(0,0){\figtype{phrase}}}
\put(0,300){\nodedot}
%
\put(50,290){\makebox(0,0){\figfeat{category}}}
\put(10,300){\vector(1,0){80}}
\put(100,310){\makebox(0,0){\figtype{np}}}
\put(100,300){\nodedot}
%
\put(10,290){\vector(1,-1){70}}
\put(60,250){\makebox(0,0)[l]{\figfeat{args}}}
\put(100,210){\makebox(0,0){\figtype{*ne-list*}}}
\put(100,200){\nodedot}
%
\put(150,190){\makebox(0,0){\figfeat{first}}}
\put(110,200){\vector(1,0){80}}
\put(200,210){\makebox(0,0){\figtype{lexeme}}}
\put(200,200){\nodedot}
%
\put(250,210){\makebox(0,0){\figfeat{orth}}}
\put(210,200){\vector(1,0){80}}
\put(300,210){\makebox(0,0){\figtype{"these"}}}
\put(300,200){\nodedot}
%
\put(250,180){\makebox(0,0)[l]{\figfeat{category}}}
\put(210,190){\vector(2,-1){80}}
\put(300,160){\makebox(0,0){\figtype{det}}}
\put(300,150){\nodedot}
%
\put(110,190){\vector(1,-1){70}}
\put(160,150){\makebox(0,0)[l]{\figfeat{rest}}}
\put(200,110){\makebox(0,0){\figtype{*ne-list*}}}
\put(200,100){\nodedot}
%
\put(250,90){\makebox(0,0){\figfeat{first}}}
\put(210,100){\vector(1,0){80}}
\put(300,110){\makebox(0,0){\figtype{synsem-struc}}}
\put(300,100){\nodedot}
%
\put(350,90){\makebox(0,0){\figfeat{category}}}
\put(310,100){\vector(1,0){80}}
\put(400,110){\makebox(0,0){\figtype{n}}}
\put(400,100){\nodedot}
%
\put(210,90){\vector(1,-1){70}}
\put(260,50){\makebox(0,0)[l]{\figfeat{rest}}}
\put(300,10){\makebox(0,0){\figtype{*null*}}}
\put(300,0){\nodedot}
\end{picture}
\end{center}
\caption{Graph representation of a feature structure}
\label{introfs}
\end{figure}
We will describe unification in detail below,
but intuitively it is just refers to combining two
typed feature structures, retaining all the information in each of them.

It is very important to realize that any non-root node
in a feature structure can be considered as the root
of another feature structure.  For instance, the following
is the feature structure rooted at the node reached by starting at the
root of the structure in Figure~\ref{introfs}
then following the arc labelled ARGS and then the arc labelled FIRST.
\begin{center}
\setlength{\unitlength}{0.3mm}
\begin{picture}(100,70)(200,150)
\thicklines
\put(200,210){\makebox(0,0){\figtype{lexeme}}}
\put(200,200){\nodedot}
%
\put(250,210){\makebox(0,0){\figfeat{orth}}}
\put(210,200){\vector(1,0){80}}
\put(300,210){\makebox(0,0){\figtype{"these"}}}
\put(300,200){\nodedot}
%
\put(250,180){\makebox(0,0)[l]{\figfeat{category}}}
\put(210,190){\vector(2,-1){80}}
\put(300,160){\makebox(0,0){\figtype{det}}}
\put(300,150){\nodedot}
\end{picture}
\end{center}

Because this graph notation is cumbersome (and difficult to draw in
LaTeX \ldots),
it is usual to illustrate feature structures with an alternative notation,
known as an {\it attribute value matrix} or {\sc avm}.  The {\sc avm}
corresponding to Figure~\ref{introfs} is shown in Figure~\ref{introavm}.
\begin{figure}
\begin{center}
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{lexeme}\\                                                
\attvaltyp{ORTH}{"these"}\\                                                      
\attvaltyp{CATEGORY}{det}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{n}}}\\
\attvaltyp{REST}{*null*}}}}}}$}
\end{center}
\caption{AVM representation of a feature structure}
\label{introavm}
\end{figure}
Note that the description language used in the grammar files is
similar to the AVM notation: a description which would correspond to
Figure~\ref{introfs} is shown in Figure~\ref{introdesc}.  
\begin{figure}
\begin{verbatim}
example := phrase &
[ CATEGORY np,
  ARGS  *ne-list* &
        [ FIRST lexeme &
                [ ORTH "these",
                  CATEGORY det ],
          REST  *ne-list* &
                [ FIRST synsem-struc &
                        [ CATEGORY n ],
                  REST *null* ]]] .
\end{verbatim}
\caption{Description language representation of a feature structure}
\label{introdesc}
\end{figure}
The main differences are that the AVM notation we're using
has the types inside the square brackets, while this description language
puts them outside, and that the description language requires
conjunction symbols \verb+&+.\footnote{It would clearly be better if the
description language mirrored the AVMs, but this is something 
we are stuck with for historical reasons, just like the QWERTY keyboard.}
Note that, although this is a perfectly valid description syntactically,
it's not usual to write something like this, both because
there's no reason generally to write a description that corresponds
to a partially instantiated rule, and because
it makes explicit a lot of 
information that is inferred automatically via the
process of making a structure well-formed, as we'll discuss in \S\ref{wf}.

There is no significance to the order in which features are
drawn.  The following two AVMs represent exactly the same structure.
\begin{center}
{\tiny
$\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attvaltyp{FIRST}{lexeme}\\                                                
\attvaltyp{REST}{*null*}}}}$}

{\tiny
$\avmplus{\att{phrase}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\                                         
\attvaltyp{REST}{*null*}\\
\attvaltyp{FIRST}{lexeme}}}\\       
\attvaltyp{CATEGORY}{np}}$}
\end{center}
However, it is obviously easier to read AVMs if the features
appear in a consistent order (the way the LKB allows the user to define
display order is discussed in \S~\ref{fdisorder}).

It is often useful to talk about {\it path}s into feature
structures, by which
we mean sequences of features that
can be followed from the root node.  In the example above, for instance,
the path CATEGORY.ARGS leads to a node with the type {\bf *ne-list*}.  
The {\it value} of
a path is the feature structure whose root is the node led to by the
path.  
The root node is the value of the empty path.  
We will sometimes use this terminology loosely however,
and say that the value of a path is a type, when strictly speaking
what we mean is that the value of the path is a feature structure
with a root node labelled with that type.

The graph in Figure~\ref{introfs} is a tree: there are no
cases where two arcs point to the same node.  But {\it reentrancy},
as the non-tree situation is called, is required to build up more interesting 
feature structure grammars.
Figure~\ref{reentrancy} 
illustrates two feature structures: the first is not reentrant, while the
second is.  We use structures with features like F and
G at this point, to abstract away from the details of encoding in
the grammar.  
(We assume that the 
types {\bf t}, {\bf a}, {\bf b} and so on are all defined in some hierarchy.)
\begin{figure}
\setlength{\unitlength}{0.55mm}
\begin{tabular}{cccc}
& Graph & AVM & description\\
\begin{tabular}{l}
Non-reentrant
\end{tabular} & 
\begin{picture}(95,40)
\thicklines
\put(96,35){\makebox(0,0)[l]{{\bf a}}}
\put(88,35){\circle*{5}}
\put(40,30){\makebox(0,0)[l]{{\tt F}}}
\put(11,18){\vector(4,1){70}}
\put(10,6){\oval(20,23)[tl]}
\put(0,0){\circle*{5}}
\put(0,-10){\makebox(0,0){{\bf t}}}
\put(40,6){\makebox(0,0)[l]{{\tt G}}}
\put(6,0){\vector(4,0){75}}
\put(88,0){\circle*{5}}
\put(96,0){\makebox(0,0)[l]{{\bf a}}}
\end{picture}
&
{\tiny $\avmplus{\att{t}\\
\attvaltyp{F}{a} \\
\attvaltyp{G}{a}}$}
&
\begin{minipage}[t]{1in}
\begin{verbatim}
t &
[ F a,
  G a ].
\end{verbatim}
\end{minipage}
\\
\begin{tabular}{l}
Reentrant
\end{tabular} &
\begin{picture}(95,30)
\thicklines
\put(40,20){\makebox(0,0)[l]{{\tt F}}}
\put(11,18){\vector(4,-1){70}}
\put(10,6){\oval(20,23)[tl]}
\put(0,0){\circle*{5}}
\put(0,-10){\makebox(0,0){{\bf t}}}
\put(40,6){\makebox(0,0)[l]{{\tt G}}}
\put(6,0){\vector(4,0){75}}
\put(88,0){\circle*{5}}
\put(96,0){\makebox(0,0)[l]{{\bf a}}}
\end{picture}
&
{\tiny $\avmplus{\att{t}\\
\attval{F}{\ind{0}\ \ \myvaluebold{a}}\\
\attval{G}{\ind{0}}}$}
&
\begin{minipage}[t]{1in}
\begin{verbatim}
t &
[ F #1 & a,
  G #1 ].
\end{verbatim}
\end{minipage}
\end{tabular}
\caption{Reentrant structures}
\label{reentrancy}
\end{figure}

Note that reentrancy is conventionally indicated by boxed integers
in the AVM diagrams, and by tags beginning with \verb+#+
in the descriptions.  The particular integer or tag
used is of no significance: their only function is to indicate
that the node is the same.  We should also note at this point that
it is conventional to distinguish between features and types
typographically by putting the former in uppercase, and the latter 
in lowercase.  Although the case isn't significant, there is nothing
to prevent the same name being used for a type and for a feature,
since the syntax of the descriptions always allows them to be distinguished.

To see reentrancy at work in a grammar, look at the grammar shown
in Figure~\ref{lkbgram2}, where we have augmented the grammar in
Figure~\ref{lkbgram1} with
information about agreement.  Try parsing the sentences {\it this dog sleeps},
{\it these dog sleeps} and so on in the LKB system (the grammar is in
{\tt tiniest/grammar2}) and look at the trees which
result from the sentences which are admitted.
The reentrancies which are specified 
on the rules ensure:
\begin{enumerate}
\item that the information about agreement in the lexical entries
is passed up to the phrases
\item that the determiner and noun have compatible agreement specifications
in the rule {\tt np\_rule} and that the noun phrase and the verb phrase
have compatible agreement specifications
in the rule {\tt s\_rule}
\end{enumerate}
However, we can't fully describe this here, because
haven't discussed unification yet, so we
will return to the details of how this works later.
\begin{figure}
{\small
\begin{minipage}[t]{3in}
\begin{verbatim}
;;; Types
string := *top*.

*list* := *top*.

*ne-list* := *list* &
 [ FIRST *top*,
   REST *list* ].

*null* := *list*.

synsem-struc := *top* &
[ CATEGORY cat,
  NUMAGR agr ].

cat := *top*.

s := cat.

np := cat.

vp := cat.

det := cat.

n := cat.

agr := *top*.

sg := agr.

pl := agr.

phrase := synsem-struc &
[ ARGS *list* ].

lexeme := synsem-struc &
[ ORTH string ].

sg-lexeme := lexeme &
[ NUMAGR sg ].

pl-lexeme := lexeme &
[ NUMAGR pl ].

root := phrase &
[ CATEGORY s ].
\end{verbatim}
\end{minipage}
\begin{minipage}[t]{3in}
\begin{verbatim}
;;; Lexicon
this := sg-lexeme &
[ ORTH "this",
  CATEGORY det ].

these := pl-lexeme &
[ ORTH "these",
  CATEGORY det ].

sleep := pl-lexeme &
[ ORTH "sleep",
  CATEGORY vp ].

sleeps := sg-lexeme &
[ ORTH "sleeps",
  CATEGORY vp ].

dog := sg-lexeme &
[ ORTH "dog",
  CATEGORY n ].

dogs := pl-lexeme &
[ ORTH "dogs",
  CATEGORY n ].

;;; Rules
s_rule := phrase &
[ CATEGORY s,
  NUMAGR #1,
  ARGS [ FIRST [ CATEGORY np,
                 NUMAGR #1 ],
         REST [ FIRST [ CATEGORY vp,
                        NUMAGR #1 ],
                REST *null* ]]] .

np_rule := phrase &
[ CATEGORY np,
  NUMAGR #1,
  ARGS [ FIRST [ CATEGORY det,
                 NUMAGR #1 ],
         REST [ FIRST [ CATEGORY n,
                        NUMAGR #1 ],
                REST *null* ]]] .
\end{verbatim}
\end{minipage}
}
\caption{Tiny grammar with number agreement}
\label{lkbgram2}
\end{figure}
%

\subsection{Exercises}
\begin{enumerate}
\item Which of the following are valid typed feature structures
(assuming all the types used are defined)?
\begin{enumerate}
\item {\tiny $\avmplus{\att{det}}$}
\item {\tiny $\avmplus{\att{t}\\
\attval{F}{\ind{0}}\\
\attval{G}{\ind{0}}}$}
\item {\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\                                               
\attvaltyp{ORTH}{"these"}\\                                                      
\attvaltyp{CATEGORY}{cat}}$}
\item {\tiny $\avmplus{\att{t}\\
\attval{F}{\ind{0} \avmplus{\att{u}\\
\attval{F}{\ind{1}  \myvaluebold{a}}\\
\attval{H}{\ind{1}}}}\\
\attval{G}{\ind{1}}\\
\attval{J}{\ind{0}}}$}
\item {\tiny $\avmplus{\att{t}\\
\attval{F}{\ind{0} \avmplus{\att{u}\\
\attval{F}{\ind{1} \myvaluebold{a}}\\
\attval{H}{\ind{0}}}}\\
\attval{G}{\ind{1}}\\
\attval{J}{\ind{0}}}$}
\end{enumerate}
\item Draw the graph and the AVM for the structure for 
{\it this dog sleeps} in the version of the grammar with agreement.
\item List all the valid paths in the following structure,
and the types of the corresponding nodes.
Also list all pairs of paths which lead to the same node.
\begin{center}
{\tiny $\avmplus{\att{t}\\
\attval{F}{\ind{0} \avmplus{\att{u}\\
\attval{F}{\ind{1}  \myvaluebold{a}}\\
\attval{H}{\ind{1}}}}\\
\attval{G}{\ind{1}}\\
\attval{J}{\ind{0}}}$}
\end{center}
\end{enumerate}

\subsection{Answers}
\begin{enumerate}
\item 
\begin{enumerate}
\item Valid.  There are no arcs, but a feature structure
need only have a single node.
\item Valid, but only because
of a notational convention which we haven't told
you about.  The structure looks invalid, since here is
no explicit type associated with the inner node, and a type
must be associated with every node in a valid
structure.  But there is a convention
when writing AVMs that it is OK to omit the type on a node
when showing a reentrant structure if the type is {\bf *top*}.
So this structure is just shorthand for:\\
{\tiny $\avmplus{\att{t}\\
\attval{F}{\ind{0} \myvaluebold{*top*}}\\
\attval{G}{\ind{0}}}$}\\
(In fact, later on we will omit the type 
on reentrant or non-terminal nodes when it
can be inferred from the type constraints.)
\item Invalid.  The structure has two features {\sc category}
coming from the same node.
\item Valid. The reentrancy is fairly complex, but there are no cycles.
Note that it is OK to have multiple cases of the feature F
since they do not come from the same node.
\item Invalid. There is a cycle in the structure:
the path F.H leads to the same node as the path F does.
\end{enumerate}
\item Load the grammar in {\tt tiniest/grammar2} into the LKB, parse {\it the dog
sleeps} and check the AVM against your answer.
The graph should have a single node in every place where the AVM
shows a boxed integer.  
If you created 
the AVM using the LKB in the first place, award yourself
one bonus point \ldots  
\item The decomposition of the structure looks something like this:\\
\begin{tabular}{ll}
path & type\\ \hline
empty path   & t\\
F            & u\\ 
F.F          & a\\
F.H           & a\\
G     &     a\\
J     &     u
\end{tabular}\\[0.1in]
\begin{tabular}{ll}
path & path\\ \hline
F  &  G\\
F.F &  F.H\\
F.F & G\\
F.H & G
\end{tabular}

One can regard these pairings as comprising the individual 
pieces of information that are encapsulated in the single structure.
Note that each of these pairs can be represented as a single
feature structure, as shown below (we have omitted the type {\bf *top}
from the nodes):\\
\begin{tabular}{l}
{\tiny $\avmplus{\att{t}}$}\\
{\tiny $\avmplus{\attvaltyp{F}{u}}$}\\
{\tiny $\avmplus{\attval{F}{\avmplus{\attvaltyp{F}{a}}}}$}\\
{\tiny $\avmplus{\attval{F}{\avmplus{\attvaltyp{H}{a}}}}$}\\
{\tiny $\avmplus{\attvaltyp{G}{a}}$}\\
{\tiny $\avmplus{\attvaltyp{J}{u}}$}\\
{\tiny $\avmplus{\attval{F}{\ind{1}}\\
\attval{G}{\ind{1}}}$}\\
{\tiny $\avmplus{\attval{F}{\avmplus{\attval{F}{\ind{1}}\\
\attval{H}{\ind{1}}}}}$}\\
{\tiny $\avmplus{\attval{F}{\avmplus{\attval{F}{\ind{1}}}}\\
\attval{G}{\ind{1}}}$}\\
{\tiny $\avmplus{\attval{F}{\avmplus{\attval{H}{\ind{1}}}}\\
\attval{G}{\ind{1}}}$}
\end{tabular}\\
When we come to talk about unification in the next section,
it will be useful to think in terms of such individual
pieces of information and the formalisation in Chapter~\ref{formal}
also makes use of this idea.
\end{enumerate}


\section{Unification}

Unification is the combination of two feature
structures, retaining all the information which they contain.
Unification of typed feature structures is always 
defined with respect to a particular type hierarchy,
since if one structure specifies that a node
has a type {\bf a}, and the other that the node is of
type {\bf b}, the structures will only unify
if {\bf a} and {\bf b} are compatible types.
If they are compatible, the node in the result will have 
the type which is the glb of {\bf a}
and {\bf b}.
The result of a successful unification is always
a single feature structure.
If the the information in
two feature structures cannot be combined to form a single
valid feature structure,
unification is said to fail.  In terms of the
decomposition of feature structures described in the last
exercise, we can think of unification as a process
of conjoining the sets of path-type and path-path 
equivalences from each of the structures to be unified
and trying to form the resulting set of structures
into a single feature structure. 

The symbol we will use for
unification is $\unify$.  For instance, assuming the type
hierarchy from Figure~\ref{th}:
\begin{ex}
{\tiny $\avmplus{\att{lexeme}\\                                             
\attvaltyp{ORTH}{"these"}}$} $\unify$
{\tiny $\avmplus{\att{lexeme}\\
\attvaltyp{CATEGORY}{np}}$}
$=$
{\tiny $\avmplus{\att{lexeme}\\
\attvaltyp{ORTH}{"these"}\\
\attvaltyp{CATEGORY}{np}}$}
\end{ex}
The root nodes of the two structures being
unified always correspond to the root node of the result
but arcs with different features always give distinct
arcs in the result.
Constrast this example with:
\begin{ex}
{\tiny $\avmplus{\att{lexeme}\\ 
\attvaltyp{ORTH}{"these"}\\                                            
\attvaltyp{CATEGORY}{*top*}}$} $\unify$
{\tiny $\avmplus{\att{lexeme}\\
\attvaltyp{CATEGORY}{np}}$}
$=$
{\tiny $\avmplus{\att{lexeme}\\                                              
\attvaltyp{ORTH}{"these"}\\
\attvaltyp{CATEGORY}{np}}$}
\end{ex}
Here both the feature structures being unified have an
arc labelled {\sc category} and this must give a single
arc in the result (since feature structures may 
only have one arc with a given feature from any node).
The first structure says that the value of {\sc category}
is {\bf *top*}, the second that it is {\bf np}, but
since these types are consistent, the type on the
node in the result is simply their greatest lower bound:
that is {\bf np}.

Unification, like other mathematical operations, can be
regarded procedurally or statically.  For instance, 
one can equivalently say `the result of adding 2 and 3 
is 5' which suggests a procedure, or `5 is the sum of 
2 plus 3', or $5=2+3$.   The description of unification
given above was somewhat
procedural, because we talked about `success' and `failure'.
To allow us to use a non-procedural description
and to write equations, it
is useful to introduce a 
symbol that stands for inconsistency, $\bot$ 
(bottom).
For instance, the unification of the following two structures is
$\bot$.
\begin{ex}
{\tiny $\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{vp}}$} $\unify$
{\tiny $\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{np}}$}
$=$
$\bot$
\end{ex}
The inconsistency arise because of the inconsistent types for the
path {\sc category} (i.e., {\bf np} and {\bf vp} don't have a glb).

We now use some further examples to illustrate unification in more
detail.  

\subsection{Examples of unification}

\begin{ex}
\begin{tabular}{lll}
{\tiny $\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{synsem-struc}\\                                                                                                      
\attvaltyp{CATEGORY}{*top*}}}}}}$} &
$\unify$ &
{\tiny $\avmplus{\att{phrase}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{phrase}\\                                                                                                      
\attvaltyp{CATEGORY}{vp}}}\\
\attvaltyp{REST}{lexeme}}}}$}
\\
&& $=$
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{phrase}\\                                                                                                      
\attvaltyp{CATEGORY}{vp}}}\\
\attvaltyp{REST}{lexeme}}}}$}
\end{tabular}
\end{ex}
In this example, we have to consider paths of length greater
than one, but unification of the substructures works
in exactly the same way.  For instance, the root node
of the result has the type {\bf phrase} because {\bf phrase}
is the glb of {\bf phrase} and {\bf synsem-struc}.
Similarly, the node at the end of the path ARGS.FIRST
also has the type {\bf phrase}.

\begin{ex}
\begin{tabular}{lll}
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{synsem-struc}\\                                                                                                      
\attvaltyp{CATEGORY}{det}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{n}}}\\
\attvaltyp{REST}{*null*}}}}}}$}
&
$\unify$
&
{\tiny $\avmplus{\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{lexeme}\\  
\attvaltyp{ORTH}{"dogs"}\\                                                                                                   
\attvaltyp{CATEGORY}{n}}}}}}$}
$=$
$\bot$
\end{tabular}
\end{ex}
Here unification fails because the value
of the path ARGS.FIRST.CATEGORY is {\bf det} in the first
structure and {\bf n} in the second.  These types are
not compatible, so unification fails.

\begin{ex}
\begin{tabular}{lll}
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{synsem-struc}\\                                                                                                      
\attvaltyp{CATEGORY}{det}}}}}}$}
&
$\unify$
&
{\tiny $\avmplus{\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{synsem-struc}\\                                                                                                      
\attvaltyp{CATEGORY}{cat}}}}}}$}\\
&&
$=$
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{synsem-struc}\\                                                                                                      
\attvaltyp{CATEGORY}{det}}}}}}$}
\end{tabular}
\end{ex}
The point about this example is to illustrate
that in some cases, the result of unification is equal to
one of the inputs.  This will be true just in case
that input structure already contains
all the information in the other input structure.
The structure with less information is said to
more general, or, formally, to
{\it subsume} the more informative one.  
The most general feature structure of all is 
{\tiny $\avmplus{\att{*top*}}$}: the result
of unifying this with an arbitrary feature structure F
will always be F.  Note also that if two identical
feature structures G are unified, the result will be G.

\begin{ex}
\begin{tabular}{lll}
{\tiny $\avmplus{\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{lexeme}\\  
\attvaltyp{ORTH}{"these"}\\                                                                                                   
\attvaltyp{CATEGORY}{det}}}}}}$}
&
$\unify$
&
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{synsem-struc}\\                                                                                                      
\attvaltyp{CATEGORY}{det}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{n}}}\\
\attvaltyp{REST}{*null*}}}}}}$}
\\
&&
$=$
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{lexeme}\\                                                
\attvaltyp{ORTH}{"these"}\\                                                      
\attvaltyp{CATEGORY}{det}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{n}}}\\
\attvaltyp{REST}{*null*}}}}}}$}
\end{tabular}
\end{ex}
The result in this example is the feature structure we showed in
\ref{introavm}.  This is a partial structure which
might be created in the course of parsing with the tiniest grammar,
because it illustrates a partial application of the np rule
to the lexical entry for {\it these}. 

\begin{ex}
\begin{tabular}{lll}
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{lexeme}\\                                                
\attvaltyp{ORTH}{"these"}\\                                                      
\attvaltyp{CATEGORY}{det}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{n}}}\\
\attvaltyp{REST}{*null*}}}}}}$}
&
$\unify$
&
{\tiny $\avmplus{\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{lexeme}\\
\attvaltyp{ORTH}{"dogs"}\\
\attvaltyp{CATEGORY}{n}}}}}}}}$}\\
&
$=$
&
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{lexeme}\\                                                
\attvaltyp{ORTH}{"these"}\\                                                      
\attvaltyp{CATEGORY}{det}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{lexeme}\\
\attvaltyp{ORTH}{"dogs"}\\
\attvaltyp{CATEGORY}{n}}}\\
\attvaltyp{REST}{*null*}}}}}}$}
\end{tabular}
\end{ex}
Here we have taken the result from the previous example 
and unified it with a structure representing a second
daughter for the rule, corresponding
to the lexical entry for {\it dogs}.
The result is a structure which is a complete representation of
the phrase {\it these dogs}.  At this point we must emphasize
a crucial property of unification, which is that the result
is independent of the order in which we combine the structures
(i.e., more formally, unification is commutative and transitive).
For instance, if we take the two daughter structures first,
unify them together, and then unify them with the rule structure,
the result will be identical to that above.
\begin{ex}
\begin{tabular}{lll}
({\tiny $\avmplus{\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{lexeme}\\  
\attvaltyp{ORTH}{"these"}\\                                                                                                   
\attvaltyp{CATEGORY}{det}}}}}}$}
&
$\unify$
&
{\tiny $\avmplus{\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{lexeme}\\
\attvaltyp{ORTH}{"dogs"}\\
\attvaltyp{CATEGORY}{n}}}}}}}}$})\\
$\unify$
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{synsem-struc}\\                                                                                                      
\attvaltyp{CATEGORY}{det}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{n}}}\\
\attvaltyp{REST}{*null*}}}}}}$}\\
&
$=$
&
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{lexeme}\\                                                
\attvaltyp{ORTH}{"these"}\\                                                      
\attvaltyp{CATEGORY}{det}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{lexeme}\\
\attvaltyp{ORTH}{"dogs"}\\
\attvaltyp{CATEGORY}{n}}}\\
\attvaltyp{REST}{*null*}}}}}}$}
\end{tabular}
\end{ex}
The effect of this is that we can guarantee that different
parsing strategies will always give the same result with a
typed feature structure grammar (if the parsing 
process ever terminates and does not run into an infinite
loop). However some strategies may be more efficient than others,
since if unifying a set of structures results in
failure, the process of determining this is quicker if
the failures occur as one of the early unification steps.

\begin{ex}
\begin{tabular}{lllll}
{\tiny $\avmplus{\att{t}\\
             \attval{F}{\ind{1} \myvaluebold{*top*}}\\
             \attval{G}{\ind{1}}}$}
&
$\unify$ 
&
{\tiny $\avmplus{\att{t}\\
             \attval{F}{\avmplus{\att{u}\\
                        \attvaltyp{J}{a}}}\\
             \attval{G}{\avmplus{\att{u}\\
                        \attvaltyp{J}{*top*}\\
                        \attvaltyp{K}{b}}}}$}
&
$=$
&
{\tiny $\avmplus{\att{t}\\
             \attval{F}{\ind{0} {\avmplus{\att{u}\\
                        \attvaltyp{J}{a}\\
                        \attvaltyp{K}{b}}}}\\
             \attval{G}{\ind{0}}}$}
\end{tabular}
\end{ex}
This example involves reentrancy.  (Again, we use
abstract feature structures for simplicity
and assume a flat type hierarchy,
where {\bf t}, {\bf u}, {\bf a} and {\bf b} are all
incompatible daughters of {\bf *top*}.)  Note that
the information on the reentrant node in the result
all comes from the second structure: effectively the
result of unification has been to combine two nodes
which were distinct in the input structure.

\begin{ex}
\begin{tabular}{lllll}
{\tiny $\avmplus{\att{t}\\
             \attval{F}{\ind{1} \myvaluebold{*top*}}\\
             \attval{G}{\ind{1}}}$}
&
$\unify$ 
&
{\tiny $\avmplus{\att{t}\\
             \attval{F}{\avmplus{\att{u}\\
                        \attvaltyp{J}{a}}}\\
             \attval{G}{\avmplus{\att{u}\\
                        \attvaltyp{J}{b}\\
                        \attvaltyp{K}{b}}}}$}
&
$=$
&
$\bot$
\end{tabular}
\end{ex}
Here we modified the example above slightly, so that the
value on G.J is {\bf b} instead of {\bf a}.  Since {\bf a}
and {\bf b} are incompatible, unification fails.

\begin{ex}
\begin{tabular}{lllll}
{\tiny $\avmplus{\att{t}\\
             \attval{F}{\avmplus{\att{u}\\
                                 \attval{G}{\ind{1}}}}\\
             \attval{H}{\ind{1}}}$}
&
$\unify$ 
&
{\tiny $\avmplus{\att{t}\\
             \attval{F}{\ind{1}}\\
             \attval{H}{\ind{1}}}$}
&
$=$
&
$\bot$
\end{tabular}
\end{ex}
This unification results in failure because the result would be
a cyclic structure.  Note that cyclic structures are the
only cases where unification failure can occur without
incompatible types.

\subsection{Exercises}
\label{unifex}

These exercises involve more examples of unification.
You could try using the LKB system to investigate
unification, by specifying entries,
viewing the feature structures, and unifying the results
according to the instructions in \S\ref{unifcheck}.
Be warned however
that if you do this, you will probably find that some of the
entries you specify get expanded with additional information
because of the well-formedness constraints.  For instance,
if you write the following description:
\begin{verbatim}
ex1 := lexeme &
[ CATEGORY np ].
\end{verbatim}
the structure displayed will be 
\begin{center}
{\tiny $\avmplus{\att{lexeme}\\
\attvaltyp{ORTH}{string}\\
\attvaltyp{CATEGORY}{np}}$}
\end{center}
We will explain how this happens in detail in the next section.

\begin{enumerate}
\item Give the results of the following unifications, assuming that the types
{\bf t}, {\bf u}, {\bf a} and {\bf b} are incompatible daughters of
{\bf *top*}.
\begin{enumerate}
\item {\tiny $\avmplus{\att{t}\\
             \attval{F}{\ind{1}\ \ \myvaluebold{a}}\\
             \attval{G}{\ind{1}}}$}
$\unify$ {\tiny $\avmplus{\att{t}\\
             \attvaltyp{G}{b}}$}
\item {\tiny $\avmplus{\att{t}\\
             \attval{F}{\ind{1}}\\
             \attval{G}{\ind{1}}\\
             \attval{H}{\ind{2}}\\
             \attval{J}{\ind{2}}}$}
$\unify$ {\tiny $\avmplus{\att{t}\\
             \attval{F}{\ind{1}}\\
             \attval{J}{\ind{1}}}$}
\item {\tiny $\avmplus{\att{t}\\
             \attval{F}{\avmplus{\att{u}\\
                                 \attval{G}{\ind{1}}}}\\
             \attval{H}{\ind{1}}}$}
$\unify$ {\tiny $\avmplus{\att{t}\\
             \attval{F}{\ind{2}}\\
             \attval{H}{\avmplus{\att{u}\\
                                 \attval{J}{\ind{2}}}}}$}
\end{enumerate}
\item Assume the type hierarchy for given for the
grammar with agreement shown in Figure~\ref{lkbgram2}.
\begin{enumerate}
\item  What is the result of the following:\\
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{NUMAGR}{\ind{1} \myvaluebold{agr}}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{synsem-struc}\\                                                                                                      
\attvaltyp{CATEGORY}{det}\\
\attval{NUMAGR}{\ind{1}}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{n}\\
\attval{NUMAGR}{\ind{1}}}}\\
\attvaltyp{REST}{*null*}}}}}}$}
$\unify$
{\tiny $\avmplus{\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{pl-lexeme}\\  
\attvaltyp{ORTH}{"these"}\\                                                                                                   
\attvaltyp{CATEGORY}{det}\\
\attvaltyp{NUMAGR}{pl}}}}}}$}
\item What happens when you unify the result obtained above with the
following structure?\\
{\tiny $\avmplus{\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{pl-lexeme}\\
\attvaltyp{ORTH}{"dogs"}\\
\attvaltyp{CATEGORY}{n}\\
\attvaltyp{NUMAGR}{pl}}}}}}}}$}
\item What about this one?\\
{\tiny $\avmplus{\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{lexeme}\\
\attvaltyp{ORTH}{"dogs"}\\
\attvaltyp{CATEGORY}{n}\\
\attvaltyp{NUMAGR}{agr}}}}}}}}$}
\item And this?\\
{\tiny $\avmplus{\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{sg-lexeme}\\
\attvaltyp{ORTH}{"dog"}\\
\attvaltyp{CATEGORY}{n}\\
\attvaltyp{NUMAGR}{sg}}}}}}}}$}
\end{enumerate}
\item
Assume that {\bf t} is an immediate daughter of {\bf *top*}.
Consider the following structure:
\begin{center}
{\tiny $\avmplus{\att{t}\\
             \attval{F}{\ind{1} \myvaluebold{*top*}}\\
             \attval{G}{\ind{1}}}$}
\end{center}
How many structures are there which subsume it?
Draw these structures in a subsumption hierarchy, with the most general
structure at the top, the intermediate structures arranged 
in order of generality and the full structure at the bottom.

(Optional) Suppose we are told that 
\begin{center}
$FS1 \unify FS2 =$ {\tiny $\avmplus{\att{t}\\
             \attval{F}{\ind{1} \myvaluebold{*top*}}\\
             \attval{G}{\ind{1}}}$}
\end{center}
but we don't know the structures FS1 and FS2.  
How many different pairs of feature structures FS1 and FS2
satisfy the equation?
Why might this sort of question be relevant to language processing?
\end{enumerate}

\subsection{Answers}

\begin{enumerate}
\item
\begin{enumerate}
\item $\bot$
\item {\tiny $\avmplus{\att{t}\\
             \attval{F}{\ind{1}}\\
             \attval{G}{\ind{1}}\\
             \attval{H}{\ind{1}}\\
             \attval{J}{\ind{1}}}$}
\item $\bot$ (cyclic structure).
\end{enumerate}
\item 
\begin{enumerate}
\item
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{NUMAGR}{\ind{1} \myvaluebold{pl}}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{pl-lexeme}\\ 
\attvaltyp{ORTH}{"these"}\\                                                                                                        
\attvaltyp{CATEGORY}{det}\\
\attval{NUMAGR}{\ind{1}}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{n}\\
\attval{NUMAGR}{\ind{1}}}}\\
\attvaltyp{REST}{*null*}}}}}}$}
\item
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{NUMAGR}{\ind{1} \myvaluebold{pl}}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{pl-lexeme}\\ 
\attvaltyp{ORTH}{"these"}\\                                                                                                        
\attvaltyp{CATEGORY}{det}\\
\attval{NUMAGR}{\ind{1}}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{pl-lexeme}\\
\attvaltyp{ORTH}{"dogs"}\\
\attvaltyp{CATEGORY}{n}\\
\attval{NUMAGR}{\ind{1}}}}\\
\attvaltyp{REST}{*null*}}}}}}$}
\item
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{NUMAGR}{\ind{1} \myvaluebold{pl}}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{pl-lexeme}\\ 
\attvaltyp{ORTH}{"these"}\\                                                                                                        
\attvaltyp{CATEGORY}{det}\\
\attval{NUMAGR}{\ind{1}}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{lexeme}\\
\attvaltyp{ORTH}{"dogs"}\\
\attvaltyp{CATEGORY}{n}\\
\attval{NUMAGR}{\ind{1}}}}\\
\attvaltyp{REST}{*null*}}}}}}$}
\item $\bot$
\end{enumerate}
As you have probably realized, this demonstrates 
how reentrancy ensures that the daughters of the np rule
have consistent values for agreement.
Note that when unification succeeds, the resulting
phrase has a value for NUMAGR which is the
unification of the values of NUMAGR on the daughters.
This ensures that there is also agreement with the verb phrase.
With respect to the last example, notice that
the three feature structures which were unified were all 
pairwise compatible: it was only the combination of
all three which fails.  Thus the np rule itself works
equally well for both singular and plural cases.

\item On the assumption that {\bf t} is an immediate daughter of {\bf *top*},
there are 10 feature structures that subsume\\
{\tiny $\avmplus{\att{t}\\
\attval{F}{\ind{1} \myvaluebold{*top*}}\\
             \attval{G}{\ind{1}}}$}\\
(including the structure itself).  These are shown in Figure~\ref{subsumption}.
If you examine this hierarchy, you will see that
any two feature structures
always have a unique greatest lower bound, which is equal to their 
unification.  We could have defined unification in this way,
by initially defining subsumption, and then defining unification
as the glb of structures in the subsumption order.  This is actually
how the definition goes in Chapter~\ref{formal}.  You should also
be able to see that every pair of structures has a unique lowest
upper bound (lub).  This corresponds to finding a feature structure
which represents the pieces of information
that common to both structures.  This operation is called {\it generalisation}
--- it is currently
relevant in the LKB only with respect to the treatment of defaults, which we
are not going to discuss in this chapter.
\begin{figure}
\begin{center}
\setlength{\unitlength}{0.8in}
\begin{picture}(2.5,4.5)(-0.25,-0.25)
\thicklines
\put(1,4){\makebox(0,0){{\tiny $\avmplus{\att{*top*}}$}}}
%
\put(0,3){\makebox(0,0){{\tiny $\avmplus{\att{t}}$}}}
%
\put(1,3){\makebox(0,0){{\tiny $\avmplus{\att{*top*}\\
\attvaltyp{F}{*top*}}$}}}
%
\put(2,3){\makebox(0,0){{\tiny $\avmplus{\att{*top*}\\
\attvaltyp{G}{*top*}}$}}}
%
\put(0,2){\makebox(0,0){{\tiny $\avmplus{\att{t}\\
\attvaltyp{F}{*top*}}$}}}
%
\put(1,2){\makebox(0,0){{\tiny $\avmplus{\att{t}\\
\attvaltyp{G}{*top*}}$}}}
%
\put(2,2){\makebox(0,0){{\tiny $\avmplus{\att{*top*}\\
\attvaltyp{F}{*top*}\\
\attvaltyp{G}{*top*}}$}}}
%
\put(0,1){\makebox(0,0){{\tiny $\avmplus{\att{t}\\
\attvaltyp{F}{*top*}\\
\attvaltyp{G}{*top*}}$}}}
%
\put(2,1){\makebox(0,0){{\tiny $\avmplus{\att{*top*}\\
\attval{F}{\ind{0} \myvaluebold{*top*}}\\
\attval{G}{\ind{0} \myvaluebold{*top*}}}$}}}
%
\put(1,0){\makebox(0,0){{\tiny $\avmplus{\att{t}\\
\attval{F}{\ind{0} \myvaluebold{*top*}}\\
\attval{G}{\ind{0} \myvaluebold{*top*}}}$}}}

\put(0.7,3.7){\line(-1, -1){0.4}}
\put(1,3.7){\line(0, -1){0.4}}
\put(1.3,3.7){\line(1, -1){0.4}}

\put(0,2.7){\line(0, -1){0.4}}
\put(0.3,2.7){\line(1, -1){0.4}}

\put(2,2.7){\line(0, -1){0.4}}
\put(1.7,2.7){\line(-1, -1){0.4}}

\put(1.3,2.7){\line(1, -1){0.4}}
\put(0.7,2.7){\line(-1, -1){0.4}}

\put(0,1.7){\line(0, -1){0.4}}
\put(0.7,1.7){\line(-1, -1){0.4}}

\put(1.7,1.7){\line(-2, -1){1.2}}
\put(2,1.7){\line(0, -1){0.4}}

\put(0.3,0.7){\line(1, -1){0.4}}
\put(1.7,0.7){\line(-1, -1){0.4}}

\end{picture}
\end{center}
\caption{Subsumption hierarchy}
\label{subsumption}
\end{figure}

There are 14 possible pairs of structures (or 27 if you count
ordered pairs) which can be combined to
give this structure.
We won't list
them all here, but  briefly go through the argument.  
We know the only structures that can be involved are
those that subsume the full structure.
We can consider each of these 
10 possibilities for FS1 and see how many possible structures for
FS2 there are in each case, ignoring any that duplicate those
we have already found.
If FS1 is the full structure,
there are 10 possibilities for FS2.  If FS1 is:\\ 
{\tiny $\avmplus{\att{*top*}\\
\attval{F}{\ind{1} \myvaluebold{*top*}}\\
             \attval{G}{\ind{1}}}$}\\
there are 5 possibilities for FS2 (all the structures which specify that
the root node is of type {\bf t}), but one of these is the case where
FS2 is the full structure, which is equivalent
to a pair we have already considered. 
Of the other 8 possibilities
for FS1, none contain any information about reentrancy, thus they 
must be combined with a reentrant structure, but we've already
considered all these cases.

In
general, if $F \unify G = H$, then we know that both $F$ and $G$ subsume (or
are equal to) $H$, and that they mutually contain all the information in $H$,
but we have no way of knowing which information
is in which structure.  There will be a finite number of possible 
candidate feature structure pairs (assuming $H$ is a finite structure),
but there could be a very large number of possibilities.
Even if we also know $F$, we still cannot in general determine $G$,
because while we know that it must contain all the information 
that is in $H$ which is not in $F$, it may also contain 
some of the same information as $F$.  The only exception to this
is if $F$ is the most general structure {\tiny $\avmplus{\att{*top*}}$},
then $G$ must be equal to $H$.

The relevance of this concerns
processing strategies.  While we can deterministically
and efficiently unify two structures, we cannot, in general, reverse
the operation efficiently.  The claim is
often made that unification-based grammars are `reversible': i.e., 
where what is meant by this is that
they can be used for parsing and generation.  To be accurate this
statement has to be qualified in multiple ways --- it certainly
isn't true that all unification-based grammars are suitable for
generation.  But the point here is just that efficient
grammar reversibility
does not involve reversing unifications.

\end{enumerate}

\section{Type constraints and inheritance}
\label{wf}

We finally arrive at a detailed description of the operation of
the type constraints.  Up till now, we have been discussing
feature structures in general, but now we want to concentrate
on those which are well-formed with respect to a set
of type constraints.
Usually the feature structures
corresponding to descriptions (e.g., of lexical entries etc) won't
be well-formed: the process of inheriting/inferring information, which we've
alluded to a couple of times informally so far, is in fact precisely the
process of making the structure well-formed. 
The primary purpose of type constraints as far as the grammar
writer is concerned is that they can be used to allow generalisations
to be expressed, so that lexical entries etc can be kept
succinct.  Their secondary purpose is to avoid errors
creeping into a grammar, such as misspelt feature names.

We will start off by defining well-formedness of a feature structure
with respect to a set of type constraints, and talk about 
how the system converts a non-well-formed structure to a well-formed one,
via type inference.  Then we'll look at how unification
operates on well-formed feature structures.  Finally we will
describe the internal conditions on a set of type constraints,
and discuss how the definitions of types we have seen in the
examples so far (e.g. in Figure~\ref{lkbgram1}) give rise to
the actual type constraints.

\subsection{Well-formedness}

To make the description of well-formedness slightly easier to follow,
we will talk about the substructures of a feature structure,
where this just means the feature structures rooted at each node
in the original structure.  Each substructure of a well-formed
feature structure must be subsumed by the constraint corresponding
to the type on its root node.  Furthermore, all the
arcs starting at that node must correspond to arcs which start at the
root node of the type constraint.  For convenience, we'll talk about
the features which label arcs starting from the root node as
the {\it top-level} features of a structure.  The top-level features
of a type constraint are referred to as the {\it appropriate features}
for that type.

Figure~\ref{fullcons} shows the full constraint and the appropriate
features for all the types in the simplest grammar.  Note
that the constraints on some types, such as {\bf phrase}, contain
some information which was not in their description: this is
because they have inherited information from types higher in the
hierarchy, as we'll see in detail in \S\ref{tc}.
\begin{figure}
\begin{center}
\begin{tabular}{lll}
type & constraint & appropriate features\\ \hline\hline
{\bf *top*} & {\tiny $\avmplus{\att{*top*}}$} & \\
{\bf string} & {\tiny $\avmplus{\att{string}}$} & \\
{\bf *list*} & {\tiny $\avmplus{\att{*list*}}$} & \\
{\bf *ne-list*} &
{\tiny $\avmplus{\att{*ne-list*}\\
\attvaltyp{FIRST}{*top*}\\
\attvaltyp{REST}{*list*}}$}
& FIRST REST\\
{\bf *null*}  & {\tiny $\avmplus{\att{*null*}}$}& \\
{\bf synsem-struc} & 
{\tiny $\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{cat}}$} 
& CATEGORY \\
{\bf cat} & {\tiny $\avmplus{\att{cat}}$} & \\
{\bf s} & {\tiny $\avmplus{\att{s}}$}& \\
{\bf np} & {\tiny $\avmplus{\att{np}}$}& \\
{\bf vp} & {\tiny $\avmplus{\att{vp}}$} & \\
{\bf det} & {\tiny $\avmplus{\att{det}}$} & \\
{\bf n} & {\tiny $\avmplus{\att{n}}$} & \\
{\bf phrase} &
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{cat}\\
\attvaltyp{ARGS}{*list*}}$} 
 & CATEGORY ARGS\\
{\bf lexeme} &
{\tiny $\avmplus{\att{lexeme}\\
\attvaltyp{ORTH}{string}\\
\attvaltyp{CATEGORY}{cat}}$} 
& CATEGORY ORTH \\
{\bf root} & 
{\tiny $\avmplus{\att{root}\\
\attvaltyp{CATEGORY}{s}\\
\attvaltyp{ARGS}{*list*}}$}
 & CATEGORY ARGS
\end{tabular}
\end{center}
\caption{Constraints and appropriate features
for the tiny grammar}
\label{fullcons}
\end{figure}
%

Note that some types, such as {\bf cat}, {\bf n} and {\bf vp}
in the example grammar, have no appropriate features.  This means
that in any well-formed feature structure, they can only
label terminal nodes.  We refer to types which
have no appropriate features and which have no descendants with
appropriate features as {\it atomic} types.
The type {\bf *top*} has no appropriate features, 
but some of its descendants do, so it is not an atomic type.

The following examples are all well-formed typed feature structures
given these type constraints (note that not
all well-formed structures are sensible!):
\begin{enumerate}
\item
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{s}\\
\attvaltyp{ARGS}{*list*}}$}
\item 
{\tiny $\avmplus{\att{lexeme}\\
\attvaltyp{ORTH}{"on"}\\
\attvaltyp{CATEGORY}{s}}$}
\item 
{\tiny
$\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\ind{1} \myvaluebold{*list*}}\\
\attval{REST}{\ind{1}}}$}
\item
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{lexeme}\\                                                
\attvaltyp{ORTH}{"these"}\\                                                      
\attvaltyp{CATEGORY}{det}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{n}}}\\
\attvaltyp{REST}{*null*}}}}}}$}
\end{enumerate}

The following examples are not well-formed and do not
subsume well-formed structures:
\begin{enumerate}
\item
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{lexeme}\\
\attvaltyp{ARGS}{*list*}}$}\\
Wrong type on CATEGORY.
\item
{\tiny $\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{s}\\
\attvaltyp{AGS}{*list*}}$}\\
AGS is not an appropriate feature for
{\bf synsem-struc} (or for anything else).
\item 
{\tiny $\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{s}\\
\attvaltyp{FIRST}{lexeme}}$}\\
FIRST is not an appropriate feature
for {\bf synsem-struc}.
\end{enumerate}


The following examples are not well-formed but they
subsume well-formed structures which can be constructed
by the type inference process 
which we will discuss below:
\begin{enumerate}
\item
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{*top*}\\
\attvaltyp{ARGS}{*list*}}$}\\
Wrong type on CATEGORY, but it is compatible with
a valid type.
\item
{\tiny $\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{s}\\
\attvaltyp{ARGS}{*list*}}$}\\
ARGS is not an appropriate feature for
{\bf synsem-struc} but it is appropriate
for {\bf phrase} which is compatible with
{\bf synsem-struc}.
\item
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{s}}$}\\
The ARGS feature is missing,
so the constraint on {\bf phrase} does not
subsume this structure, but it is compatible with it.
\end{enumerate}

\subsubsection{Exercises on well-formedness}

\begin{enumerate}
\item For each of the following, specify whether it is
well-formed or not (assuming the constraints
given in Figure~\ref{fullcons}).  For the
non-well-formed structures which
subsume some well-formed
structures, list the most general such well-formed structure.
\begin{enumerate}
\item {\tiny $\avmplus{\att{*null*}\\
\attvaltyp{FIRST}{synsem-struc}}$}
\item {\tiny $\avmplus{\att{*top*}\\
\attvaltyp{FIRST}{synsem-struc}}$}
\item {\tiny $\avmplus{\att{*top*}\\
\attval{REST}{\avmplus{\attvaltyp{REST}{*null*}}}}$}
\item {\tiny $\avmplus{\att{*top*}\\
\attval{CATEGORY}{\ind{0}}\\
\attval{ARGS}{\avmplus{\attval{REST}{\ind{0} \avmplus{\att{*top*}\\
              \attvaltyp{REST}{*top*}}}}}}$}
\end{enumerate} 
\item Consider the subsumption hierarchy you constructed in the answers to
the exercises in
\S\ref{unifex}.  Which of the structures are well-formed feature
structures under the
assumption that the type {\bf t} is an immediate daughter of {\bf *top*},
and has the following constraint:\\
{\tiny $\avmplus{\att{t}\\
\attvaltyp{F}{*top*}\\
\attvaltyp{G}{*top*}}$}
\end{enumerate}

\subsubsection{Answers to exercises on well-formedness}

\begin{enumerate}
\item
\begin{enumerate}
\item not well formed --- FIRST isn't an appropriate feature for {\bf *null*}.
\item not well formed, but subsumes:\\
{\tiny $\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{cat}}}\\
\attvaltyp{REST}{*list*}}$}
\item not well formed, but subsumes:\\
{\tiny $\avmplus{\att{*ne-list*}\\
\attvaltyp{FIRST}{*top*}\\
\attval{REST}{\avmplus{\att{*ne-list*}\\
\attvaltyp{FIRST}{*top*}\\
\attvaltyp{REST}{*null*}}}}$}
\item not well formed --- CATEGORY's value must be of type
{\bf cat} but this node is reentrant with a node that must be
of type {\bf *ne-list*}, which isn't compatible with {\bf cat}.
\end{enumerate}
\item The only well-formed structures are the following (shown in 
a subsumption hierarchy):
\begin{center}
\setlength{\unitlength}{0.8in}
\begin{picture}(2,2.5)(0,-0.25)
\thicklines
\put(1,2){\makebox(0,0){{\tiny $\avmplus{\att{*top*}}$}}}
\put(1,1.7){\line(0, -1){0.4}}
\put(1,1){\makebox(0,0){{\tiny $\avmplus{\att{t}\\
\attvaltyp{F}{*top*}\\
\attvaltyp{G}{*top*}}$}}}
\put(1,0.7){\line(0, -1){0.4}}
\put(1,0){\makebox(0,0){{\tiny $\avmplus{\att{t}\\
\attval{F}{\ind{0} \myvaluebold{*top*}}\\
\attval{G}{\ind{0} \myvaluebold{*top*}}}$}}}
\end{picture}
\end{center}
In general, we can talk about a subsumption
hierarchy of well-formed feature structures, 
and define well-formed unification as 
determining the glb within that hierarchy.

\end{enumerate}

\subsection{Type inference}

Type inference is the process of transforming a non-well-formed
structure into a well-formed one which it subsumes.  Thus
it always preserves the information in the initial structure.
As you should have realized from the exercise
above, it is always possible to find a most general
well-formed structure from a non-well-formed initial structure
if it subsumes any well-formed structures.
The LKB system carries
out type inference on all entries.  It is an error to
define an entry which cannot be converted into a well-formed 
feature structure.  

The first part of type inference consists of specialising
the types on each node in the structure until all the features on that
node are ones appropriate for that type.
For instance, given the following structure
ARGS is not an appropriate feature for {\bf synsem-struc}
but it is appropriate for {\bf phrase}, which is a subtype of 
{\bf synsem-struc}.  
\begin{ex}
{\tiny $\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{s}\\
\attvaltyp{ARGS}{*top*}}$}
\end{ex}
The result is therefore
\begin{ex}
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{s}\\
\attvaltyp{ARGS}{*top*}}$}
\end{ex}
Both ARGS and CATEGORY are also appropriate for {\bf root}
but since this is a subtype of {\bf phrase}, we don't consider it.
(This works deterministically because of a condition on
how features are introduced in the type constraints
as we will discuss in \S\ref{tc}.)  If the initial structure had been:
\begin{ex}
{\tiny $\avmplus{\att{*top*}\\
\attvaltyp{CATEGORY}{s}\\
\attvaltyp{FIRST}{lexeme}}$}
\end{ex}
we could not have found a possible type, because there are
no types for which both CATEGORY and FIRST are appropriate.
If it had been:
\begin{ex}
{\tiny $\avmplus{\att{*list*}\\
\attvaltyp{CATEGORY}{s}}$}
\end{ex}
we also could not have found a possible type, because
although CATEGORY is an appropriate feature for {\bf synsem-struc},
this is not a subtype of {\bf *list*}.

The second stage in type inference consists of ensuring that
all nodes are subsumed by their respective type constraints.
This simply involves unifying each node with the constraint 
of the type on each node.  So from
\begin{ex}
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{s}\\
\attvaltyp{ARGS}{*top*}}$}
\end{ex}
we obtain
\begin{ex}
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{s}\\
\attvaltyp{ARGS}{*list*}}$}
\end{ex}
Type inference may also fail at this point.  For instance, given:
\begin{ex}
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{*list*}\\
\attvaltyp{ARGS}{*top*}}$}
\end{ex}
we find that the type on CATEGORY is inconsistent
with the constraint on {\bf phrase}.

\subsection{Unification revisited}
\label{wfunif}

Unification of well-formed feature structures can 
be described 
in the same sort of way as unification of typed feature structures
in general: the result is the most general
well-formed feature structure which
combines all the information in two well-formed feature structures.
In most cases, this gives exactly the
same result as we've seen from unification previously.
However, there is a complication which arises only in some cases
where there is multiple inheritance, when there is a constraint
on a glb type which is more specific than the constraints
of the two ancestor types. 

To illustrate this,
consider the corrected
animal hierarchy shown in Figure~\ref{glbanimal},
and suppose it also contains the type {\bf boolean},
as a daughter of {\bf *top*}, and {\bf true} and {\bf false}
inheriting from {\bf boolean}.
Suppose that the constraint on the type {\bf swimmer} is\\
{\tiny
$\avmplus{\att{swimmer}\\
\attvaltyp{FINS}{boolean}}$}
the constraint on
{\bf mammal} is\\
{\tiny $\avmplus{\att{mammal}\\
\attvaltyp{FRIENDLY}{boolean}}$}\\
and the constraint on {\bf whale} is\\
{\tiny $\avmplus{\att{whale}\\
\attvaltyp{HARPOONED}{boolean}\\
\attvaltyp{FINS}{true}\\
\attvaltyp{FRIENDLY}{boolean}}$}

Consider what happens when we unify the following
feature structures (both of which are well-formed):\\
{\tiny $\avmplus{\att{mammal}\\
\attvaltyp{FRIENDLY}{true}}$} $\unify$
{\tiny
$\avmplus{\att{swimmer}\\
\attvaltyp{FINS}{boolean}}$}\\
If we ignored the requirement for
well-formedness, the result would be:\\
{\tiny $\avmplus{\att{whale}\\
\attvaltyp{FINS}{boolean}\\
\attvaltyp{FRIENDLY}{true}}$}
but this isn't well-formed --- it lacks the feature 
HARPOONED, and the value of FINS isn't {\bf true}.
To get a well-formed feature
structure, we also have to add the constraint information
on {\bf whale}, to get:\\
{\tiny $\avmplus{\att{whale}\\
\attvaltyp{HARPOONED}{boolean}\\
\attvaltyp{FINS}{true}\\
\attvaltyp{FRIENDLY}{true}}$}

This result is the most general well-formed structure that
contains all the information in the structures being unified,
but of course it also contains additional information.  This actually arises
in ordinary unification too.  For instance, when
a structure which states that the value of a feature F is {\bf a} is unified
with a structure that says that F is reentrant with G and that their value
is {\bf *top*}, the result is a structure where the value for G is
{\bf a}, which is information that wasn't contained in either
of the original structures.  Of course this arises from the
logical properties of the structures, rather than the type
information, so you may find the properties of well-formed unification
less intuitive.  

If the constraint on the glb type is inconsistent with
the given information, unification fails.
For instance:\\
{\tiny $\avmplus{\att{mammal}\\
\attvaltyp{FRIENDLY}{true}}$} $\unify$
{\tiny
$\avmplus{\att{swimmer}\\
\attvaltyp{FINS}{false}}$} $=$ $\bot$\\

\subsection{Conditions on type constraints}
\label{tc}

The final part of the story concerns the construction of the
full type constraints as shown in Figure~\ref{fullcons} from
the descriptions shown in Figure~\ref{lkbgram1}.
We will refer to the feature
structures which are directly specified in the descriptions
as the {\it local} constraints.
There are basically four conditions on type constraints
which determine how the local constraints are expanded into the 
full constraints:
\begin{description}
\item[Type] The type of the feature structure
expressing the constraint on a type is always that type.
\item[Consistent inheritance]
The constraint on a type must be subsumed by the constraints
on all its parents.  This means that any local constraint
specification must be compatible with the inherited
information, and that
in the case of multiple inheritance,
the parents' constraints must unify.
\item[Maximal introduction of features]
Any feature must be introduced at a single point in the hierarchy.
That is, if a feature, F, is an appropriate feature
for some type, $t$, and 
not an appropriate feature for any of its
ancestors, then F cannot be appropriate for a type which is not
a descendant of $t$.  Note that the consistent inheritance
condition guarantees that the feature will be appropriate
for all descendants of $t$.
\item[Well-formedness of constraints]  All full constraint feature
structures must be well-formed as described in
\S\ref{wf}.  
\end{description}

You should check Figure~\ref{fullcons} and Figure~\ref{lkbgram1}
to make sure you follow how this applies to the tiny grammar.
Although the constraints in this grammar are all very
simple, constraints can in general be
arbitrarily complex feature structures.  For instance,
taking the version of the grammar with agreement, shown in Figure~\ref{lkbgram2},
we could have simplified the description of the rules
that was given there by making them inherit from a subtype of {\bf phrase}
with the following description:
\begin{verbatim}
agr-phrase := phrase &
[ NUMAGR #1,
  ARGS [ FIRST [ NUMAGR #1 ],
         REST [ FIRST [  NUMAGR #1 ],
                REST *null* ]]] .
\end{verbatim}

There is one non-obvious consequence of the conditions on
type constraints, which is that it disallows type
descriptions such as the following:
\begin{verbatim}
list := *top* &
[ FIRST *top*,
  REST list ].
\end{verbatim}
The reason for this is that it would result in an infinite structure
when we tried to make it well-formed.  That is, it would be expanded so
that the feature structure which is the value of REST would have
features FIRST and REST and that value of REST would be
of type {\bf list} which would be expanded in the same way, and so on.
The solution to this problem is to define lists in the way
we've done in the example grammars, so that there are distinct
subtypes for empty and non-empty lists, with the latter having
no appropriate features.

\subsection{Exercises}
\begin{enumerate}
\item What would be the full constraint on {\bf agr-phrase}
if it was defined as shown above?
\item Considering the corrected animal hierarchy, Figure~\ref{glbanimal},
augmented as described in \S\ref{wfunif}.  Write type
definitions which would result in the full constraints
described.  Is it necessary that there be any local constraint on
{\bf whale}?
\item Suppose I change the definition of {\bf phrase} in the
tiniest grammar to:
\begin{verbatim}
phrase := synsem-struc &
[ ORTH *top*,
  ARGS *list* ].
\end{verbatim}
What else would I have to do to the type system
to keep it valid?
\end{enumerate}

\subsection{Answers}
\begin{verbatim}
agr-phrase := phrase &
[ NUMAGR #1,
  ARGS [ FIRST [ NUMAGR #1 ],
         REST [ FIRST [  NUMAGR #1 ],
                REST *null* ]]] .
\end{verbatim}
\begin{enumerate}
\item {\tiny $\avmplus{\att{agr-phrase}\\
\attvaltyp{CATEGORY}{cat}\\
\attval{NUMAGR}{\ind{1} \myvaluebold{agr}}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{synsem-struc}\\                                                                                                      
\attvaltyp{CATEGORY}{cat}\\
\attval{NUMAGR}{\ind{1}}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{synsem-struc}\\
\attvaltyp{CATEGORY}{cat}\\
\attval{NUMAGR}{\ind{1}}}}\\
\attvaltyp{REST}{*null*}}}}}}$}
\item One possible set of constraint definitions is:
\begin{verbatim}
swimmer := animal &
[ FINS boolean ].

vertebrate := animal &
[ FRIENDLY boolean ].

vertebrate-swimmer := vertebrate &
[ HARPOONED boolean,
  FINS true ].
\end{verbatim}
so there need be no local constraint on {\bf whale}.
\item ORTH would also have to be added to the
type {\bf synsem-struc} so it was only introduced in one place.
\end{enumerate}

\section{Grammar rules, lexical entries and parsing}

Having gone through all of this to define typed feature
structures, we are now in the happy position where parsing 
is relatively straightforward to describe.  
We will concentrate on parsing, since 
talking about generation really requires that we talk about semantic
representation, and we don't want to go into that here.
For the parser to work, in addition to the grammar
it just needs two extra pieces of
information, which in the LKB are specified in the globals and user-fns files:
\begin{enumerate}
\item The location of orthographic information in lexical structures
(here the feature ORTH).  This is needed so the system can go
from an input string to a set of lexical structures.
\item The location of the daughters and the mother structure in
grammar rules.  Here the daughters are the elements of the ARGS list
and the mother is the entire structure.  The application of
a grammar rule can then be implemented by unification, in the way that
we saw in \S\ref{unifex}. 
\end{enumerate}

The normal assumption is that
the input to the parser is a string which can be regarded
as a list of word strings.  We want to define parsing itself as an 
operation on typed feature structures, so we have to assume that
there is an initial lexical lookup phase, where the system
goes from this list of words strings to a list of sets of lexical
structures.  We have to think about sets of lexical
structures because of lexical
ambiguity: a valid parse is going to involve exactly one structure from
each set.  For the tiniest grammar, we simply assume that the word
strings are delimited by spaces, and that a structure corresponding
to a lexical entry
is added to the set for a word string if the value of its ORTH feature
matches the string.  In realistic grammars, lexical lookup is complicated
by morphology and {\it multiword entries}, but we will ignore that here.

We will
also assume that each phrase is associated with a string
which is formed by concatenating the strings associated with 
its daughters in a fixed order (which in this grammar corresponds
to the order of the elements on the ARGS list of the rule).
This is one of the assumptions we made without much discussion
at the beginning of the chapter, although it's not, in general,
a necessary assumption
for grammar writing with typed feature structures.
However, since the parser supplied with the LKB
depends on this condition for its operation, we will continue to
assume it here.  We'll see at the end of the chapter that
we can actually implement this constraint in the grammar rules.


In abstract terms, at the start of parsing we have a list of sets of
lexical structures, with associated strings, and a root structure,
which is a typed feature structure corresponding to the start
symbol.  The string associated with the the root structure
corresponds to the string for the complete sentence.
Parsing then essentially involves
throwing in as many structures corresponding to
grammar rules as are needed to link
the lexical structures
to the root structure \ldots
To make this a little more precise,
let us say that a valid link
is a well-formed typed feature structure
which is subsumed by a grammar rule
and which has structures in the daughter positions
which are each either subsumed by a member of the set of lexical
structures or are themselves valid links.
We then count as a solution any valid link which is
also subsumed by the root structure and which
has the same string (i.e., te string for the sentence).\footnote{This
statement gets more complex if the mothers of rules are
not the complete structures, but nothing essential changes.}

For instance, if we consider the tiniest grammar,
and constrain the set of lexical structures to
be those licensed by the string {\it this dog sleeps},
there is only one 
solution.  This is the structure which was shown in 
Figure~\ref{phraseavm}, repeated as Figure~\ref{phraseavmrep}.
\begin{figure}
\semnest{
\begin{center}
{\tiny $\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{s}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{phrase}\\
\attvaltyp{CATEGORY}{np}\\
\attval{ARGS}{\avmplus{\att{*ne-list*}\\
\attval{FIRST}{\avmplus{\att{lexeme}\\                                                
\attvaltyp{ORTH}{"this"}\\                                                      
\attvaltyp{CATEGORY}{det}}}\\                            
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{lexeme}\\
\attvaltyp{ORTH}{"dog"}\\   
\attvaltyp{CATEGORY}{n}}}\\
\attvaltyp{REST}{*null*}}}}}}}\\
\attval{REST}{\avmplus{\att{*ne-list*}\\                                                   
\attval{FIRST}{\avmplus{\att{lexeme}\\
\attvaltyp{ORTH}{"sleeps"}\\                                                      
\attvaltyp{CATEGORY}{vp}}}\\
\attvaltyp{REST}{*null*}}}}}}$}
\end{center}}
\caption{AVM for {\it this dog sleeps}}
\label{phraseavmrep}
\end{figure}

Although formally parsing in the LKB corresponds to the description
given above, for practical purposes we need something rather more
deterministic.  The approach used is known as {\it chart parsing},
where the chart is the data structure which records the partial results,
known as {\it edges}.  Edges correspond to phrases with their associated
string.
In the case of the LKB, processing proceeds
{\it bottom-up}.  This means the parser initially
combines lexical structures to form phrases,
and then combines these phrases with one another, or
with other lexical structures.  The information embodied in
the root structure is not checked till the end.
We will not go through chart parsing in detail
here, but if you open a window
showing the chart in the LKB, you will see all the edges that
are constructed during an attempt to parse a string.

\section{The description language}

To complete the description of typed feature structures,
we will make a few remarks about the description language.
The purpose of the description language is, of course,
to allow the grammar designer to write definitions of types and of
feature structures in a way which is readable
by a machine.  The same grammar could be described using
different description languages.
We think that
it's not really useful to try and give a
non-formal account of the description language used in the LKB 
which explains exactly where to put the \verb+&+'s etc.
The section on the description language in Chapter~\ref{formal}
(\S\ref{syntax}) goes through a series of examples
which you may find useful, but most people find it easiest
to proceed by modifying existing examples.  You may find the
discussion of the error messages in \S\ref{error} is helpful.

However, it is important to
notice a distinction between the definitions of types
and everything else (i.e., what we've been calling the entries).
The type definitions
package together two distinct pieces of information: the
location of the type in the hierarchy, 
and the definition of the
constraint on that type.  So the following description of the type {\bf a}:
\begin{verbatim}
a := b & c.
\end{verbatim}
is read as stating that the type {\bf a} is a
lower bound of {\bf b} and {\bf c} in the hierarchy.  
Elsewhere in the description language,
\verb+&+ can be read as effectively equivalent to unification,
but here it must be read as an operation on the type
hierarchy.
It follows from this definition
that the constraint on {\bf a} will be the unification of 
the constraints on {\bf b} and {\bf c}, but as we've seen
we need to know what the type hierarchy is before we can 
talk about unification, so the statement about the hierarchy
is logically prior.

Note that the same description used in an entry (e.g., in the 
description of a lexical entry or grammar rule) must be read quite
differently.  In this context, it
means that the feature structure named {\tt a} is the unification of
the feature structures {\tiny $\avmplus{\att{b}}$} and {\tiny $\avmplus{\att{c}}$}.
\footnote{If you read Chapter~\ref{formal},
you will notice that we overload the same operator there
so it stands for both unification and greatest-lower bound
(join) in the type
hierarchy.  In that context, however, there is no ambiguity
as to whether we are referring to types or feature
structures.  Unfortunately the TDL description language 
does not distinguish between a type, {\bf t}, and the feature structure 
consisting of a single node labelled with that type, which we write as
$\avmplus{\att{t}}$ in the AVM notation.  
This means that a syntactically
identical description is understood quite differently if it is the 
description of a type as opposed to the description of an entry.  
QWERTY keyboards
again.} 
Since this is only valid if there is some type {\bf d}
which is the greatest lower bound
of {\bf b} and {\bf c}, it isn't usual to write descriptions of entries like
this, since it's clearer to simply write:
\begin{verbatim}
a := d.
\end{verbatim}



\section{Some grammar engineering techniques}

Please note that this section will probably be expanded at some
future date.
%FIX

The types {\bf *top*}, {\bf string}, {\bf *list*}, {\bf *null*}
and {\bf *ne-list*} are ones we will see over and over again
in different grammars.  And as we have
mentioned, the method of encoding rules
with the feature ARGS is used in several grammars.  
But the grammars in this chapter are not 
to be taken as examples of linguistically good grammar
design.  So this final section in this chapter is
simply structured as 
a series of examples which introduce various encoding
techniques.
You may find it useful to go through these before going back to
working with a more linguistically
interesting grammar, such as the toy grammar.

\subsection{Encoding information with types vs with features} 

You may have started to wonder what the criteria are for encoding information
with types or with features.  
In some cases, they are more or less interchangeable,
and the choice made by a particular grammar writer may be
more or less a matter of aesthetics.  But in a lot of cases
there is a distinct formal difference.  
We will illustrate this by going back to
the agreement example.

The reason that the simple agreement 
example works out more nicely with typed feature structures than with the
atomic category grammar is because of the use of features to encode
agreement, because that allows the specification of reentrancy between
parts of the structure.  If we had used subtypes of category 
instead, there would have been 
little advantage over
the atomic category version, because we could not have encoded the fact we
want the structures to match on agreement, but not on nouniness or verbiness,
for instance.  

In the agreement example, we only considered number agreement.  Of course,
person is also relevant.  
One way of encoding this would have been to use a feature AGR instead of NUMAGR
which takes the type {\bf pernum}
and to have NUM and PER as appropriate features for {\bf pernum}.
For instance:\\
{\tiny $\avmplus{\att{agr}\\ \attvaltyp{PER}{3rd}\\  \attvaltyp{NUM}{sg}}$}\\
We can then stipulate the reentrancy between the value of AGR on verbs and their
subjects for instance, and ensure that they are consistent with respect to both
number and person.

However, although this encoding has frequently been used, this may not be the
most perpicuous way to treat agreement in English.  The problem is that
for most English verbs, the distinction in morphological form is between
third person singular forms and everything else.  While we can represent
third person singular simply enough, as above, expressing the generalization
`everything else', is not so easy.\footnote{One option
is to add negation to the formalism, and indeed this has been done with some
variants of constraint-based formalisms, but it turns out to have lots
of formal consequences that we find unpleasant.  Rather than go through
the formal issues here, we will simply observe that we in general take the 
position that the formalism should only be expanded when there's a real
gain in expressiveness and naturalness for a reasonable
number of linguistic examples, and negation
on our view fails this test, since it's unusual to find examples of its use in
the literature other than the `not third person singular' case \ldots}
With the LKB system, we can do
this by making use of the type hierarchy, so that {\bf pernum}
has subtypes {\bf 3sg} and {\bf non3sg}, 
where {\bf 3sg} has the constraint shown above, and all the other
combinations are represented as subcases of {\bf non3sg}.  This is shown below
(we have omitted the definition of the types {\bf per} and {\bf num} and their
subtypes).
\begin{verbatim}
pernum := *top* &
[ PER per, 
  NUM num ].

non3sg := pernum.

1sg := non3sg &
[ PER 1st, 
  NUM sg ].

1pl := non3sg &
[ PER 1st, 
  NUM pl ].

2sg := non3sg &
[ PER 2cd, 
  NUM sg ].

2pl := non3sg &
[ PER 2cd, 
  NUM pl ].

3sg := pernum &
[ PER 3rd, 
  NUM sg ].

3pl := non3sg &
[ PER 3rd, 
  NUM pl ].

\end{verbatim}

This sort of encoding is used in the toy grammar.
However this still isn't very elegant because
nothing blocks the 
following structure:\\
{\tiny $\avmplus{\att{non3sg}\\ \attvaltyp{PER}{3rd}\\  \attvaltyp{NUM}{sg}}$}\\
The reason for this is that the notion of well-formedness used in the
LKB is actually quite weak and it does not preclude a feature structure
of a type {\bf t}
being well-formed even if it is not compatible with any of the subtypes
of {\bf t}.\footnote{Again, there are feature structure formalisms in which
this could be ruled out, but there's a potentially high cost in computational
efficiency.}

At this point, however, we have now almost duplicated the feature information 
in the type hierarchy and
we should ask ourselves what the justification is for
the use of the features PER and NUM.  
If you look at the toy grammar and
its successors in the LKB data files, you will discover that there
is never a case where the grammar uses reentrancies between
number information
separately from person information.  
If there is no reason in the grammar
to make these pieces of information be independently
accessible, we can simply drop that part of the structure, and use a hierarchy
of atomic types instead, as shown below.
\begin{verbatim}
pernum := *top*.
non3sg := pernum.
1sg := non3sg.
1pl := non3sg.
2sg := non3sg.
2pl := non3sg.
3sg := pernum.
3pl := non3sg.
\end{verbatim}
A slightly more complicated version of this hierarchy is in fact
used in the large-scale
English resource grammar.

\subsection{Difference lists}

Suppose we added the following type to 
the type system in Figure~\ref{lkbgram1}:
\begin{verbatim}
*diff-list* := *top* &
 [ LIST *list*,
   LAST *list* ].
\end{verbatim}
and redefined {\bf synsem-struc}, {\bf lexeme} and
{\bf phrase} as follows:
\begin{verbatim}

synsem-struc := *top* &
[ ORTH *diff-list*,
  CATEGORY cat ].

phrase :=  synsem-struc &
 [ ORTH [ LIST #first,
	         LAST #last ],
   ARGS [ FIRST [ ORTH [ LIST #first,
                         LAST #middle ] ], 
          REST [ FIRST [ ORTH [ LIST #middle,
                                LAST #last ]]]]].
lexeme := synsem-struc &
[ ORTH [ LIST [ FIRST string,
                REST #end ],
         LAST #end ]].

\end{verbatim}
We also would have to
redefine the lexical entries so their orthography
goes in the right place, for instance: 
\begin{verbatim}
this := lexeme &
[ ORTH [ LIST [ FIRST "this"]],
  CATEGORY det ].
\end{verbatim}

A grammar with these modifications is in the directory {\tt difflist}.
Experiment with parsing some sentences 
and look at the values for the ORTH of the phrases.

Structures like these, where a pointer is maintained to the end of the
list (the LAST feature), are known as {\it difference lists}.
They can be used in general in grammars as a way of appending lists
simply using unification.
The way we have used them here, to allow the orthography
to be built up on phrases, means that the grammar itself encodes
the way that phrases correspond to strings. 
If we make the assumption that the feature structures themselves
encode orthography, the description of parsing that we gave in
\S\ref{parse} can be made general enough to include grammars
which do not make the assumptions about word order and
concatenation that we started off with.  But we won't explore this
further here, since the LKB chart parser builds in these assumptions.

\end{document}

