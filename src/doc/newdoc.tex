\documentclass[12pt]{report}

% lacunae etc indicated by FIX
% places which need to be checked for current 
% truth are indicated by UPDATE

\usepackage{latexsym}
\usepackage{times}
% use the time fonts for better results with
% pdf (CM fonts don't index correctly)
\usepackage[dvips]{hyperref}

%- comment out epsf box for TeXtures
\input epsf
%\newcommand{\epsfbox}[1]{}
%\def\epsfxsize{}
\input thedefs1
%\input thedefs

\setlength{\evensidemargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{9in}

\newcommand{\itsdb}{{\sf \lbrack incr tsdb()\rbrack}}

\newcommand{\lispcommand}[1]{\noindent\rm #1}%
\newcommand{\ldescnl}{\\}
\newenvironment{error}%
{\begin{quote}
\tt
}%
{\end{quote}
}
\newenvironment{warning}%
{\begin{quote}
\tt
}%
{\end{quote}
}


\begin{document}
\begin{center}
{\Huge\bf The (new) LKB system}\\[0.2in]
{\LARGE Ann Copestake}\\
{\tt aac@csli.stanford.edu}\\
\today
\end{center}
This document describes the new version of the LKB system:
a grammar and lexicon development environment for use with
constraint-based linguistic formalisms.

\tableofcontents

\chapter{Introduction}

\section{A brief introduction to the LKB system}

The LKB system is a grammar and lexicon development environment for use with
constraint-based formalisms.\footnote{Development 
of the LKB system was originally supported by ACQUILEX projects
BRA-3030 and 7315 under the Esprit program.  More recent research
has been supported by the
National Science Foundation under grant number IRI-9612682.
Detailed acknowledgments are in \S\ref{ack}.}
It is specifically designed for the use of typed
feature structures.  It is intended to be used for Natural Language Processing
research involving unification-based linguistic formalisms for parsing and/or
generation, and also for teaching.  It has been most extensively tested with
grammars based on HPSG (Pollard and Sag, 1987, 1994), but it is intended to be
framework independent.  In this philosophy, and in much else, 
we have tried to follow PATR
(Shieber, 1986).  The LKB system is implemented in Common Lisp and the
basic system is intended to 
run on any Common Lisp implementation, although the
graphical environment is currently limited to Macintosh Common Lisp (MCL)
or Allegro Common Lisp with the Common Lisp Interface
Manager (CLIM).\footnote{{\bf MCL} and 
{\bf Macintosh Common Lisp} are trademarks of Digitool, Inc.
{\bf Macintosh} is a trademark of Apple Computer.
{\bf Allegro CL} is a trademark of Franz Inc.
All other trademarks used in this document are the property of their
owners.}
Sites without a Lisp license that have suitable hardware can 
also use the LKB system:
for details of hardware requirements and installation, 
see Chapter~\ref{installation}.

The best way to think about the LKB system, and other comparable systems such
as \href{http://www.sfs.nphil.uni-tuebingen.de/~gpenn/ale.html}{ALE} 
and \href{http://www.dfki.de/lt/systems/page/page.html}{PAGE}, 
is as a development environment for a very high-level
specialized programming language.  Typed feature structure
languages are essentially based on one data structure --- the typed feature
structure, and one operation --- unification.  This combination is powerful
enough to allow the grammar developer to write grammars and lexicons that can
be used to parse and generate natural languages.  Essentially the grammar
developer is a programmer, and the grammars and lexicons 
comprise code to be run by
the system.\footnote{From now on, we'll use grammar to mean
all the linguistic code: that is grammar rules, lexicon, lexical rules 
and so on.}  
But because the typed feature structure language 
is so high level, working in it requires relatively little knowledge
of computers.  Typed feature structure languages were designed
(by linguists) as a formal way of specifying linguistic
behaviour rather than as programming languages, and they are used
in this way by people who have no involvement in computational 
linguistics.  So, at least potentially, these languages have several
advantages: they make computational
linguistics and natural language processing accessible to linguists
with a very limited background in computers, they enable computational 
linguists to adopt techniques from theoretical linguistics with minimal 
reinterpretation, and they allow formal theories to be tested on
a range of data in a way that allows the interaction between
treatments to be checked.

The LKB system is therefore a software package for writing
linguistic programs, i.e., grammars, and it's
important not to confuse the system and the grammars which run on it. By
itself the LKB system is of no use in an application, since
there has to be some associated grammar.
Although we distribute some grammars with the LKB, they are there to act as
examples, just as programming language packages include example code.  
The LKB system can be used to develop many different grammars.
What's more,
a grammar developed on the LKB system could in principle
be run on another platform that used the same typed feature 
structure language, just as a C$++$ program can be run
by a variety of compilers.\footnote{In practise, however, there
is no agreed standard for typed-feature structure systems.
The current situation is roughly similar to the different Lisp implementations
which existed before Common Lisp was initially developed, which is
to say that some grammars can be converted relatively easily between
platforms, while others are extremely difficult to convert because 
they use facilities which are not shared by all typed feature structure
based systems.  For example, the LKB system uses a version of 
default unification
which is not available on other typed feature structure systems,
and so grammars which use defaults are less portable than ones which
don't.  It's unlikely that there will ever be an ANSI-standard
typed feature structure language, though developers of the different
systems are trying to make portability easier, so the situation should
improve in the near future.}  
The performance of the LKB system in tasks such
as parsing depends critically on the grammar: it makes little sense to 
talk about performance except with respect to a 
particular grammar.

The LKB system
must therefore be distinguished from the LinGO English resource grammar
(ERG) which is being developed at CSLI.  The LinGO ERG runs on 
both the LKB system and on PAGE.
The ERG is not distributed as a standard part
of the LKB system.\footnote{Researchers interested in obtaining the
LinGO ERG should contact Dan Flickinger {\tt dan@csli.stanford.edu}.}
However,
some examples of the ERG are used in this documentation in order
to illustrate some of the facilities the system has for dealing 
with large and complex grammars.

\section{This manual}

The purpose of this manual is to teach users how to use the LKB system
to program their own grammars or to adapt existing ones.
We have tried not to make too many assumptions
about what the user knows how to do, but there are some things we have 
to assume if the documentation is ever going to be finished.
With respect to the content of the system, we've tried to write the 
documentation so that a user can start building grammars even if
they have never used a grammar development environment before,
but we do assume some knowledge of constraint-based formalisms,
such as HPSG.  The grammars distributed with the LKB are
linked to a textbook which introduces this sort of approach
to formal syntax: Sag and Wasow (1999).
There are some places where to fully understand
the documentation a knowledge of some NLP concepts, such as chart parsing,
will be helpful, but this should not be essential to use the system.
On the interaction side,
we assume the user knows how to open, close, resize windows etc
and how to select menu items.  We also assume that the user knows
how to use emacs, at least at a fairly elementary level, if running
the ACL/CLIM version of the LKB, or the MCL editor, if using a Mac.
There are a couple of places where it's useful to have a 
very basic understanding of Common Lisp, in particular when installing the
system, but we've tried to avoid this being essential.
 
The rest of this chapter contains a few more brief details
of the LKB system and how it has been developed.
The following chapter describes installation of the LKB.
The third chapter
is intended for first time users: it gives
a fairly detailed tour through some of the LKB's functionality.
The fourth chapter is a basic
and informal introduction to typed feature structures.
Chapter~5 contains a formal description
of typed feature structures as used in the LKB.
The subsequent three chapters are the reference manual proper --- they
are not intended to be read straight through, but to act as 
a source of information about error messages,
user interface commands which have less than
obvious behaviour and so on.
The first of these chapters
covers the
file organization, loading, error messages and so on.  The
next describes the user interface.  The last chapter
covers `advanced' features: generally speaking, this can be
ignored by people who are adapting an existing grammar.
The appendices cover details of the
available parameters and other details which most readers probably
do not wish to know about.

\section{Some history}

Initially the LKB was developed on the 
EU-funded ACQUILEX project primarily as a
tool to allow the construction of typed feature structure based lexicons
(both monolingual and bilingual).  
Although it incorporated a parser,
this was of secondary importance, since it was mostly intended as
a way of verifying lexical entries.  The most extensive documentation
of that version of the system is Copestake (1993): published
papers include Copestake (1992) and several papers in Briscoe et al (1993).
The current version of the LKB has 
been extensively updated.  Some of the main changes compared to the older
version are:
\begin{enumerate}
\item Extensive efficiency improvements, so the system is capable
of parsing reasonable length sentences with a large grammar.
\item Default unification is based on YADU, defined in Lascarides and
Copestake (in press).
\item Automatic computation of greatest lower bounds in the type hierarchy.
\item Integration with the \itsdb\footnote{This is really
what the system is called, but this is not
the responsibility of the current author.  Please direct any comments
about \itsdb\ to {\tt oe@coli.uni-sb.de}.}\ test suite machinery 
(Oepen and Flickinger, 1998).
\item Integration with MRS semantics (Copestake et al, 1997).
\item Tactical generation from MRS input (experimental). 
\item Many new user interface features.
\end{enumerate}
Some of the earlier functionality has not made it into the new version:
either because the developers consider it obsolete or simply because of
shortage of time.  

\section{Acknowledgments}
\label{ack}

The main developers of the LKB system 
are Ann Copestake, John Carroll
and Rob Malouf.  The distributed system also incorporates code
written by Bernie Jones, Stephan Oepen and   
John Bowler.  Ted Briscoe and Antonio Sanfilippo had a great 
deal of influence on the design of the original system,
Dan Flickinger played a similar role for the new version.
Invaluable input has also
been provided by users too numerous to thank individually, including
researchers on the ACQUILEX and LinGO projects and at
the Copenhagen Business School,
students
at the University of Cambridge Computer Laboratory and
Stanford University and students on a course taught by Copestake, Flickinger
and Oepen at ESSLLI-98.  Special thanks go to
Matt Kodama, Ryan Ginstrom, Christopher Callison-Burch, Scott Guffey
for their detailed comments on design and extensive testing.


\chapter{Installing and running the system}
\label{installation}

This chapter gives details of downloading, installing and 
starting up the LKB system.
CSLI users can skip to \S\ref{down-csli}.
Other users will need to look at the next section to decide in
what form to download the system, then go through the steps in the
other sections, depending on their 
choice.

\section{Hardware and software requirements}
\label{require}

The LKB system is freely available for teaching, research or commercial use,
though we retain copyright:
the conditions placed on redistribution are described on the 
\href{http://www-csli.stanford.edu/~aac/lkb.html}{LKB Web Site}.
We encourage users to make enhancements and upgrades to the system,
and will be happy to make any improvements available from the Web site.

The full LKB system is available in three configurations:
\begin{enumerate}
\item Solaris image
%\item Mac image
\item Source files for Allegro Common Lisp and CLIM
\item Source files for Macintosh Common Lisp (MCL)
\end{enumerate}
To use any version of the LKB, you need a machine with adequate amounts of memory.
We would suggest 32 Meg of RAM as a minimum on MCL, 
and 64 Meg for ACL with CLIM.
To run larger grammars
you will need larger amount of memory.  The size of the source
files is about 3.5 Meg.  The documentation is about 8 MBytes
(because of Postscript bitmaps).
The Solaris image should run on recent versions of SunOS.
%The Mac image will run on (FIX what?)
If you have Allegro Common Lisp with CLIM or Macintosh Common Lisp, you 
can download the source files
and compile them and/or make an image yourself.  This will be necessary
if you want to modify the LKB source code, and is likely to be
advantageous in any case, since we are likely to make minor bug fixes available
as source, rather than providing new images.
% UPDATE POINT
We have tested the system on Allegro Common Lisp 4.3 and 5.0 on SunOS 5.6
and on 
Macintosh Common Lisp 2.0.1, 3.9 and 4.0, under MacOS 7.1 and 7.5.

If you do not have access to a machine for which we provide an image,
and do not have either Allegro Common Lisp with CLIM or Macintosh Common
Lisp, the LKB can still be run in `tty' mode from other Common Lisps.
tty mode is also useful if you are accessing a machine over a slow network
where the graphical display would take too long.
In this mode, commands are entered by typing rather than menu selection,
and display commands result in ASCII output (or in some cases are
not available).
tty mode is described in detail in Appendix~\ref{tty}.

For up-to-date information of what is available, check the 
\href{http://www-csli.stanford.edu/~aac/lkb.html}{LKB Web Site}
You may want
to consider acquiring ACL/CLIM or MCL in order to run the source
--- see the links on the LKB web page.

\section{Downloading the LKB}
\label{down}

\subsection{Downloading source files}
\label{down-src}

The source files are distributed as a 
gzipped Unix tar file which is available from the
\href{http://www-csli.stanford.edu/~aac/lkb.html}{LKB Web Site}.
Download the file, making sure you are using binary mode.
Then:
\begin{verbatim}
gunzip lkbsrc5-0.tar.gz
\end{verbatim}
this should give you a file called {\tt lkbsrc5-0.tar}.  
Make sure this in a directory where you want the LKB system to reside
and extract the LKB by:
\begin{verbatim}
tar xf lkbsrc5-0.tar
\end{verbatim}
This will create the new directory and various subdirectories.
It may take a few minutes to do this.  At this point, you have 
your own local copy of the LKB system source.  Note that
you may need to change the directory locations in {\tt general/loadup}
and {\tt main/userfns}
to be appropriate for your system
(especially if you wish to run the system using MCL).


Now follow the instructions in \S\ref{acl-src} to compile the
files and optionally
make an image. 


\subsection{Downloading an image}
\label{down-images}

The image files for Sun Solaris are available from the
\href{http://www-csli.stanford.edu/~aac/lkb.html}{LKB Web Site}
as a gzipped Unix tar file.
Download the file, making sure you are using binary mode.
Then:
\begin{verbatim}
gunzip climimage5-0.tar.gz
\end{verbatim}
this should give you a file called climimage5-0.tar.
Put this in a directory where you want the LKB system to reside
and extract the LKB image directory by:
\begin{verbatim}
tar xf climimage5-0.tar
\end{verbatim}
This will create the new directory called {\tt lkb} containing
several files. 

Also download the data files, and unzip etc in the same way:
\begin{verbatim}
gunzip data5-0.tar.gz
tar xf data5-0.tar
\end{verbatim}
This will create a new directory tree called {\tt data} containing
various sample grammars.

Now follow the instructions in \S\ref{emacs-clim-image}
or \S\ref{clim-image}
%or \S\ref{mcl-image} 
to start the LKB.

\subsection{Getting a local copy of the LKB at CSLI}
\label{down-csli}

In order to get a local copy of the LKB at CSLI, you need to
checkout the latest version from the cvs system.  To do this,
you must have access to eoan or eo.  You will also need filespace
on these machines.

In the directory which you wish to act as the root for the LKB source
(your home directory is fine), execute the following:
\begin{verbatim}
cvs checkout newlkb
\end{verbatim}
This will create the new directory and various subdirectories.
It may take a few minutes to do this.  At this point, you have 
your own local copy of the LKB system.  Instructions on using
cvs are on the
\href{http://www.loria.fr/~molli/cvs/doc/cvs_toc.html}{CVS website}. 
Unless you are modifying the LKB
code yourself, the main command you need
to know about is \verb+cvs update+, which you should use
when someone modifies the code and you wish to update your local version.

Now follow the instructions in \S\ref{acl-src} to compile the
files, and in \S\ref{acl-emacs} to start the LKB system from within
emacs.


\section{Compiling the source files and making an image}
\label{src}

\subsection{Allegro Common Lisp and CLIM (ACL/CLIM)}
\label{acl-src}

To compile the LKB system files, and optionally make an image
in ACL/CLIM, do the following:
\begin{enumerate}
\item cd to \verb+newlkb/src+.  
\item Start ACL/CLIM.  The command needed will vary with your local
installation.  For instance,
at CSLI, the command to start ACL with CLIM is \verb+/usr/acl/bin/acl+.
\footnote{Note that since the objective is just to compile the system,
we describe the procedure for
starting the system from the command line.  For actually running the LKB,
it is preferable to start Lisp from within emacs, as described
in \S\ref{acl-emacs}.  If you prefer, you can follow the steps described
there, replacing the {\tt load-system} command with {\tt compile-system},
as below.}
\item At the Lisp prompt enter:
\begin{verbatim}
(load "general/loadup") 
\end{verbatim}
\item Once the loadup files have been loaded, enter the following:
\begin{verbatim}
(compile-system "lkb" :force t)
\end{verbatim}
This compiles and loads all the LKB files that are appropriate for
your system.\footnote{There may be
some warning messages --- these can usually be ignored, but if you subsequently
have problems using the LKB, you should redo this process
and examine the warning messages.}
You
should now see the LKB interaction window, as shown in 
Figure~\ref{lkbtop-unix}.
\item At this point, you may simply exit Lisp:
you can restart the system from the compiled files
as described in \S\ref{acl-emacs}.
Alternatively, if you want to make a Lisp image, execute \verb+(dump-lkb)+.
This simplifies and speeds up starting the LKB slightly, at the cost of
using a lot of file space.
On the whole, you should only do this if you don't expect
frequent changes in the code or if you are setting up the LKB for
a group of users who won't be modifying the code.
See the instructions in \S\ref{dumplkb} for details.
The LKB system can be started from the saved image as described in 
\S\ref{emacs-clim-image}.
\end{enumerate}


\subsection{Macintosh Common Lisp (MCL)}
To compile the LKB system files, and optionally make an image
in MCL, do the following:
\begin{enumerate}
\item Double-click on the MCL icon to start MCL
\item If the LKB files are somewhere other than \verb+Macintosh HD:newlkb+
(e.g. because your hard disk is called something other than 
\verb+Macintosh HD+),
open the \verb+src:general:loadup.lisp+ file (Open is in the File menu 
in MCL) and edit the definition of \verb+%sys-home%+ to be appropriate
for your machine.  For instance, if your hard disk is called \verb+carrot+,
and you want to use the folder name \verb+lkb5.0+ rather than \verb+newlkb+,
the definition would look like:
\begin{verbatim}
(defparameter %sys-home%
  #-:mcl (rest (butlast (pathname-directory *load-truename*) 2))
  #+:mcl '("carrot" "lkb5.0"))
\end{verbatim}
Save the file.
You may also need to edit the file \verb+src:main:user-fns.lsp+ to
change the function definition of lkb-tmp-dir which refers to
\verb+Macintosh HD+.  These are the only places where you should need to
change file names.
\item Use the Load menu option to load \verb+newlkb:src:general:loadup+
\item Once the loadup files have been loaded, enter the following into the
Listener window:
\begin{verbatim}
(compile-system "lkb" :force t)
\end{verbatim}
This compiles and loads all the LKB files that are appropriate for
your system.\footnote{There may be
some warning messages --- these can probably be ignored, but if you subsequently
have problems using the LKB, you should examine the
warning messages.}
You
should now see LKB on the menu-bar.
\item
At this point, you may simply exit Lisp:
you can restart the system from the compiled files
as described in \S\ref{mcl-comp}.
Alternatively, if you want to make a Lisp image, execute \verb+(dump-lkb)+.
This simplifies and speeds up starting the LKB slightly, at the cost of
using a lot of file space.
On the whole, you should only do this if you don't expect
frequent changes in the code or if you are setting up the LKB for
a group of users who won't be modifying the code.
See the instructions in \S\ref{dumplkb} for details.
The LKB system can be started from the saved image as described in 
\S\ref{mcl-image}.
\end{enumerate}


\section{Running the LKB from compiled files}
\label{compiled}


\subsection{ACL/CLIM with emacs}
\label{acl-emacs}

To use the LKB commands for interacting with source
files in ACL/CLIM, you must use the emacs text editor and
run a command that allows the linkage.  We suggest
the use of {\tt fi:common-lisp} which is available
from \href{ftp: //ftp.franz.com/pub/emacs/eli/eli-2.0.21.16.2/}{ftp: //ftp.franz.com/pub/emacs/eli/eli-2.0.21.16.2/}.  The following
instructions assume that this is installed on your system.
You can run the LKB from emacs in a shell using the standard
emacs commands, but the commands to show the grammar files
will not work.

\begin{enumerate}
\item The first time you use the system,
add the following lines to the bottom of your {\tt .emacs}
file, with the pathnames modified as appropriate for your
installation.
\begin{verbatim}

(if (not (member "/usr/acl/fi" load-path))
    (setq load-path (cons "/usr/acl/fi" load-path)))
(if (not (member "~/newlkb/src" load-path))
    (setq load-path (cons "~/newlkb/src" load-path)))
	
(load "fi-site-init")
(load "tdl-mode")
(load "lkb")
\end{verbatim}
\item The first time you run the system,
create a temporary directory (see \S\ref{tempdir}).  By default, this is
a directory called {\tt tmp} in your home directory.
\item Start emacs.  We will assume you
do this in the {\tt newlkb/src} directory.
\item Start ACL/CLIM from within emacs using the 
command \verb+fi:common-lisp+ which takes a number
of arguments. The
default values should be accepted with the exception
of the image name (the Lisp image appropriate for your
system should be specified), as follows:
\begin{quote}
\verb+M-x fi:common-lisp+\\
{\bf Buffer:} \verb+<RET>+\\
{\bf Host:} \verb+<RET>+\\
{\bf Process directory:} \verb+<RET>+\\
{\bf Executable image name:} \verb+/usr/acl/bin/acl+\\
{\bf Lisp image:}  \verb+<RET>+\\
{\bf Image arguments:}  \verb+<RET>+
\end{quote}
You should then get a new buffer called \verb+*common-lisp*+ with
a running Lisp process.  
\item At the Lisp prompt enter:
\begin{verbatim}
(load "general/loadup") 
\end{verbatim}
\item Once the loadup files have been loaded, enter the following:
\begin{verbatim}
(load-system "lkb")
\end{verbatim}
(If you haven't been following the instruction exactly, you may be asked whether
you want to compile some or all of these files.  Respond {\tt y} to this
question.)
\end{enumerate}
You
should now see the LKB interaction window, as shown in 
Figure~\ref{lkbtop-unix}.
To continue, see Chapter~\ref{firstsession}.
\begin{figure}
\epsfbox[0 0 400 100]{figs/lkbtop-unix.ps}
\caption{The LKB interaction window in ACL/CLIM}
\label{lkbtop-unix}
\end{figure}

\subsection{ACL/CLIM without emacs}
\label{acl-comp}

If for some reason you cannot or do not wish to use
the LKB from emacs, you can start the LKB from compiled files as follows:
\begin{enumerate}
\item The first time you run the system,
create a temporary directory (see \S\ref{tempdir}).  By default, this is
a directory called {\tt tmp} in your home directory.
\item cd to \verb+newlkb/src+.  
\item Start ACL/CLIM as in \S\ref{acl-src}
\item At the Lisp prompt enter:
\begin{verbatim}
(load "general/loadup") 
\end{verbatim}
\item Once the loadup files have been loaded, enter the following:
\begin{verbatim}
(load-system "lkb")
\end{verbatim}
\end{enumerate}
You
should now see the LKB interaction window, as shown in 
Figure~\ref{lkbtop-unix}.
To continue, see Chapter~\ref{firstsession}.

\subsection{MCL}
\label{mcl-comp}

To start the LKB from MCL with compiled files, proceed as follows:
\begin{enumerate}
\item Create a temporary directory (see \S\ref{tempdir}).
\item Start MCL (double-click on the MCL icon)
\item At the Lisp prompt enter:
\begin{verbatim}
(load "general/loadup") 
\end{verbatim}
\item Once the loadup files have been loaded, enter the following:
\begin{verbatim}
(load-system "lkb")
\end{verbatim}
\end{enumerate}
You
should now see the LKB interaction menu on the menu-bar.
To continue, see Chapter~\ref{firstsession}.

\section{Running the LKB from a saved image}
\label{images}


\subsection{ACL/CLIM from emacs}
\label{emacs-clim-image}
To use the LKB commands for interacting with source
files in ACL/CLIM, you must use the emacs text editor with 
a command that allows the linkage.  We suggest
the use of {\tt fi:common-lisp} which is available
from \href{ftp: //ftp.franz.com/pub/emacs/eli/eli-2.0.21.16.2/}{ftp: //ftp.franz.com/pub/emacs/eli/eli-2.0.21.16.2/}.  The following
instructions assume that this is installed on your system.
You can run the LKB from emacs in a shell using the standard
emacs commands, but the commands to show the grammar files
will not work.

\begin{enumerate}
\item The first time you use the system,
add the following lines to the bottom of your {\tt .emacs}
file, with the pathnames modified as appropriate for your
installation.
\begin{verbatim}

(if (not (member "/usr/acl/fi" load-path))
    (setq load-path (cons "/usr/acl/fi" load-path)))
(if (not (member "~/newlkb/src" load-path))
    (setq load-path (cons "~/newlkb/src" load-path)))
	
(load "fi-site-init")
(load "tdl-mode")
(load "lkb")
\end{verbatim}
\item The first time you run the system,
create a temporary directory (see \S\ref{tempdir}).  By default, this is
a directory called {\tt tmp} in your home directory.
\item Start emacs
\item Start the LKB from within emacs using the 
command \verb+fi:common-lisp+ which takes a number
of arguments.  If you have downloaded the image from our website
and stored it in the directory {\tt lkb}, accept all the default
suggestions, with the exception of the {\bf Executable image name:}
as follows:
\begin{quote}
\verb+M-x fi:common-lisp+\\
{\bf Buffer:} \verb+<RET>+\\
{\bf Host:} \verb+<RET>+\\
{\bf Process directory:} \verb+<RET>+\\
{\bf Executable image name:} \verb+/lkb/lkb+\\
{\bf Lisp image:}  \verb+<RET>+\\
{\bf Image arguments:}  \verb+<RET>+
\end{quote}
(if you are working with an image you have saved via {\bf Dump lkb}, 
specify the standard executable at
{\bf Executable image name:} and the image file at {\bf Lisp image:}).
You should then get a new buffer called \verb+*common-lisp*+ with
a running Lisp process (without a compiler, if you are using our
image).\footnote{If you get an error message concerning a missing library,
see \S\ref{problems}.}
\end{enumerate}
You
should now also see the LKB interaction window, as shown in 
Figure~\ref{lkbtop-unix}.
To continue, see Chapter~\ref{firstsession}.


\subsection{ACL/CLIM without emacs}
\label{clim-image}

If you cannot use emacs, or do not wish to for some reason,
simply start the saved image by running the executable
{\tt lkb} within the lkb directory (or whatever
other name you chose for it in the earlier stages).
You
should now see the LKB interaction window, as shown in 
Figure~\ref{lkbtop-unix}.\footnote{If 
you get an error message concerning a missing library,
see \S\ref{problems}.}
The first time you run the system, you should
create a temporary directory (see \S\ref{tempdir}).  By default, this is
a directory called {\tt tmp} in your home directory.
To continue, see Chapter~\ref{firstsession}.

\subsection{Macintosh}
\label{mcl-image}

If you are starting the LKB from a saved
Mac image, you simply double-click on the
icon for the image. 
You
should now see the LKB interaction menu on the menu-bar. 
The first time you run the system, you will need to
create a temporary directory (see \S\ref{tempdir}).
To continue, see Chapter~\ref{firstsession}.


\subsection{Temporary file locations}
\label{tempdir}

The LKB requires some temporary files be created for lexicon handling.  
Unfortunately, there is no way of ensuring that these will be created 
in a sensible place for every user on every Lisp system.
For
Unix, the default location for the
files is in a directory {\tt tmp} in the user's
home directory.  For MCL, the default location is {\tt Macintosh HD:tmp}.  If
it is possible to use these locations, then simply create the requisite
directory before loading the grammar.  If it is not convenient to have these
directories, if you are using source files, you can edit the file {\tt
main/user-fns.lsp} so that the function {\tt lkb-tmp-dir} identifies a more
suitable directory.  If you are using a supplied image file, you need to add a
{\tt lkb-tmp-dir} 
function to the {\tt user-fns} file to be loaded by every grammar 
you use.  For instance,
if your Mac has a disc called {\tt Applications}, add the following
to the {\tt user-fns} file (or replace {\tt lkb-tmp-dir} if it exists
already):
\begin{verbatim}
(defun lkb-tmp-dir nil 
  (let ((pathname  
          #-mcl (user-homedir-pathname)
          #+mcl (make-pathname :directory "Applications"))
        (tmp-dir '("tmp")))
    (make-pathname
     :host (pathname-host pathname) 
     :device (pathname-device pathname)
     :directory (append (pathname-directory pathname) tmp-dir)
     :name (pathname-name pathname) 
     :type (pathname-type pathname)
     :version (pathname-version pathname))))
\end{verbatim}



\section{Problems}
\label{problems}

Some of the likely error messages are covered in various places in this
documentation, so if you see something you don't understand, try searching for
significant word(s) from the error message.

In case of installation or other problems which don't seem to be covered in the
documentation, please look at the instructions on the
\href{http://www-csli.stanford.edu/~aac/lkb.html}{LKB Web Site}.  We will make
fixes for known problems available there.  The Web site also contains details
of how to report bugs that you find: note that it's important that you give us
enough information, so the Web site describes what information we need.

\subsection{Missing libraries}

When you start the LKB from a downloaded image, you may get an error message that
says a library is missing (e.g., {\tt libXm.so.3}).  The CLIM interface uses
Motif libraries and on some systems these are not made available by default.
You should be able to fix this by adding {\tt /usr/dt/lib} to your {\tt
LD\_LIBRARY\_PATH}: if this doesn't work (or if this explanation is
incomprehensible \ldots), please talk to your system administrator (it's a
standard difficulty with software that uses Motif).

\subsection{Compilation: file writing problems}

If you get an error while compiling the system to the effect that
a fasl file cannot be written, this is probably because a directory it
expects is missing.
The system assumes that there
is a directory tree named according to the type of system you have
(search for {\tt BINARY-DIR-NAME} in the {\tt general} files).
The source tree contains some empty fasl directory trees, but you
may have to construct you own.
If you don't do this in advance, Lisp will give error messages, but
you can continue from these after creating the files.


\subsection{Temporary directory missing}

One likely error message is something like the following:
\begin{verbatim}
Failure to open temporary lexicon file 
/user/bloggs/tmp/templex correctly.
Please create the appropriate directory 
if it does not currently exist or redefine
the user functions lkb-tmp-dir and/or
set-temporary-lexicon-filenames.  Then reload grammar.
\end{verbatim}
This is a problem with the temporary files: see \S\ref{tempdir}.

\chapter{A first session}
\label{firstsession}
The following chapter takes the new user through an initial session
with the LKB system.  It assumes that you have a running LKB system,
either using ACL/CLIM or MCL,
installed as described in the previous chapter.
It covers the basics of:
\begin{enumerate}
\item The LKB top menu
\item Loading an example grammar
\item Examining feature structures and type constraints
\item Parsing sentences
\item Adding a lexical entry
\item Adding a type with a constraint
\end{enumerate}
The intention is that this chapter should be useful
to readers who have no previous exposure to
typed feature structure formalisms as well as
to people who have some knowledge of typed feature structure
formalisms and who want an introduction to the LKB's user
interface and functionality.  But if you are in the first
category, you will probably find that you don't fully understand all the
terminology and notation used here.  A detailed introduction
is contained in Chapter~\ref{easy}, but we expect that for most people
that chapter will be easier to digest after this guided tour.

\section{The LKB top menu}
\label{topmenu}.

The main way of interacting with the LKB is through the LKB top menu.
The ACL/CLIM version is shown in Figure~\ref{lunix2}, the MCL version
is on the top menu-bar.
\begin{figure}
\epsfbox[0 0 400 100]{figs/lkbtop-unix.ps}
\caption{The LKB interaction window in ACL/CLIM}
\label{lunix2}
\end{figure}
The main difference between the Mac and ACL/CLIM versions of the
LKB is the appearance of the top-level interaction menu.
The Mac version makes use of the Mac main menu-bar, and adds an LKB
menu to that.  LKB error messages etc appear in the MCL
Listener window.  For the ACL/CLIM version, there is a distinct
top level interaction window, with the menu displayed using
buttons across the top of the window.  From now on,
we will ignore these differences.
We will use the term `LKB top menu' 
for the menu and `LKB interaction window' for the window in which messages 
appear
for both the Mac and CLIM versions.  The illustrations in 
the rest of this
document show the CLIM interface.

Note that, in the ACL version, it is possible (though 
a little difficult) for the interaction
window to be closed inadvertently.
It may be reopened by evaluating
(clim-user::restart-lkb-window) in Lisp (i.e., the
{\tt *common-lisp*} buffer in emacs).

\section{Loading a grammar}
\label{gramload}

The first step in this guided tour is to load an existing grammar: i.e.,
a set of files containing types and constraints, 
lexical entries, grammar rules etc.  
The LKB comes supplied with a series of grammars,
all of which are in the directory \verb+data+.  In this section, we
will assume that you are working with the smallest, which is called
`toy'.  

Select {\bf Complete grammar} from the LKB {\bf Load}
menu, and choose the file {\tt script} from the {\tt toy} directory
as shown in Figure~\ref{loadscript}.  The script file is responsible for
loading the remainder of the files into the system.  You should see various
messages appearing in the interaction window, as shown in
Figure~\ref{loadmess}
(the interaction window has been enlarged).  
If there are any errors in the grammar which the system
can detect at this point, error messages will be displayed in this window.
With the toy grammar, there should be no errors, unless there is a problem 
associated with the temporary directory (see \S\ref{tempdir}).
If
you get another error message when trying to load the {\tt toy} script, it is
possible you have selected another file instead of script --- try again.
\begin{figure}
\epsfxsize=3in
\epsfbox[0 0 400 500]{figs/loadscript.ps}
\caption{Selecting the script file in ACL/CLIM}
\label{loadscript}
\end{figure}
\begin{figure}
\epsfbox[0 0 400 250]{figs/loadmess.ps}
\caption{Loading a file}
\label{loadmess}
\end{figure}

Once a file is successfully loaded, the menu commands are all available
and a type hierarchy window is displayed (as shown in Figure~\ref{typehier}).
\begin{figure}
\epsfxsize=3in
\epsfbox[0 0 400 500]{figs/typehier.ps}
\caption{The type hierarchy window}
\label{typehier}
\end{figure}


\section{Examining feature structures and type constraints}

In this section we will go through some of the ways in which
you can look at the data structures in the grammar, such as the types,
type constraints,
lexical entries and grammar rules.

\subsection{The type hierarchy window}
The backbone of any grammar in the LKB is the type system,
which consists of a hierarchy of types, each of which has a
constraint which is expressed as a feature structure.
The type hierarchy allows for inheritance of constraints.
In the LKB system, the type hierarchy window is shown 
with the most
general type displayed at the left of the window.  In the toy grammar, 
this is {\bf *top*}.  
You will notice that there is some multiple inheritance in the hierarchy
(i.e., some types have more than one parent).

Click on the type {\bf ne-list}
which is a daughter of {\bf *list*}, which is a daughter of {\bf *top*},
and choose {\bf Expanded Type} from the menu.  A window will appear
as shown in Figure~\ref{ne-list}.
\begin{figure}
\epsfxsize=1in
\epsfbox[0 0 200 200]{figs/ne-list.ps}
\caption{An expanded type constraint window}
\label{ne-list}
\end{figure}
This window shows the constraint on type {\bf ne-list}: it has
two features, {\sc first} and {\sc rest}.  The value of
{\sc first} is {\bf *top*}, which indicates it can unify with any
feature structure.  The value of {\sc rest} is {\bf *list*}
which indicates it can only unify with something which is
of type {\bf *list*} or one of its subtypes.  {\bf *list*} and 
its daughters are important because they are used to implement
list structures which are found in several places in the grammar.

Look at the entry for the type {\bf ne-list} in the actual source
file {\tt toy/types.tdl} (i.e., open that file in emacs, if you are
running the ACL/CLIM version of the LKB, or in the MCL editor, if you are
running the MCL version).
\begin{verbatim}
ne-list := *list* &
 [ FIRST *top*,
   REST *list* ].
\end{verbatim}
The syntax of the language in which the type and its constraint
are defined (the
{\it description language}) is detailed in \S\ref{typesyntax}.
The type definition obligatorily
specifies the parent or parents of a type (in this case,
{\bf *list*}) and optionally gives a constraint definition.
In this particular case, the constraint described in the file corresponds
very closely to the expanded constraint shown in the
feature structure window, because the only parent of {\bf ne-list} is
{\bf *list*} and this does not have any features in its constraint.
However, in general, type constraints inherit a lot of information from
the ancestors of the type so the description of a constraint is very compact
compared to the expanded constraint.

To see a more complicated type constraint, 
click on {\bf grule} (grammar rule)
in the type hierarchy 
window (follow the path {\bf feat-struc}, {\bf synsem-struc}, {\bf phrase}
from {\bf *top*}).  The feature structure window is shown in
Figure~\ref{grule}.  {\bf grule} illustrates that types can have
complex constraints: that is the value of a feature in a constraint
can be a feature structure.  
\begin{figure}
\epsfxsize=1in
\epsfbox[0 0 200 400]{figs/grule.ps}
\caption{A complex type constraint}
\label{grule}
\end{figure}
Look at the definition of {\bf grule} in the source file:
\begin{verbatim}
grule := phrase &
 [ ARGS *list* ].
\end{verbatim}
In contrast to {\bf ne-list}, the constraint on
{\bf grule} has inherited a lot of
information from types higher in the
hierarchy: you can see how this inheritance
operates by looking at the constraints of {\bf grule}'s ancestors in the
type hierarchy window.

You will find that
you can click on the types within the feature structures to get menus
and also on the description at the top of the window
(e.g., {\tt grule - expanded}).
Details of the menu options in feature structure windows 
are given in \S\ref{activefs}.
More details of the type hierarchy window, including an explanation of all the
commands,
are given in \S\ref{thier}.

\subsection{The View commands}

The view commands let you get at entities such as lexical entries which cannot
be accessed from the type hierarchy window.  

Select {\bf View} from the LKB top menu and then select {\bf Word entries}.
You will be prompted for a word which corresponds to a lexical entry.
Enter {\tt kim} (case doesn't matter), deleting the default that
is specified (unless of course the default is {\tt KIM}, in which
case just select {\tt OK}).
You should get a feature structure window corresponding to the entry 
which has orthography ``kim'' in the toy grammar.
If there were multiple entries with the spelling `kim' they would all
be displayed. 

You can also access lexical entries by entering an identifier: to
do this you select {\bf View} {\bf Lex Entry}.  
To try this out, try entering {\tt kim\_1}.
An identifier will always
give a unique lexical entry.  

You might like 
to compare the feature structures shown with the actual lexical
entries in the file toy/lexicon.tdl, to see how the inheritance from type
constraints operates.  For instance, the actual entry for {\it Kim} is
simply:
\begin{verbatim}
kim_1 := pn-lxm & 
 [ ORTH <! "kim" !>,
   SEM [ RESTR <! [ NAME 'Kim ] !> ] ].
\end{verbatim}

Now try {\bf View} {\bf Grammar rule} and enter the identifier
{\tt head-specifier-rule}.  You will see the the grammar rule
is also a typed feature structure as shown in Figure~\ref{rulefs}.
We do not reproduce it in full here, because
it will not fit on one page, but the boxes indicate which parts of the
structure have been `shrunk' (this is done by clicking on a node,
and choosing the menu option {\bf Shrink/expand}).
\begin{figure}
\epsfxsize=4in
\epsfbox[0 0 400 800]{figs/rulefs.ps}
\caption{The head specifier rule}
\label{rulefs}
\end{figure}
A feature structure that encodes a rule can be thought of
as consisting of a number of `slots', into which the phrases for
the daughters and the mother fit.  To see how this works,
we will next describe how to parse a sentence.

\section{Parsing}

To parse a sentence, click on {\bf Parse} / {\bf Parse input}.
The default selection is {\tt Kim sleeps}.  Click {\tt OK} to
accept this default. You will get a window with one tiny parse
tree, as shown in Figure~\ref{kimsleeps}.\footnote{The reason for the
small size is that with non-trivial grammars and sentences, the parse 
trees can be very large and numerous \ldots} If you click on the parse
tree itself you will get a menu with an option to {\bf Show enlarged tree}.
If you choose this, you will see a window with a more readable version
of the tree, as shown in Figure~\ref{ptree}.
\begin{figure}
\epsfxsize=2in
\epsfbox[0 0 400 400]{figs/kimsleeps.ps}
\caption{The parse result window in ACL/CLIM}
\label{kimsleeps}
\end{figure}
\begin{figure}
\epsfxsize=2in
\epsfbox[0 0 400 400]{figs/ptree.ps}
\caption{A parse tree window}
\label{ptree}
\end{figure}

In a linguistic framework such as HPSG, a parse tree is just a convenient
user interface device, which is shorthand for a much larger typed
feature structure.  Click on the uppermost (S) node of the
enlarged parse tree and choose the option {\bf Edge 3}.  You will
see a large feature structure, which is shown in Figure~\ref{bigfs}.
As before, we have `shrunk' some parts of the structure so that
it can be displayed on the page.
This structure represents the entire
sentence.  It is an instantiation of the {\tt head-specifier-rule}
shown in Figure~\ref{rulefs}.
\begin{figure}
\epsfxsize=4in
\epsfbox[0 0 400 800]{figs/bigfs.ps}
\caption{A feature structure representing the sentence {\it Kim sleeps}}
\label{bigfs}
\end{figure}

Note that the mother node in the parse tree corresponds
to the root node of the feature structure shown in Figure~\ref{bigfs}.
The structure for the word {\it Kim} is the node which
is the value of the feature {\sc nh1}.  The structure for the
verb {\it sleeps} is the value of {\sc h}.
The parse trees are created from the feature structures
by matching these substructures against a set of node specifiers
defined in the file {\tt parse-nodes.tdl}.
If you now try parsing the sentence {\it Kim likes Sandy}
and examining the trees and the feature structures as before, you
will see that the daughter structures may themselves have daughters.
We will go into a lot more detail about how grammar rules work
in the next chapter.

Now try {\bf Parse} / {\bf Batch Parse}.  You will be prompted for the
name of a file which contains a test suite: i.e. a list of sentences
which either should or should not parse in a grammar.  A suitable file,
{\tt test.items}, already exists in the toy directory.  Select 
{\tt test.items} and enter the name of a new file for the output,
e.g., {\tt test.items.out}.  The output file shows the number of parses, if
any, and a time for the total processing.

Note that the toy grammar can parse exactly four 
sentences.  In the next sections, we'll expand that coverage slightly.


\section{Adding a lexical entry}

In this section and the next one, we illustrate the basics of how you can
modify an existing grammar.
In this section, we describe how to add a new lexical entry.\footnote{You 
may want to make a backup copy of the toy
directory at this point before you start editing the files.}
First open the file {\tt toy/lexicon.tdl} in your text 
editor.
You will see that
the toy grammar has four lexical entries: two proper names and two verbs.
Suppose you want to add another proper name,
perhaps your own.  The entry will look exactly like
the entry for {\it Kim}, but with the orthography and value
of the semantic feature {\sc name} replaced with your name.\footnote{If your name
has spaces in it, please write it without a space for now.  
The LKB can deal with words with spaces in them, see \S\ref{multiword},
but discussing this would take us too far afield at the moment.  If your name contains
diacritical marks etc, like \'{e}, then you can enter these 
using the normal Macintosh sequences 
in the MCL text editor.  For emacs, {\tt M-x iso-accents-mode} allows
you to enter accented characters.  
However, there are some problems --- see \S\ref{accents} for a more
detailed discussion.}
Add such an entry to the end of the file.
For instance:
\begin{verbatim}
ann_1 := pn-lxm & 
 [ ORTH <! "ann" !>,
   SEM [ RESTR <! [ NAME 'ann ] !> ] ].
\end{verbatim}

Save the file, and then select {\bf Load}
followed by {\bf Reload grammar}.  This will reload the script file that
you loaded before, this time loading the changed version of the lexicon.
You may get some error messages in the LKB interaction window
at this point, if you have left
out a character, for example.  If you cannot see what is wrong, the
description of the error messages in \S\ref{synwf} may help you
track it down.  When you have successfully reloaded the system,
you should be able to parse some additional sentences, such as:
\begin{verbatim}
Ann sleeps
\end{verbatim}
If you find that a sentence won't parse, and you do not understand
why a grammar rule is not applying to a particular lexical
item, there are some tools to help you.  In particular,
\S\ref{unifcheck} describes an interactive tool for 
showing why unification is failing.

Once you have successfully parsed some new sentences,
you could add them to the list of test sentences for batch parsing.

\section{Adding a type and its constraint}

The way the toy grammar is set up, although you could add a lexical
entry for a proper name by just adding a description to the lexicon file,
in order to add a new verb to the lexicon, you have to first add a type
representing its relation.  Suppose you want to add a new entry for
the intransitive verb {\it snores}.  First you have to add a type such as
{\bf snore\_rel}.  Open the file {\tt toy/types.tdl}.  Search the file for
the entry for the relation for sleeps, {\bf r\_sleep}.  Then add a new
entry for {\bf r\_snore}.  You can then save the file, and add a new entry
to the lexicon file.  As with the previous entry, you can basically do
this by copying the existing entry for {\it sleeps},
but this time you have to 
change the relation to be the type you have just added.
Change the orthography and the identifier for the entry as before.
Save the file.  Then select {\bf Reload grammar}.  As before, check
the LKB interaction window to make sure reloading was successful 
and try parsing
some new sentences.


\section{Next steps}

If you have worked through this chapter, and are a beginner to the use of typed
feature structure systems, we have now to admit that we've led you gently to
the edge of a small but steep cliff and are not going to provide much in the
way of climbing equipment.\footnote{There is the possibility of Copestake and
Flickinger (next millennium), which would be an introduction to linguistic
engineering using typed feature structure systems, and the LKB in particular.
If you think this would be a worthwhile enterprise, please send encouraging
email, cash donations etc to us at CSLI.}  
A very basic introduction to the notion of typed feature structure
grammars is in the next chapter.
The formal exposition of types and typed feature structures as used in the LKB
is in Chapter~\ref{formal}.  
This is designed for readers who have some familiarity
with feature structures and are happy with a fairly technical exposition.  
Many people will be happier investigating the properties of the
system informally first and then checking Chapter~\ref{formal} 
to understand the precise behaviour.  With
the exception of those chapters, the rest of this document is intended as a
reference manual.  

Ideally, once you understand the basics of the formalism to the level presented
in Chapter~\ref{easy}, you would investigate the capabilities of the LKB
further by working through some of the details of the example grammars, and try
extending them, first by adding lexical entries as we did in this chapter, and
then by extending the coverage by adding new lexical types.  You should refer
to the relevant sections in chapters~\ref{formal}, \ref{srcfiles} and \ref{ui}
when problems arise or when you need to know the details of some facility.
Then \S\ref{own}, gives some hints as to how to go about building your own
grammar.  Some of the advanced capabilities discussed in Chapter~\ref{advanced}
will become relevant at this point.

If you want to learn more about a
linguistic framework that uses the typed feature
structure formalism, then Sag and Wasow (1999) is
an introductory textbook.  Some of the grammars provided with the LKB system
implement the grammars described there.

\chapter{To be added}
\label{easy}

\chapter{Feature structures and types in the LKB system}
\label{formal}
% Note to self - the way to handle this stuff in the textbook would be
% to put this in an appendix.
% For now, it stays in the main text, so that it can be cross referenced
% from other places.

This chapter contains a (fairly) formal description of the notion of
feature structures and types used in the LKB system.\footnote{This chapter is based
on earlier documents, in particular Copestake (1993).  
Defaults are not discussed here, see \S\ref{defaults}.}
It also defines the description language used in the TDL compatibility mode. 
The following chapter gives details of how the descriptions are
evaluated and checked by the system, including error messages and so on.
 
It is intended to be comprehensible
to someone who has a knowledge
of feature structures, as used in PATR, for instance, and some
formal background.  The LKB system
uses typed feature structures, but we start off by giving definitions
for untyped feature structures, since the typed feature structure
definitions will build on these.

\section{Untyped feature structures}
\label{extvsint}

In this section we briefly recapitulate 
some of the basic definitions of untyped feature structures, including
subsumption, unification and generalisation.  
We will use a notation in which more specific entities
(feature structures and, later on, types) are uniformly described as
being lower in some hierarchy.  (This is the opposite direction
to that used by Shieber (1986) and Carpenter (1992).)

A feature structure is either basic, in which case it must
be an {\em atomic value}, or it is complex, in which case it
provides values for one or more {\em feature}s.  
These values in turn are either atomic or complex feature structures.
The set \dom{Feat} of features and 
set \dom{Atom} of atomic values are assumed to be finite.
Feature structures can be defined in terms of
labelled finite-state automata
following Kasper and Rounds (1986, 1990).  The version of the
definitions given here follows Carpenter (1993).    
%
\begin{definition}[Feature Structure]
A feature structure is a tuple $F =
\tuple{Q,q_{0},\delta,\alpha}$ where:
\begin{itemize}
\item
$Q$:  a finite set of nodes 
\item
$q_{0} \in Q$: the root node
\item
$\fcn{\delta}{\dom{Feat} \times Q}{Q}$:  
the partial feature value function (transition function)
\item
$\fcn{\alpha}{Q}{\dom{Atom}}$: the partial atomic value function
\end{itemize}
with the following constraints:
\begin{description}
\item [Connectedness] 
There must be some {\em path} from the root to every node.\\
Paths are defined as sequences of zero or more features.
Let $\dom{Path} = \kleene{\dom{Feat}}$ and $\epsilon$ be the empty path.  
The definition of the transition function can be extended to paths by taking
$\delta(\epsilon,q) = q$ and $\delta(f\cdot\pi, q) =
\delta(\pi,\delta(f,q))$.  Then the connectedness condition
can be expressed as stating that there must be a path 
$\pi$ such that $q = \delta(\pi,q_{0})$ for all $q \in Q$
\item [Atomic values]
Only nodes without features can be atomic values, so that if
$\alpha(q)$ is defined then $\delta(f,q)$ is undefined for every $f
\in \dom{Feat}$
{\rm (But note that some nodes without
features may not have values.)}
\item [Acyclicity] The resulting graph is acyclic in that
there is no path $\pi$ and non-empty path $\pi'$ such that
$\delta(\pi,q_{0}) = \delta(\pi \cdot \pi',q_{0})$. \\
{\rm (Although it is not formally 
necessary to rule out cycles in untyped or typed
feature structures, doing so makes some definitions 
simpler and the implementation more straightforward and efficient.)}
\end{description}
\end{definition}

\subsection{Subsumption}
\label{untsub}

Feature structures can be regarded as being ordered by information
content --- a feature structure is said
to {\em subsume} another if the latter carries extra information.
Subsumption can be formalised by assuming that
a pair of sets is associated with each feature structure
which determine the path equivalences that hold and the
atomic values assigned to paths.  
Let $\equiv_{F}$ be the equivalence relation induced
between paths by the structure sharing in $F$ and ${\cal P}_{F}$ be
the partial function induced by $F$ which maps paths in $F$ to atomic
values. 
%
\begin{definition}[Abstract Feature Structure]
If $F = \tuple{Q,q_{0},\delta,\alpha}$ is a feature structure, we let
$\equiv_{F} \; \subseteq \dom{Path} \times \dom{Path}$ and
$\fcn{{\cal P}_{F}}{\dom{Path}}{\dom{Atom}}$ be such that:
\begin{itemize}
\item
(Path Equivalence) \\
$\pi \equiv_{F} \pi'$ 
if and only if $\delta(\pi,q_{0}) = \delta(\pi',q_{0})$
\item
(Path Value) \\
${\cal P}_{F}(\pi) = \sigma$ 
if and only if
$\alpha(\delta(\pi,q_{0})) = \sigma$.
\end{itemize}
The pair $\tuple{{\cal P},\equiv_{F}}$ is called the abstract
feature structure corresponding to $F$.
\end{definition}
%

A feature structure $F$ subsumes another feature
structure $F'$ if and only if the information in $F$ is contained in
the information in $F'$;  that is, if $F'$ provides at least as much
information about path values and structure sharing as $F$.  The
abstract feature structure corresponding to $F$ is sufficient to
determine its information content and thus subsumption.
%
\begin{definition}[Subsumption]
$F$ subsumes $F'$, written $F' \subsumedby F$, iff:
\begin{itemize}
\item if $\pi \equiv_{F} \pi'$ then $\pi \equiv_{F'} \pi'$
\item if ${\cal P}_{F}(\pi) = \sigma$ then ${\cal P}_{F'}(\pi) = \sigma$
\end{itemize}
\end{definition}
%
Thus $F$ subsumes $F'$ if and only if every piece of information in
$F$ is contained in $F'$.   For untyped feature structures,
$\top$ is defined to be the single node feature structure with no atomic
value assigned.  Thus $\pi \equiv_{\top} \pi'$ if and only if $\pi =
\pi' = \epsilon$ and ${\cal P}(F)$ is undefined everywhere.  Note that
$F \subsumedby \top $ for every feature structure $F$.  $\top$ is used
in attribute-value matrices to denote the lack of
any known value.

\subsection{Unification}
\label{untunif}

Unification corresponds to conjunction of information, and thus
can be defined in terms of subsumption, which is a relation of
information containment.  The unification of two feature structures is
defined to be the most general feature structure which contains all
the information in both of the feature structures.  
%
\begin{definition}[Unification]
The unification $F \unify F'$ of two feature structures $F$ and $F'$ is
taken to be the greatest lower bound of $F$ and $F'$ in the collection of
feature structures ordered by subsumption.  
\end{definition}
%
Thus $F \unify F' = F''$ if and only if
$F'' \subsumedby F$, $F'' \subsumedby F'$ and for every $F'''$ such
that $F''' \subsumedby F$ and $F''' \subsumedby F'$ it 
is also the case that $F'''
\subsumedby F''$.  

\subsection{Generalisation}
\label{untgen}

Generalisation is the opposite of unification in which the
lowest upper bound of two feature structures is taken.
The generalisation of two feature structures is defined
to be the most specific feature structure which contains only
information found in both feature structures.
%
\begin{definition}[Generalisation]
The generalisation $F \generalize F'$ of two feature structures is
defined to be their greatest lower bound in the subsumption ordering. 
\end{definition}
Thus $F \generalize F' = F''$ if and only if
$F \subsumedby F''$, $F' \subsumedby F''$ and for every $F'''$ such
that $F \subsumedby F'''$ and $F' \subsumedby F'''$ then $F''
\subsumedby F'''$.
Although unification corresponds to
conjunction, generalisation does not correspond to disjunction 
but is more like information
intersection.  In systems which support it,
disjunction of feature structures will give 
a result which is either more specific than that produced by
generalisation (defined as an operation which produces a non-disjunctive
feature structure)
or equal to it.

\section{Typed feature structures}
\label{types}

A type system
can be described as having three components; a fixed finite set of features
$\dom{Feat}$, a fixed type hierarchy $\tuple{\dom{Type}, \subsumedby}$
with a finite set of
types, and a constraint function $C$ which associates a constraint
feature 
structure with
every type.
The constraints determine which feature structures are {\em well-formed}.
The definition of types in the LKB system follows Carpenter (1992)
quite closely, although the type constraints are somewhat different.

\subsection{The type hierarchy}
\label{formthier}

The type hierarchy $\tuple{\dom{Type}, \subsumedby}$
defines a partial order (notated $\subsumedby$,
``is more specific than'')
on the types and specifies
which types are {\em consistent}.
A set of types is said to be consistent if the members of the 
set share a common subtype.
That is the subset $S\subseteq {\sf TYPE}$ is
consistent
iff there is some $t_{0}$ in  ${\sf TYPE}$ such that 
$t_{0} \sqsubseteq t $ for any $t$ in $S$.
The significance of this is that only feature structures with
mutually consistent types can be unified.  Two types which are
unordered in the hierarchy are assumed to be inconsistent
unless a common subtype has been explicitly specified (this is sometimes
referred to as a {\em closed world assumption}).
It is a condition on the type hierarchy that
every {\em consistent} set of types  $S\subseteq {\sf TYPE}$
must have a unique greatest lower bound or
meet (notation  $\meet S$). 
The unique greatest lower bound condition on consistent types
allows feature structures
to be typed deterministically; if two feature structures of types
{\bf a}
and {\bf b} are unified the type of the result will be 
${\bf a} \meet {\bf b} $,
which must
be unique if it exists.  If ${\bf a} \meet {\bf b}$ 
does not exist unification fails.  Thus, in the fragment of a type
hierarchy shown in Figure~\ref{itypehier}, {\bf intrans} and 
{\bf verb\_syn} are
consistent; ${\bf intrans} \meet {\bf verb\_syn} = {\bf
intrans\_verb}$.
We will use a very simple type system at this point for
ease of exposition.
\setlength{\unitlength}{1.2in}
\begin{figure}
\begin{center}
\begin{picture}(4, 1.7)(1.5, 1.4) 
\put(3,3.0){\makebox(0,0){\bf $\top$}}
\put(2.9,2.9){\line(-2, -1){0.6}}
\put(3.1,2.9){\line(2, -1){0.6}}
\put(2,2.5){\makebox(0,0){\bf sign}}
%\put(1.9,2.4){\line(-2, -1){0.6}}
%\put(1,2.0){\makebox(0,0){\bf lex-sign}}
%\put(0.9,1.9){\line(-2, -1){0.6}}
%\put(0,1.5){\makebox(0,0){\bf lex-noun-sign}}
\put(4,2.5){\makebox(0,0){\bf syn}}
\put(3.9,2.4){\line(-2, -1){0.6}}
\put(4.1,2.4){\line(2, -1){0.7}}
\put(5,2){\makebox(0,0){\bf verb\_syn}}
\put(4.9,1.9){\line(-2, -1){0.6}}
\put(3,2){\makebox(0,0){\bf intrans}}
\put(2.9,1.9){\line(-2, -1){0.6}}
\put(2,1.5){\makebox(0,0){\bf intrans\_adj}}
\put(3,1.9){\line(0, -1){0.3}}
\put(3,1.5){\makebox(0,0){\bf card\_num}}
\put(3.1,1.9){\line(2, -1){0.6}}
\put(4,1.5){\makebox(0,0){\bf intrans\_verb}}
\end{picture}
\caption{A fragment of a type hierarchy}
\label{itypehier}
\end{center}
\end{figure}

Because the type hierarchy is a partial order 
it has properties of
reflexivity, transitivity and
anti-symmetry (from which it follows that the type hierarchy cannot
contain cycles).
Note that the empty set is (vacuously) consistent, as
for  any $t_{0}$ in ${\sf TYPE}$ it satisfies the condition 
that $t_{0}\sqsubseteq t$ for
all $t$'s in the empty set. 
The maximal element $\top$  of $\langle {\sf TYPE},\sqsubseteq\rangle$ 
is defined as the meet of the empty  set, 
$\top=\sqcap \emptyset$. This element  $\top$ is 
such that $t\sqsubseteq \top$ for any $t$ in 
${\sf TYPE}$.  Thus the join operation on the type hierarchy is total.

The  meet 
$\sqcap$ operation can be made total by adding
the join of the empty-set $\bot=\sqcup\emptyset$ to 
$\langle {\sf TYPE},\sqsubseteq\rangle$.
But even if $\bot$ is added to make
$\langle {\sf TYPE},\sqsubseteq\rangle$ a lattice, 
this lattice need not be distributive.
For example, given the type hierarchy in Figure~\ref{itypehier}
$({\rm \bf intrans\_adj} \join {\rm \bf card\_num}) 
\meet {\rm \bf verb\_syn} 
= {\rm \bf intrans\_verb}$ 
but $({\rm \bf intrans\_adj} \meet {\rm \bf verb\_syn}) 
\join ({\rm \bf card\_num} \meet {\rm \bf verb\_syn}) = \bot$.
Although meet will be taken as corresponding to conjunction, 
join does not correspond
to disjunction.  
There is thus a symmetry with the behaviour
of feature structures, 
where generalisation and disjunction are not equivalent.
Nothing in what follows depends on whether the existence of a bottom
element in the
type hierarchy, $\bot$, is assumed or not.  We use $\bot$ as a notational
convenience to indicate inconsistency, just as $\bot$ is used to 
indicate unification
failure for feature structures.

\subsection{Typed feature structures}
\label{formtfs}

Initially we will 
define the set $\cal F$ of typed feature structures to include 
both those which are, and those which are not, well-formed with respect
to a particular type system.
A typed feature structure is defined as a tuple 
$F = \langle Q, q_{0}, \delta, \alpha \rangle$ 
where the only significant difference from the definition in the
untyped case 
is that instead of $\alpha$ being a partial atomic value function it is
a total node typing function, $\alpha\colon Q \rightarrow {\sf TYPE}$.  
Thus every node has a type.
\begin{definition}[Typed Feature Structure]
A typed feature structure is a tuple $F =
\tuple{Q,q_{0},\delta,\alpha}$ where:
\begin{itemize}
\item
$Q$:  a finite set of (connected, acyclic) nodes 
\item
$q_{0} \in Q$: the root node
\item
$\fcn{\delta}{\dom{Feat} \times Q}{Q}$:  
the partial feature value function
\item
$\fcn{\alpha}{Q}{\dom{Type}}$: the total node typing function
\end{itemize}
with connectedness and acyclicity being defined as in the
untyped case.
\end{definition}

Consider the following example of a feature structure:
\begin{center}
{\tiny
   $F_{1}=\avmplus{\att{sign}\\
             \attval{SEM}{\avmplus{\att{vsem}\\
                                   \attvaltyp{RELN}{r\_die}\\
                                   \attvaltyp{CORPSE}{index}}}}$}
\end{center}
As every feature structure has a unique initial node, $q_{0}$, the
type of a feature structure can be said to be
the type of its initial node, that is: 
\begin{definition}[Type of a feature structure]
If $F = \langle Q, q_{0}, \delta, \alpha \rangle$ 
then $Typeof(F) = \alpha(q_{0})$. 
\end{definition}
Thus the type of feature structure $F_{1}$ is $\bf sign$.  

The definition of subsumption of typed feature structures 
is very similar to that for untyped feature structures, with the
additional proviso that the ordering
must be consistent with
the ordering on their types.  The symbol
$\subsumedby$
(``is-more-specific-than'', ``is-subsumed-by'')
is overloaded
to express subsumption of feature structures as well as the ordering
on the type hierarchy.
Thus
if $F_{1}$ and $F_{2}$ are feature structures of types $t_{1}$ and $t_{2}$
respectively, then $F_{1} \subsumedby F_{2}$ only if $t_{1} 
\subsumedby t_{2}$.   
If two feature structures are identical except
for their types then the subsumption ordering on the feature
structures will be equivalent to the ordering on their types.
\begin{definition}[Abstract Typed Feature Structure]
If $F = \tuple{Q,q_{0},\delta,\alpha}$ is a feature structure, we let
$\equiv_{F} \; \subseteq \dom{Path} \times \dom{Path}$ and
$\fcn{{\cal P}_{F}}{\dom{Path}}{\dom{Type}}$ be such that:
\begin{itemize}
\item
(Path Equivalence) \\
$\pi \equiv_{F} \pi'$ 
if and only if $\delta(\pi,q_{0}) = \delta(\pi',q_{0})$
\item
(Path Value) \\
${\cal P}_{F}(\pi) = t$ 
if and only if
$\alpha(\delta(\pi,q_{0})) = t$.
\end{itemize}
\end{definition}
%
\begin{definition}[Subsumption of typed feature structures]
$F$ subsumes $F'$, written $F' \subsumedby F$, iff:
\begin{itemize}
\item if $\pi \equiv_{F} \pi'$ then $\pi \equiv_{F'} \pi'$
\item if ${\cal P}_{F}(\pi) = t$ then ${\cal P}_{F'}(\pi) = t'$ where $t' \subsumedby t$
\end{itemize}
\end{definition}
%

Unification of typed feature structures is defined 
in the same way as for untyped feature structures, that is
the unification of two typed feature structures
will be their greatest lower bound in the subsumption ordering.
Since if $F$ and $F'$ are feature structures of types $t$ and $t'$
respectively, their unification $F \sqcap F'$ has to have
type $t \sqcap t'$, unification will fail if
$t \sqcap t'$ does not exist.  
Generalisation can also be defined in the same way
as for untyped feature structures, that is as the lowest upper bound
of two 
feature
structures in the subsumption ordering.

Thus the type hierarchy makes a finer grained differentiation of 
feature structures possible, because the ordering is dependent on the
type hierarchy as well as the order of untyped 
feature structure subsumption. 
For example, if $F_{1}$ and $F_{2}$ are the two feature structures given
below, then $F_{2} \ssubsumedby F_{1}$ assuming that {\bf obj}
$\ssubsumedby$ {\bf index}:
\begin{center}
{\tiny
$F_{1}=\avmplus{\att{sign}\\
             \attval{SEM}{\avmplus{\att{vsem}\\
                                   \attvaltyp{RELN}{r\_die}\\
                                   \attvaltyp{CORPSE}{index}}}}$
$F_{2}=\avmplus{\att{sign}\\
             \attval{SEM}{\avmplus{\att{vsem}\\
                                   \attvaltyp{RELN}{r\_die}\\
                                   \attvaltyp{CORPSE}{obj}}}}$

}
\end{center}
This is in itself 
useful; for example it allows selectional restrictions to be encoded as 
sorts in a way which is extremely cumbersome with untyped feature
structures
(cf. Moens {\it et al.}, 1989).
However the type system also specifies a set of
{\em well-formed} feature structures, which 
satisfy a constraint function, and this gives the system a functionality
which includes the properties of inheritance, error-checking and 
classification.

\subsection{Constraints}
\label{formcons}

The constraint function $C$ defines the set of feature structures ${\cal WF}$
which are well-formed with respect to the type system.
In the LKB system every type must have exactly one associated
feature structure which acts as a constraint on all feature structures
of that type by subsuming all well-formed feature structures of that
type.  The constraint also defines which features are {\em
appropriate} for a particular type. In a well-formed feature structure 
each node must have
all the features appropriate to its type and no others.  Constraints 
are inherited by
all subtypes of a type, but a constraint on a 
subtype may introduce new features
(which will be inherited as appropriate features by all its subtypes).
A constraint on a type is a well-formed feature structure of that
type; all constraints must therefore be mutually consistent.

For example, considering some of the types shown in Figure~\ref{itypehier},
the constraint associated with the
type {\bf verb\_syn} might be:
\begin{center}
{\tiny
   $\avmplus{\att{verb\_syn}\\
             \attval{HEAD}{\avmplus{\att{head}\\
                                  \attvaltyp{POS}{verb}\\
                                  \attvaltyp{INV}{boolean}}}}$}
\end{center}
This constraint states that
any feature structure of type {\bf verb\_syn} 
must have a feature structure of type {\bf head} as the value for
its {\sc head} feature.  This constraint must be
consistent with the constraints on all of its subparts, i.e. it must itself
be a well-formed feature structure.
In this case, for example, the constraint on
{\bf head} is: 
\begin{center}
{\tiny
$\avmplus{\att{head}\\
          \attvaltyp{POS}{pos}\\
          \attvaltyp{INV}{boolean}}$}
\end{center}
and thus the constraint on {\bf verb\_syn} is well-formed, given that 
{\type verb} $\subsumedby$ {\type pos}.
Here {\type verb},
{\bf pos} and {\bf boolean} are atomic types, and have no appropriate
features.  So, for example, the constraint on {\bf pos}
is simply the atomic feature structure
[{\bf pos}].


The type {\bf intrans} might have the constraint:
\begin{center}
{\tiny
   $\avmplus{\att{intrans}\\
             \attval{VAL}{\avmplus{\att{val}\\
                                   \attval{SPR}{\avmplus{\att{ne-list}\\
                                             \attvaltyp{HD}{np}\\
                                             \attvaltyp{TL}{e-list}}}\\
                                   \attvaltyp{COMPS}{e-list}}}}$}
\end{center}
(Note: we will pretend for the sake of this example that
{\bf np} is an atomic type.  Of course this wouldn't 
really work, {\bf np} would
have to be a subtype of {\bf sign} with a constraint such that the
value of
{\sc pos} was {\bf noun}
and so on.)

The constraint on {\bf
intrans\_verb} will contain information inherited from both parents,
thus:
\begin{center}
{\tiny
   $\avmplus{\att{intrans\_verb}\\
             \attval{HEAD}{\avmplus{\att{head}\\
                                  \attvaltyp{POS}{verb}\\
                                  \attvaltyp{INV}{boolean}}}\\
             \attval{VAL}{\avmplus{\att{val}\\
                                   \attval{SPR}{\avmplus{\att{ne-list}\\
                                             \attvaltyp{HD}{np}\\
                                             \attvaltyp{TL}{e-list}}}\\
                                   \attvaltyp{COMPS}{e-list}}}}$}
\end{center}

Given that {\bf true} is an atomic subtype of {\bf boolean},
the feature structure below is
well-formed since it
contains all the 
appropriate features and no inappropriate ones,
it is subsumed by the constraints on
its type and all its substructures are well-formed.
\begin{center}
{\tiny
   $\avmplus{\att{intrans\_verb}\\
             \attval{HEAD}{\avmplus{\att{head}\\
                                  \attvaltyp{POS}{verb}\\
                                  \attvaltyp{INV}{true}}}\\
             \attval{VAL}{\avmplus{\att{val}\\
                                   \attval{SPR}{\avmplus{\att{ne-list}\\
                                             \attvaltyp{HD}{np}\\
                                             \attvaltyp{TL}{e-list}}}\\
                                   \attvaltyp{COMPS}{e-list}}}}$}
\end{center}

Formally, the notion of appropriate
features is defined as follows:  
\begin{definition}[Appropriate features]
If $C(t)=\langle Q,q_{0},\delta,\alpha\rangle$
then the appropriate features of $t$ are defined as 
$Appfeat(t)= Feat(\tuple{F, q_{0}})$
where $Feat(\tuple{F, q})$ is defined to be the set of features labelling
transitions from the node $q$ in some feature structure $F$
i.e. $f \in Feat(\tuple{F, q})$ 
such that $\delta(f, {q})$ is defined.
\end{definition}

We can then define the constraint function:
\begin{definition}[Constraint function] The constraint function which
associates
constraint feature structures with types is given by
$C\colon \langle {\sf TYPE},\sqsubseteq\rangle\to {\cal F}$.\\
This must satisfy the following conditions:
\begin{description}
\item[Monotonicity] Given types  $t_{1}$ and $t_{2}$  if
$t_{1} \sqsubseteq t_{2}$ then $C(t_{1}) \sqsubseteq C(t_{2})$
\item[Type] For a given type $t$, if $C(t)$ is the feature structure
$\langle Q,q_{0},\delta,\alpha\rangle$ then 
$\alpha({q_{0}})=t$.
\item[Compatability of constraints] 
For all $q \in Q$ the feature structure
$F'= \langle Q',q,\delta,\alpha\rangle 
\sqsubseteq C(\alpha(q))$ and $Feat(q) = Appfeat(\alpha(q))$.

\item[Maximal introduction of features] 
For every feature $f \in {\sf FEAT}$
there is a unique type $t = Maxtype(f)$ such that 
$f \in Appfeat(t)$ 
and there is no type $s$ such that  $t \sqsubset s$
and $f \in Appfeat(s)$.
The maximal appropriate value of a feature $Maxappval(f)$ is the type $t$
such that if $C(Maxtype(f))= \langle Q,q_{0},\delta,\alpha\rangle$ then 
$t=\alpha (\delta (f, {q_{0}})) $
\end{description}
\end{definition}

The compatibility condition implies that no constraint feature
structure $C(t)=F$ can strictly contain a feature structure of type $t$
or any subtype of $t$.  That is, if $F$ is given by 
$\langle Q,q_{0},\delta,\alpha\rangle$, then for all non-initial nodes
$q\in Q$ such that $q \neq {q_{0}} $ the type of the node $\alpha(q) \not\sqsubseteq t$.
If such a node existed it would have to be the initial node of a
feature structure $F_q$ which was more specific than $F$, 
i.e.~$F_q \sqsubseteq F$, and would therefore itself have to contain such
a node, and so on.  Thus such a constraint could only be satisfied by
a cyclic or infinite structure, and we disallow both of these possibilities.
This condition does not, however, rule out recursive structures such
as lists, because the type {\bf list} can be defined to have two
subtypes {\bf e-list} (empty list) and {\bf ne-list}, where the former
has no appropriate features and the latter has two, {\sc hd} which
can take any value, and {\sc tl}, which will take a value of type
{\bf list}.  The type {\bf ne-list} does not violate the
compatibility condition, since the structure can be terminated.

\begin{definition}[Well-formed feature structures]
We say that a given feature structure 
$F=\langle Q,q_{0},\delta,\alpha\rangle$ is a
well-formed feature structure
iff for all $q \in Q$, we have that 
$F'= \langle Q',q,\delta,\alpha\rangle 
\sqsubseteq C(\alpha(q))$ and $Feat(q) = Appfeat(\alpha(q))$.
\end{definition}
From these definitions it can be seen that all constraint feature
structures are themselves well-formed feature structures.

Since the type system gives a concept of a well-formed feature structure,
it follows that non-well-formed feature structures can be detected, allowing 
error checking.
Typing also allows for a form of classification; a feature may only be 
introduced as appropriate at one point in the type hierarchy (and will be
inherited as an appropriate feature by all subtypes of that type); it follows 
from this that there is a unique maximal type for any set of features, and 
therefore an untyped feature structure can always be typed deterministically.
For example, assuming the type system introduced above, the 
description:
\begin{verbatim}
 [ HEAD [ INV true ],
   VAL  [ COMPS e-list ] ].
\end{verbatim}
would give rise to the following feature structure:
\begin{center}
{\tiny
   $\avmplus{\att{intrans\_verb}\\
             \attval{HEAD}{\avmplus{\att{head}\\
                                  \attvaltyp{POS}{verb}\\
                                  \attvaltyp{INV}{true}}}\\
             \attval{VAL}{\avmplus{\att{val}\\
                                   \attval{SPR}{\avmplus{\att{ne-list}\\
                                             \attvaltyp{HD}{np}\\
                                             \attvaltyp{TL}{e-list}}}\\
                                   \attvaltyp{COMPS}{e-list}}}}$}
\end{center}
(Full details of the feature structure description language
are given in section~\ref{typesyntax}, below.)
The type of the feature structure 
is determined automatically; since the features {\feature head} and 
{\feature val} are specified, its type has to be {\bf intrans\_verb}
(or some subtype of that type).
The procedure for making a feature structure well-formed, $WF$, 
involves recursively unifying the constraints of the maximal types of
the subparts of the feature structure; since we disallow cyclic
feature structures, this operation terminates straightforwardly at the
atomic feature structures.

\subsection{Unification and well-formedness}
\label{formunif}
The definition of subsumption for well-formed feature structures is 
exactly the same as
that for typed feature structures.  
The result of well-formed unification of well-formed feature structures, 
$\tuple{{\cal WF}, \subsumedby}$,
is defined to be the greatest lower bound in the subsumption ordering 
of well-formed 
feature structures and is thus itself well-formed.  
This does however mean that 
the algorithm for unification involves an additional step, because 
it is potentially
necessary to unify in the constraint feature structure associated with the meet
of the types of the feature structures being unified.  

If $F_{1}$ and $F_{2}$ are well-formed feature structures of types 
$t_{1}$ and $t_{2}$
respectively, then $F_{1} \sqcap F_{2}$, if it exists,
 has type $t_{1} \sqcap t_{2}$. 
Since $F_{1}$ and $F_{2}$ are well-formed, in particular we know that 
$F_{1}\sqsubseteq C(t_{1})$ and
$F_{2} \sqsubseteq C(t_{2})$. Thus if $F_{1}$ and $F_{2}$ are consistent,
 $F_{1}\sqcap F_{2}\sqsubseteq C(t_{1})\sqcap C(t_{2})$.
But to be well-formed 
$F_{1} \sqcap F_{2}$ has to satisfy 
$F_{1} \sqcap F_{2} \sqsubseteq C(t_{1} \sqcap
t_{2})$ and  $C(t_{1} \sqcap t_{2})$ might be more specific than 
$C(t_{1}) \sqcap C(t_{2})$.
Consider the following example of a type hierarchy:
\begin{center}
\setlength{\unitlength}{0.4mm}
\begin{picture}(180, 100)
\put(75, 90){\makebox(0,0)[t]{$\top$}}
\put(42.5, 55){\line(1,1){25}}
\put(107.5, 55){\line(-1,1){25}}
\put(150, 55){\line(-2,1){50}}
\put(37.5, 50){\makebox(0,0){$t_{1}$}}
\put(112.5, 50){\makebox(0,0){$t_{2}$}}
\put(152.5, 50){\makebox(0,0){$t_{4}$}}
\put(42.5,45){\line(1,-1){25}}
\put(107.5,45){\line(-1,-1){25}}
\put(75,10){\makebox(0,0)[b]{$t_{3}$}}
\put(130,10){\makebox(0,0)[b]{$t_{5}$}}
\put(180,10){\makebox(0,0)[br]{$t_{6}$}}
\put(151.5,45){\line(1,-1){25}}
\put(151.5,45){\line(-1,-1){25}}
\end{picture}
\end{center}
Assume that the types $t_{4}$, $t_{5}$ and $t_{6}$ are atomic
(i.e. they have constraints $[t_{4}]$, $[t_{5}]$ and $[t_{6}]$, respectively)
and the constraints on types $t_{1}$, $t_{2}$ and $t_{3}$ are:\\
\begin{tabular}{lll}
$C(t_{1}) =$
{\tiny
   $\avmplus{\att{$t_{1}$}\\
             \attvaltyp{F}{$t_{4}$}}$} &
$C(t_{2}) =$
{\tiny
   $\avmplus{\att{$t_{2}$}\\
             \attvaltyp{G}{$\top$}}$} &
$C(t_{3}) =$
{\tiny
   $\avmplus{\att{$t_{3}$}\\
             \attvaltyp{F}{$t_{5}$}\\
             \attvaltyp{G}{$\top$}\\
             \attvaltyp{H}{$\top$}}$}
\end{tabular}\\
We then have:
\begin{tabular}{l}
$C(t_{1}) \sqcap C(t_{2})=$
{\tiny
   $\avmplus{\att{$t_{3}$}\\
             \attvaltyp{F}{$t_{4}$}\\
             \attvaltyp{G}{$\top$}} $}
\end{tabular}\\
Then\  $t_{3} = t_{1} \sqcap t_{2}$ but $C(t_{3}) 
\sqsubset C(t_{1}) \sqcap C(t_{2})$

Consider the following well-formed feature structures, 
$F_{1}$, $F_{2}$ and $F_{3}$:\\
\begin{tabular}{lll}
$F_{1} =$
{\tiny
   $\avmplus{\att{$t_{1}$}\\
             \attvaltyp{F}{$t_{4}$}}$}
&
$F_{2} =$
{\tiny
   $\avmplus{\att{$t_{2}$}\\
             \attvaltyp{G}{$\top$}}$}
&
$F_{3} =$
{\tiny
   $\avmplus{\att{$t_{1}$}\\
             \attvaltyp{F}{$t_{6}$}}$}\\
$F_{1}\sqcap F_{2} =$
{\tiny
   $\avmplus{\att{$t_{3}$}\\
             \attvaltyp{F}{$t_{4}$}\\
             \attvaltyp{G}{$\top$} }$}\\
$F_{2}\sqcap F_{3} =$
{\tiny
   $\avmplus{\att{$t_{3}$}\\
             \attvaltyp{F}{$t_{6}$}\\
             \attvaltyp{G}{$\top$} }$}
\end{tabular}\\
$F_{1}\sqcap F_{2} $ is not  a well-formed feature structure of type $t_{3}$
as $F_{1}\sqcap F_{2}\not \sqsubseteq C(t_{3})$.  To extend it to a well-formed 
feature structure involves unifying in $C(t_{3})$.  $F_{2}\sqcap F_{3} $
is also not a well-formed feature structure 
but it
cannot be extended to a  well-formed  feature structure, because its value for
F is inconsistent with the constraint for $t_{3}$.

To overcome this, we use the following definition
for the operation of well-formed unification, $\unify_{w}$:
\begin{definition}[Well-formed unification]
The well-formed unification of $F_{1}$ and $F_{2}$, $F_{1} \unify_{w} F_{2}$
is given by $F_{1}\sqcap F_{2}\sqcap C(t_{1}\sqcap t_{2})$.  This will be
well-formed if it exists. 
\end{definition}

No such complication arises with generalisation, since 
$F = F' \generalize F''$ will always be well-formed if $F'$ and $F''$ 
are well-formed.
From now on, when talking about typed feature structures, we will use
the term feature structure to mean well-formed typed feature
structure and $\unify$ will be
used to refer to well-formed unification, $\unify_w$, dropping the
subscript.

\section{Description language}
\label{syntax}

In order to specify types, constraints, lexical entries and so on, we require
a {\it description language}.
The LKB allows a variety of description languages to be used: the one
specified here adopts a simplified version of the TDL syntax from the PAGE
system (Uszkoreit et al, 1994).
We will introduce the syntax by
going through some examples, in increasing order of complexity.

\subsection{Syntax of type and constraint descriptions}
\label{typesyntax}

\paragraph{Example 1}

\begin{verbatim}
feat-struc := *top*.
\end{verbatim}
or
\begin{verbatim}
feat-struc :< *top*.
\end{verbatim}
In the LKB, these are alternative ways of defining 
the type {\bf feat-struc} to inherit from the single
parent {\bf *top*}, i.e., $\top$,
which is the only built-in type 
in the LKB system (the name can be specified as a 
system parameter).\footnote{In the full 
TDL syntax, the symbol {\tt :<} is 
used to indicate simple inheritance
from a single type when there is no constraint specification.
In the LKB, {\tt :<} is allowed in this situation (and only in this situation)
but {\tt :=} may always be used instead.}
Since no constraint is specified for {\bf feat-struc} and it is
not possible to specify a constraint for {\bf *top*},
$C(\mbox{\bf feat-struc})= \mbox{\tiny $\avmplus{\att{feat-struc}}$}$.
Note that:
\begin{enumerate}
\item the type 
definition, and in fact all descriptions,  is terminated by a `.'
\item new lines and spaces are not significant, except that spaces
may not occur in identifiers, such as type names
\item case is not significant, though we follow the
convention of expressing features in uppercase and types in lowercase
\item the use of {\tt *} in the type name {\bf *top*} is a convention
indicating this is a standard type, however the {\tt *}s are not significant
to the system
\item  the following characters may not be used in identifiers:
{\tt <} {\tt >} {\tt !} {\tt =} {\tt :} 
{\tt .} {\tt \#} {\tt \&} {\tt ,} {\tt [}
{\tt ]} {\tt ;} {\tt @} {\tt \$} {\tt (} {\tt )} {\tt \verb+^+} {\tt "}
\end{enumerate}

\paragraph{Example 2}

\begin{verbatim}
agr-cat := gen-agr-cat &
[ PER per,
  NUM num,
  GEND gend ].
\end{verbatim}
(The symbol \verb+:=+ has to be used because the
constraint is specified.)
This description
defines {\bf agr-cat} to inherit from {\bf gen-agr-cat} and to
have a constraint specification which is equivalent to the
following in the `pretty' AVM notation:
\[
\mbox{\tiny
   $\avmplus{\attvaltyp{PER}{per}\\
             \attvaltyp{NUM}{num}\\
             \attvaltyp{GEND}{gend}}$}
\]
The actual constraint on the type will be the following:
\[
\mbox{\tiny
   $\avmplus{\att{agr-cat}\\
             \attvaltyp{PER}{per}\\
             \attvaltyp{NUM}{num}\\
             \attvaltyp{GEND}{gend}}$} \unify
C(\mbox{\bf gen-agr-cat})
\]


\paragraph{Example 3}

\begin{verbatim}
head-feature-principle := grule & head-dtr-type & 
 [ SYN [ HEAD #head ],
   H [ SYN [ HEAD #head ] ] ].
\end{verbatim}
The type {\bf head-feature-principle}
has two parents: {\bf grule} and {\bf head-dtr-type}. 
The structure shows the notation for reentrancy: the
reentrant node is indicated by \verb+#head+.  Note that
the name of the tag is not significant.  
The constraint on this type will be:
\[
\mbox{\tiny
   $\avmplus{\att{head-feature-principle}\\
             \attval{SYN}{\avmplus
                           {\attval{HEAD}{\ind{1} \ \myvaluebold{*top*}}}}\\
             \attval{H}{\avmplus{\attval{SYN}{\avmplus
                                              {\attval{HEAD}{\ind{1}}}}}}}$}
\unify
C(\mbox{\bf  grule}) \unify
C(\mbox{\bf head-dtr-type})
\]

Note that the TDL syntax also allows an alternative notation to be used
for concatenation of features:
\begin{verbatim}
 [ SYN.HEAD #head,
   H.SYN.HEAD #head ].
\end{verbatim}
is equivalent to:
\begin{verbatim}
 [ SYN [ HEAD #head ],
   H [ SYN [ HEAD #head ] ] ].
\end{verbatim}


\subsection{A formal description of the syntax of type descriptions}
\label{bnftype}

To formally express the syntax of the description language, we
use Backus-Naur Form (BNF) basically following the typographic conventions used
in Aho et al (1982).
As there, alternative right-hand sides of productions are indicated by
$|$.  
However, we follow the convention that both non-terminals and
terminal tokens are italicized: non-terminals are capitalized.
For example {\it Avm-def} is a non-terminal, {\it identifier}
is a terminal token that could match, for example, a type name
such as `head-dtr-type'.
Using these conventions, the following is the basic
syntax of the type specification 
language:
\begin{list}{}
   {\leftmargin 5em
    \itemindent -\leftmargin
    \itemsep 0pt plus 1pt
    \parsep 0pt plus 1pt}
\bnfitem{{\it Type-def} $\rightarrow$ {\it Type} {\it Avm-def} {\bf .} $|$  
                                      {\it Type} {\it Subtype-def} {\bf .}}
\bnfitem{{\it Type}  $\rightarrow$ {\it identifier}}
\bnfitem{{\it Subtype-def} $\rightarrow$ :< {\it Type}}
\bnfitem{{\it Avm-def} $\rightarrow$ := {\it Conjunction}}
\bnfitem{{\it Conjunction} $\rightarrow$ {\it Term} $|$ 
                                      {\it Term} \& {\it Conjunction}}
\bnfitem{{\it Term} $\rightarrow$ {\it Type}  $|$ {\it string}  $|$ {\it Feature-term} 
 $|$ {\it Coreference }}
\bnfitem{{\it Feature-term} $\rightarrow$  [] $|$ [ {\it Attr-val-list} ] }
\bnfitem{{\it Attr-val-list} $\rightarrow$ {\it Attr-val} $|$
                                           {\it Attr-val} , {\it Attr-val-list}}
\bnfitem{{\it Attr-val} $\rightarrow$ {\it Attr-list} {\it Conjunction}}
\bnfitem{{\it Attr-list} $\rightarrow$ {\it Attribute} $|$
                                       {\it Attribute}.{\it Attr-list}}
\bnfitem{{\it Attribute}  $\rightarrow$ {\it identifier}}
\bnfitem{{\it Coreference}  $\rightarrow$ \#{\it identifier}}
\end{list}
This syntax is elaborated to allow for 
TDL templates, lists, and difference lists,
as described in \S\ref{templatedesc} and \S\ref{listdesc}.  Strings are 
are indicated by
{\tt "}: e.g., {\tt "me"} or {\tt '}: e.g.,  {\tt 'Kim}.  They are
discussed in the next section.  The modification to allow
for defaults is given in \S\ref{defaults}.

TDL also allows `status' indications, e.g.,
\begin{verbatim}
:begin :type
\end{verbatim}
These are allowed in LKB files for compatibility with the PAGE
system, but are ignored.  

Comments are allowed in LKB files, but only in between descriptions.
Comments are either single lines
beginning with {\tt ;} or they are bracketed by {\tt \#|} {\tt |\#}.



\subsection{Lexical entries}
\label{lexsyntax}
The syntactic description for lexical entries is
very similar to that for type constraints.  For example:
\begin{verbatim}
me_1 := pron-lxm & 
[ ORTH "me",
  SYN  [ HEAD noun & 
              [ AGR non-3sing 
                    & [ PER 1st ],
		CASE acc ] ] ].
\end{verbatim}
However the interpretation is different.  Rather than
stipulating that me\_1 is a type in the hierarchy under the
type {\bf pron-lxm}, this definition 
states that me\_1 is a feature structure which has the
type {\bf pron-lxm}.  If the definition were simply:
\begin{verbatim}
me_1 := pron-lxm.
\end{verbatim}
this should be
read as stating that the feature structure named by me\_1 is
{\tiny $\avmplus{\att{pron-lxm}}$}: that is the single-node 
feature structure with the node having type {\bf pron-lxm}
(though obviously the process of making this well-formed expands the
structure).
Given the way the formalism is defined (following Carpenter),
it is a category error to
think of lexical entries as being like maximally-specific types,
or even as `instances' of types: lexical entries are typed feature 
structures and types label nodes in feature structures.
However, the terminology in the literature is very confused,
because {\it type} is often used to mean something more like the concept
we refer to as the type constraint.  

Note that the value of the feature {\sc orth} in this
definition is a string.
Strings are a method of allowing a feature structure to contain a 
value without an explicit declaration of that value as a type
being necessary.  Formally, all strings can be treated as
being a daughter of a specific atomic type, usually called {\bf string}
(the name can be specified as a parameter to the system).
String types are all atomic,
have no parent other than {\bf string} and have no daughters.

The BNF for lexical entries is as follows:
\begin{list}{}
   {\leftmargin 5em
    \itemindent -\leftmargin
    \itemsep 0pt plus 1pt
    \parsep 0pt plus 1pt}
\bnfitem{{\it Lexentry} $\rightarrow$ {\it LexID}  {\it Avm-def}.} 
\bnfitem{{\it LexID} $\rightarrow$ {\it identifier}}
\end{list}
Note that, although it is legal syntax to define a 
lexical entry with multiple types on a node,
this will not be valid unless the types have an
existing type as their greatest lower bound.
Thus it is normal to specify this type directly
in lexical entries.

In order for lexical entries to be handled correctly by
the morphology system and parser, they must have a specified
orthography, and the path to access that orthography
must be specified as a system parameter.

\subsection{Rules}
\label{rules}
Grammar and lexical rules in the LKB 
system are typed feature structures,
which represent relationships between two or more signs.
The BNF is essentially identical to that for
lexical entries:
\begin{list}{}
   {\leftmargin 5em
    \itemindent -\leftmargin
    \itemsep 0pt plus 1pt
    \parsep 0pt plus 1pt}
\bnfitem{{\it Ruleentry} $\rightarrow$ {\it RuleID}  {\it Avm-def}.} 
\bnfitem{{\it RuleID} $\rightarrow$ {\it identifier}}
\end{list}

Rules must expand out into feature structures which have identifiable paths
for the mother and daughters of the rule.  For example,
a possible type constraint for binary rules might specify:
\begin{center}
{\tiny
   $\avmplus{\att{binary-rule}\\
             \attval{ARGS}{\avmplus{\attvaltyp{FIRST}{sign}\\
                                    \attval{REST}{\avmplus{\attvaltyp{FIRST}{sign}}}}}}$}
\end{center}
Here the mother is given by the empty path, one daughter 
by the path {\sc args}.{\sc first} and another by the path
{\sc args}.{\sc rest}.{\sc first}.  
The mother path and a function which gives the
daughters in the correct linear order must be specified as system parameters.

Rules can be regarded statically,
as expressing a dependency relationship between signs.
For example, given the assumption about paths made above, 
we can say that a rule R applies to a mother, M and daughters D1 and D2,
iff the following is well
formed:
\[
R \unify M \unify
\mbox{\tiny
   $\avmplus{\attval{ARGS}{\avmplus{\attval{FIRST}{D1}\\
                                    \attval{REST}{\avmplus{\attval{FIRST}{D2}}}}}}$}
\] 
If the linear order is specified so that {\sc args.first} precedes
{\sc args.rest.first} then D1 must precede D2.


Rules can alternatively be regarded as a means of constructing
new signs.  For
instance, in bottom-up processing,
if a series of signs, $F_{1}, F_{2} \ldots F_n$ can be unified 
with the feature structures at the end of the paths $P_{1}, P_{2} \ldots P_{n}$
in a rule R, where $P_{1}$ corresponds to the path associated with the
first daughter, $P_{2}$ the second, etc,
then the feature structure at the end of
the mother path in R is a new sign.  
Rule application is order-independent, so parsers and generators 
following different 
strategies will achieve the same result.  

Rules must have a definite number of daughters: there is no direct way of
defining a rule using Kleene star,
or of encoding optionality directly.  A weakly-equivalent
alternative formulation is always possible,
using extra rules to get the required effect.

Lexical rules are treated as unary grammar rules: 
they are special only in that
they may apply before affixation.  In principle
they may also be applied to phrases ---
all constraints on lexical rule application must be explicitly encoded,
e.g., by stipulating that lexical rules may only apply
to {\bf word}s rather than {\bf phrase}s.
Normally this will be done by defining a type for lexical rules.

\subsection{Morphological rules}
\label{morphology}

In the terminology we will use here,
morphological rules are a special case of lexical rules which 
involve affixation.  
The current form of orthographemics in the LKB system
assumes that affixes are associated with specific rules.
The LKB system uses a simple string unification approach for encoding
orthographemics.\footnote{This was implemented by Bernie Jones
and the following description is adapted from text written by him.}
Affixation and any associated spelling changes
is indicated by means of information
which augments the standard lexical rules.
This affixation information
is specified after the identifier, but before the remainder 
of the rule description.

For example, in the following rule, the information introduced
by \% describes the spelling changes --- the rest of the rule
is just described in the standard TDL syntax.
\begin{verbatim}
past_verb_infl_rule :=
%suffix (* ed) (!ty !tied) (e ed) (!t!v!c !t!v!c!ced) 
lex_rule_infl_affixed &
[ NEEDS-AFFIX +,
  ARGS [ FIRST [ AFFIX past_verb ]]].
\end{verbatim}
The spelling rule has two parts --- the first is simply \verb+prefix+ or
\verb+suffix+ and the second
contain a set of subrules in the form of matched simple
partial regular expressions. The first of each pair of expressions must match
letters in the word stem, either at the beginning of the stem (for prefixes) or
at the end (for suffixes, as illustrated here).  For example, the
subrule \verb+(e ed)+ will match the stem {\it like}.
The second
expression in the subrule
describes the set of letters which replace
the letters matching the input in the affixed form of the word.
Thus the stem {\it like} corresponds to the affixed {\it liked}.

The asterisk \verb+*+ and the symbols introduced by \verb+!+
are special characters.
The asterisk represents a null character, and is interpreted
so that the first sub-rule is the default ({\it ed} is added at the word-final
position). 
Subrules without asterisks are treated as exceptions to the default.
For instance, none of the explicit subrules match the stem {\it match}.
This therefore corresponds to the first subrule and the affixed form
is {\it matched}.
The {\it !}  structure is used to denote special macro symbols
which correspond to sets of letters. These sets are
defined at the beginning
of the file with the morphological rules, for example:
\begin{verbatim}
%(letter-set (!c bdfglmnprstz))
%(letter-set (!s abcdefghijklmnopqrtuvwxyz))
%(letter-set (!t bcdfghjklmnpqrstvwxz))
%(letter-set (!v aeiou))
\end{verbatim}
The interpretation of these symbols is that they match any one of the
characters in the set, but the matching character must be the same 
throughout the subrule.  For instance, (\verb+!t!v!c !t!v!c!ced+) 
will match ({\it pot} {\it potted}), but would not match 
({\it pot} {\it podded}).
 
As with the rest of the system, morphology is reversible, and can
thus be used for parsing and generation.
Irregular forms can be specified in the rule, or in a separate
file.  

The BNF for the letter sets is as follows:
\begin{list}{}
   {\leftmargin 5em
    \itemindent -\leftmargin
    \itemsep 0pt plus 1pt
    \parsep 0pt plus 1pt}
\bnfitem{{\it Letterset} $\rightarrow$ \%({\bf letter-set} ({\it Macro} {\it letters}))}
\bnfitem{{\it Macro} $\rightarrow$ !{\it letter}}
\end{list}
where {\it letter} corresponds to any character, {\it letters} to one or
more characters and {\bf letter-set} is a literal: i.e., `letter-set' must appear
in the file.

The rules themselves have the following BNF:
\begin{list}{}
   {\leftmargin 5em
    \itemindent -\leftmargin
    \itemsep 0pt plus 1pt
    \parsep 0pt plus 1pt}
\bnfitem{{\it Mruleentry} $\rightarrow$ {\it RuleID} {\it Mgraph-spec-list} {\it Avm-def}.}
\bnfitem{{\it Mgraph-spec-list} $\rightarrow$ {\it Mgraph-spec} 
$|$ {\it Mgraph-spec} {\it Mgraph-spec-list}}
\bnfitem{{\it Mgraph-spec} $\rightarrow$  \%{\bf prefix} {\it SPair-list} $|$
\%{\bf suffix} {\it SPair-list}}
\bnfitem{{\it SPair-list} $\rightarrow$ {\it SPair} 
$|$ {\it SPair} {\it SPair-list}}
\bnfitem{{\it SPair} $\rightarrow$ ( * {\it Char-list} ) 
$|$ ( {\it Char-list} {\it Char-list} )}
\bnfitem{Char-list -> {\it letter} $|$ {\it Macro} $|$ {\it letter} {\it Char-list}
$|$ {\it Macro} {\it Char-list}}
\end{list}

If a separate file is used for irregular forms, it consists of
a string containing a list of irregular entries, one per line,
where each entry has the following form:
\begin{list}{}
   {\leftmargin 5em
    \itemindent -\leftmargin
    \itemsep 0pt plus 1pt
    \parsep 0pt plus 1pt}
\bnfitem{{\it Irregentry} $\rightarrow$ {\it base} {\it Rulespec} {\it inflected}}
\end{list}
where both {\it base} and {\it inflected} are words.
For example:
\begin{verbatim}
"
fell PAST-VERB fall
felt PAST-VERB feel
"
\end{verbatim}
For compatibility with PAGE, the rule name can be constructed
from the rule specification found in the file by concatenating a 
suffix.  The suffix is specified as a parameter,
{\tt *lex-rule-suffix*} (see \S\ref{mpglob}).
Details of the treatment of irregular forms are given in
\S\ref{irregs}.

\subsection{Lists and difference lists}
\label{listdesc}

Lists and difference lists are very frequently used constructs
in typed feature structure grammars.  Because of this, the syntax
provides a notation for them directly, which expands out
into the feature structure representation.  In the LKB,
the names of features and types involved are controlled 
by system parameters.  For example, suppose lists are defined as follows:
\begin{verbatim}
list :< *top*.
e-list :< list.
ne-list := list & 
            [ HD *top*,
              TL *top* ].
\end{verbatim}
Then the full representation of a list of elements a,b,c would be:
\begin{verbatim}
[ HD a,
  TL [ HD b,
       TL [ HD c,
            TL e-list ]]]
\end{verbatim}
Using the abbreviatory notation, we could simplify this to:
\begin{verbatim}
< a, b, c >
\end{verbatim}
Here and below,
the a, b, c can be arbitrarily complex conjunctions of terms, which themselves
contain lists etc.
It is often necessary to specify potentially non-terminated lists, for example:
\begin{verbatim}
[ HD a,
  TL [ HD b,
       TL [ HD c,
            TL list ]]]
\end{verbatim}
which allows the possibility of additional elements after c.  This is represented
as 
\begin{verbatim}
< a, b, c ...>
\end{verbatim}
Finally, the syntax allows for {\it dotted-pairs}.  For instance, the pair
a.b could be represented as:
\begin{verbatim}
[ HD a,
  TL b ]
\end{verbatim}
which is abbreviated as
\begin{verbatim}
< a . b >
\end{verbatim}

Difference lists can also be abbreviated.  For example,
assume the following type definition:
\begin{verbatim}
diff-list := *top*
            [ LIST list,
              LAST list ].
\end{verbatim}
The full notation for the difference list containing a,b,c would be:
\begin{verbatim}
[ LIST [ HD a,
         TL [ HD b,
              TL [ HD c,
                   TL #last ]]],
  LAST #last ]
\end{verbatim}
This can be abbreviated as:
\begin{verbatim}
<! a, b, c !>
\end{verbatim}
Note that the list component of the difference list is not terminated.
It is not possible to stipulate a difference list which has an arbitrary
number of elements, since the coindexation of {\sc list}
and {\sc last} requires a determinate
path.

The following amends the BNF given earlier to allow for lists and difference
lists:
\begin{list}{}
   {\leftmargin 5em
    \itemindent -\leftmargin
    \itemsep 0pt plus 1pt
    \parsep 0pt plus 1pt}
\bnfitem{{\it Term} $\rightarrow$ {\it Type}  $|$ {\it string}  $|$ {\it Feature-term} 
 $|$ {\it Coreference } $|$ {\it List} $|$ {\it Diff-list}}
%
\bnfitem{{\it Diff-list} $\rightarrow$ <! !> $|$ <! {\it Conjunction-list} !>}
%
\bnfitem{{\it Conjunction-list} $\rightarrow$ {\it Conjunction} $|$
{\it Conjunction} , {\it Conjunction-list}}
%
\bnfitem{{\it List} $\rightarrow$ <> $|$ < {\it Conjunction-list} > 
$|$ < {\it Conjunction-list} , ... > $|$\\ 
< {\it Conjunction-list} . {\it Conjunction} >}
\end{list}
%Term -> Type | Feature-term | Diff-list | List | Coreference | Templ-call
%Diff-list -> <! !> | <! Conjunction {, Conjunction}* !>
%List -> < > | < Conjunction {, Conjunction}* > |
%                < Conjunction {, Conjunction}* , ...> |
%                 < Conjunction {, Conjunction}* . Conjunction> 

\subsection{Parameterized templates}
\label{templatedesc}

The PAGE system allows templates with parameters to occur in descriptions.
These are an abbreviatory notation: they always expand out into 
typed feature structures.
The LKB supports a limited use of parameterized
templates, for compatibility with PAGE.
For completeness the augmentation to the description language
syntax is given here, although the use of such templates
is not encouraged.  

\begin{list}{}
   {\leftmargin 5em
    \itemindent -\leftmargin
    \itemsep 0pt plus 1pt
    \parsep 0pt plus 1pt}
\bnfitem{{\it Term} $\rightarrow$ {\it Type}  $|$ {\it string}  $|$ {\it Feature-term} 
 $|$ {\it Coreference } $|$ {\it List} $|$ {\it Diff-list} $|$ {\it Templ-call}}
\bnfitem{{\it Templ-call} $\rightarrow$ @{\it Templ-name} ( ) |
                      @{\it Templ-name} ({\it Templ-par-list})}
\bnfitem{{\it Templ-name} $\rightarrow$ {\it identifier}}
\bnfitem{{\it Templ-par-list} $\rightarrow$ {\it Templ-par} $|$ 
                         {\it Templ-par} , {\it Templ-par-list}}
\bnfitem{{\it Templ-par}  $\rightarrow$ \${\it Templ-var} $|$
                                        \${\it Templ-var} = {\it Conjunction}}
\bnfitem{{\it Templ-var} $\rightarrow$ {\it identifier}}
\end{list}

% Templ-call -> @templ-name ( ) | @templ-name (Templ-par {, Templ-par}*)
% Templ-par -> $templ-var | $templ-var = conjunction

\subsection{Semantics of typed feature structure descriptions}
\label{descsem}

The descriptions can be taken as formulas which must be 
satisfied by the feature structures they describe, as was done in
Pereira and Shieber (1984). 
The logic of the description language used here
is identical to that described by 
Carpenter (1992:52f) with the exception that we do not 
allow disjunctive descriptions
(and thus the algorithm for deciding the
satisfiability of a formula
has the same order of complexity as unification).
Thus, as Carpenter shows we have the usual results for the satisfaction
relation, $\models$:
\begin{description}
\item[Monotonicity]
If $\phi$ is a formula then if
$F_{1}\models\phi$ and $F_{2}\sqsubseteq F_{1}$ then $F_{2}\models 
\phi$.
\item[Most general satisfier]
For every satisfiable formula 
$\phi$, there is a unique most general feature structure $MGSat(\phi)$ that
satisfies it.
\item[Description]
For any feature structure in ${\cal F}$ 
there is a description ${\sf Desc}(F)$ such that
\[ F\sim MGSat({\sf Desc}(F))\]
\end{description}

Note that this differs from some other typed feature structure
formalisms in which the satisfier of a description is taken to be a
set of maximally specific feature structures.  The practical
consequences of this distinction are not great, however.


\section{A note about disjunction and negation}
\label{disj}

Although many feature structure based languages allow disjunctive feature
structures and negation, this is avoided in the LKB system.  
Arbitrary disjunction can result
in a computationally intractable system.
Furthermore, support for disjunction and
negation causes computational overheads in feature structure representation
and operations such as unification, even if the particular grammar
contains no disjunctions or negations.

We do not believe that disjunction within feature structures is
necessary, given that the type system can be set up in a way which
allows degrees of underspecification.\footnote{This is not to say
there is no disjunction in the system: lexical ambiguity, for
instance, can be regarded as a form of disjunction.  However,
this is a case where two feature structures are regarded as 
alternatives, rather than there being disjunctive
information within a single structure.}  
Note that the lowest common supertype (join) of two types might be more general
than their disjunction.
In this case,
a new type may have to be created to
get the required level of specification. 
For example if the type {\bf person} was defined to
have the subtypes {\bf first}, {\bf second}, {\bf third}, 
a new type could be inserted in the hierarchy
in order
to express the equivalent of the disjunction {\bf second} or {\bf third}
(see Figure~\ref{addtypes}).
\begin{figure}
\setlength{\unitlength}{0.7in}
\begin{center}
\begin{picture}(3, 2)(1, 0)
\put(2,2.0){\makebox(0,0){\bf person}}
\put(1.8,1.8){\line(-1, -1){0.6}}
\put(2.2,1.8){\line(1, -1){0.6}}
\put(2.0,1.8){\line(0, -1){0.6}}
\put(1,1.0){\makebox(0,0){\bf first}}
\put(2,1.0){\makebox(0,0){\bf second}}
\put(3,1.0){\makebox(0,0){\bf third}}
\end{picture}
\begin{picture}(4, 2)(1, 0)
\put(2,2.0){\makebox(0,0){\bf person}}
\put(1.8,1.8){\line(-1, -1){0.6}}
\put(2.2,1.8){\line(1, -1){0.6}}
\put(1,1.0){\makebox(0,0){\bf first}}
\put(3,1){\makebox(0,0){\bf secorthird}}
\put(2.8,0.8){\line(-1, -1){0.6}}
\put(3.2,0.8){\line(1, -1){0.6}}
\put(2,0){\makebox(0,0){\bf second}}
\put(4,0){\makebox(0,0){\bf third}}
\end{picture}
\end{center}
\caption{Adding types to the hierarchy}
\label{addtypes}
\end{figure}


The effect of the non-equivalence of disjunction and join
may in fact be exploited
to allow a more precise specification of
a language.  For example, Pollard and Sag (1994)
give a description of the inflection of German adjectives 
such as {\it klein}, in which the following values 
for case are given as
possibilities:
{\type nom}, {\type acc}, {\type gen}, {\type dat},
{\type nom $\vee$ acc}, {\type gen $\vee$ dat}, {\it unspecified}.
Encoding this in the type system, as shown in Figure~\ref{germancase},
rather than making use of disjunction, 
would directly express the restriction that
only these values were available and that the following were not:
{\type nom $\vee$ gen}, 
{\type acc $\vee$ dat}, {\type nom $\vee$ acc $\vee$ gen},
{\type nom $\vee$ acc $\vee$ dat}, {\type nom $\vee$ gen $\vee$ dat},
{\type acc $\vee$ gen $\vee$ dat}.  
\begin{figure}
\centering
\begin{picture}(4,2)
\put(0.5, 0){\makebox(0,0){\bf nom}}
\put(1.5, 0){\makebox(0,0){\bf acc}}
\put(1, 1){\makebox(0,0){\bf nom\_acc}}
\put(2.5, 0){\makebox(0,0){\bf gen}}
\put(3.5, 0){\makebox(0,0){\bf dat}}
\put(3, 1){\makebox(0,0){\bf gen\_dat}}
\put(2, 2){\makebox(0,0){\bf case}}
\put(1.8, 1.8){\line(-1, -1){0.6}}
\put(2.2, 1.8){\line(1, -1){0.6}}
\put(0.8, 0.8){\line(-1, -2){0.3}}
\put(1.2, 0.8){\line(1, -2){0.3}}
\put(2.8, 0.8){\line(-1, -2){0.3}}
\put(3.2, 0.8){\line(1, -2){0.3}}
\end{picture}
\caption{Restricting the possibilities for case specification} 
\label{germancase}
\end{figure}

Negation is not allowed in the LKB system: the interpretation
of negation in a feature structure language is far from intuitive and
the logic we have used to
define typed feature structures does not in fact allow for any straightforward
form of negation.  We do not believe this is a disadvantage in developing
practical grammars.


\chapter{LKB source files}
\label{srcfiles}

The previous chapter described the formalism used by the LKB.  This chapter
goes into the details of how the system evaluates grammar source files
expressed in that formalism.
Most of this chapter should be equally relevant to people using the
graphical or tty versions of the system.
The source file organization is based on the assumption that
a single file contains one class of object.  The user may however 
choose to split
the descriptions in a particular class over multiple files, for example to have
different type files for the types used in rules as opposed to those used
in lexical entries, or have multiple files for the lexicon.
This often enhances modularity.

The main classes of object within the LKB system 
are:
\begin{description}
\item[Types and constraints]
\item[Lexical entries]
\item[Grammar rules]
\item[Lexical and morphological rules]
\end{description}
Apart from types and
their constraints, all the rest of the objects are generically referred to
as {\it entries}.
They are all typed feature structures which are associated 
with an identifier.\footnote{By {\it entry} we mean roughly
what was meant by {\it psort} in the old LKB documentation,
and what is sometime referred to as an {\it instance}.  
We've changed the terminology because {\it psort}
was just confusing and {\it instance} is somewhat inaccurate.}
They are distinguished because of their 
function within the system, with respect to
parsing, for example.
Two further categories of entry are also used in the sample
grammars:
\begin{description}
\item[Root descriptions]
\item[Parse node descriptions]
\end{description}

If you have worked through Chapter~\ref{firstsession}, you have seen
examples of types, type constraints, lexical entries and grammar rules.
Lexical and morphological rules were not used in the `toy' grammar,
but are introduced in a slightly more complex grammar
`rattle', so where necessary we will use examples from 
that.\footnote{There are further ways to use entries in the LKB which are
not described in this document.  These include 
translation equivalences (sometimes referred to
a bilexical entries) and rules for semantic conversion.}
This chapter contains full details for all the different
types of source file in the `rattle' grammar.
It does not, however, cover all the capabilities of the LKB system
for dealing with larger grammars or ones which have a different
approach to syntax --- the details of the advanced features
are left to Chapter~\ref{advanced}.

\section{The script file}
\label{script-intro}

The following is the script file for `rattle', which illustrates
how the files of the different sorts of object are loaded into the 
LKB system:
\begin{verbatim}
(lkb-load-lisp (parent-directory) "globals.lsp")
(lkb-load-lisp (parent-directory) "user-fns.lsp")
(load-lkb-preferences (parent-directory) "user-prefs.lsp")
(read-tdl-type-files-aux
     (list (lkb-pathname (this-directory) "types.tdl")))
(read-tdl-lex-file-aux 
     (lkb-pathname (this-directory) "lexicon.tdl"))
(read-tdl-grammar-file-aux 
     (lkb-pathname (this-directory) "grules.tdl"))
(read-morph-file-aux 
     (lkb-pathname (this-directory) "infl-rules.tdl"))
(load-irregular-spellings 
     (lkb-pathname (this-directory) "irregs.txt"))
(read-tdl-psort-file-aux 
          (lkb-pathname (this-directory) "roots.tdl"))
(read-tdl-parse-node-file-aux 
          (lkb-pathname (this-directory) "parse-nodes.tdl"))
\end{verbatim}
The script file is written in Lisp, although for convenience
a number of functions have been defined to make writing a script file
easier.  These are discussed in detail in \S\ref{script-adv}.

The first three lines read in some Lisp files which parameterize the
system in various ways, as briefly discussed in \S\ref{ancil}.
The following lines read in the types with their constraints
and the entries of the various
classes listed above.  Note there is also a file for irregular
forms, as discussed in \S\ref{irregs}.  All these files are read
in by functions which take as an argument the pathname for the file.


\section{Types, constraints and type files}

The type system is defined in one or more type files.  It is necessary
to have a valid type system before anything else will work.
The system loads the type files and then carries out a series of
checks and expands the type constraints.
With an error free type hierarchy, you will see messages such as the
following while the system loads the files and
does this computation.
\begin{verbatim}
Reading in type file /home/aac/toy/types.tdl
Checking type hierarchy
Checking for unique greatest lower bounds
Expanding constraints
Making constraints well formed
Type file checked successfully
Computing display ordering
\end{verbatim}
Once a type file is loaded successfully, a window showing the type
hierarchy is displayed. 
This is described in detail in \S\ref{thier}, below.

The formal conditions on the type hierarchy and the syntax of the
language are
detailed in Chapter~\ref{formal}.
Here we will go through those conditions informally, and discuss what
happens when you try and load a file in which they are violated.
This section is lengthy compared to the sections
on lexical rules and so on, because the type hierarchy conditions
are relatively complex.

Many examples of errors 
are given below: these all assume that we have made the minimal
change to the `toy' grammar to make it match the structures shown.
The errors are not supposed to be particularly realistic!

IMPORTANT NOTE:  always look at the first error message first!
Error messages may scroll off the screen, so you may need to scroll up
in order to do this.  Sometimes errors propagate,
causing other errors, so it's often a good idea to reload the grammar
after you have fixed the first error.

\subsection{Syntactic well-formedness}
\label{synwf}
If the syntax of the constraint specifications in the type file is not 
correct, according to the definition in \S\ref{bnftype},
then error messages will be generated.  The system tries
to make a partial
recovery from syntactic errors, either by
skipping to the end of a definition or inserting the
character it expected,
and then continuing to read the file.
This recovery does not always work: sometimes the
inserted character is not the intended one and sometimes an
error recovery affects a subsequent definition.
Thus you may get multiple error messages from a single error.
The system will
not try to do any further well-formedness checking
on files with any syntactic errors.
In the examples below, an incorrect definition is shown
followed by the error message that is generated when the correct form of the
corresponding definition in {\tt toy/types.tdl} is replaced by 
the incorrect form.

\begin{verbatim}
feat-struc : *top*.
\end{verbatim}
\begin{error}
Syntax error at position 273: \\
Syntax error following type name (FEAT-STRUC)\\
Ignoring (part of) entry for FEAT-STRUC\\
Error: Syntax error(s) in type file
\end{error}
The error is caused by the missing {\tt =} following the
{\tt :}.  The error message indicates the position of
the error (using emacs you can use the command goto-char to
go to this position in the file).  The number given will
not always indicate the exact position of the problem, since the reader
may not be able to detect the problem immediately, but is likely to be quite close.  
The system then says what it is doing to try and recover from the error
(in this case, ignore the rest of the entry) and finally stops processing with
the error message {\tt Syntax error(s) in type file} (we will
omit this in the rest of the examples).

\begin{verbatim}
agr-cat := feat-struc &
 [ PER per,
   NUM num,
   GEND gend .
\end{verbatim}
\begin{error}
Syntax error: ] expected and not found in AGR-CAT at position 343\\
Inserting ]
\end{error}
In this example, the system tries to recover by inserting the character
it thinks is missing, correctly here.

\begin{verbatim}
agr-cat := feat-struc &
 [ PER per,
   NUM num,
   GEND gend ] 

pos := feat-struc &
 [ FORM form-cat ].

\end{verbatim}
\begin{error}
Syntax error: . expected and not found in AGR-CAT at position 354\\
Inserting .\\
Syntax error at position 365:\\ 
Incorrect syntax following type name (FEAT-STRUC)\\
Ignoring (part of) entry for FEAT-STRUC
\end{error}
Here the system's recovery attempt doesn't work, and the error 
propagates to the subsequent definition.  This illustrates
why you should reload the grammar after fixing the first error
unless you are reasonably sure the error messages are independent.

\begin{verbatim}
agr-cat := feat-struc &
 [ PER per,
   NUM num
   GEND gend ]. 
\end{verbatim}
\begin{error}
Syntax error: ] expected and not found in AGR-CAT at position 332\\
Inserting ]
\end{error}
Here the system diagnosed the error incorrectly, since in fact
a comma was missing rather than a `]'.  

\begin{verbatim}
head-feature-principle := hd-grule & 
 [ SYN [ HEAD #head ],
   H [ SYN [ HEAD #headd ] ] ].
\end{verbatim}
\begin{error}
Syntax error at position 3802: Coreference (HEADD) only used once
Syntax error at position 3802: Coreference (HEAD) only used once
\end{error}
In this example, the system warns that the coreference was only used 
once: it is assumed that this would only be due to an error on the part of 
the user.

You may also get syntax errors such as the following:
\begin{error}
Unexpected eof when reading X
\end{error}
(eof stands for end of file --- this sort of message is usually 
caused by a missing character).

\subsection{Conditions on the type hierarchy}
After a syntactically valid type file (or series of type files)
is read in, the hierarchy of types is constructed and checked 
to ensure it meets the conditions specified in \S\ref{formthier}.

\label{typehcond}
\begin{description}
\item[All types must be defined]
If a type is specified to have a parent which is not defined anywhere in the
loaded files, an error message such as the following is generated:
\begin{error}
FEAT-STRUC specified to have non-existent parent *TOPTYPE*
\end{error}
Although it is conventional to define parent types before their 
daughters in the file, this is not required, and order of type definition
in general has no significance for the system.
Note however that it is possible to redefine types, and if this is done, the
actual definition will be the last one the system reads.
If two definitions for types of the same name occur,
a warning message will be generated, e.g.,:
\begin{warning}
Type FEAT-STRUC redefined
\end{warning}
\item[Connectedness / unique top type]
There must be a single hierarchy containing all the types.
Thus it is an error for a type to be defined without any parents,
for example:
\begin{verbatim}
agr-cat := 
 [ PER per,
   NUM num,
   GEND gend ]. 
\end{verbatim}
Omitting the parent(s) of a type will cause an error message such as
the following:
\begin{error}
Error: Two top types *TOP* and AGR-CAT have been defined
\end{error}
To fix this, define a parent for the type which is not intended to be
the top type (i.e., {\bf agr-cat} in this example).

If a type is defined with a single parent which is also specified to be its 
descendant, the connectedness check will give error messages such as the
following for every descendant of the type:
\begin{error}
GRULE not connected to top
\end{error}
(This situation is also invalid because cycles are not allowed in the
hierarchy, but because of the way cycles are checked for, the error will be
found by the connectedness check rather than the cyclicity check).
\item[No cycles]
It is an error for a descendant of a type to be one of that type's ancestors.
This causes an error message to be generated such as:
\begin{error}
Cycle involving TV\_PRED
\end{error}
The actual type specified in the
messages may not be the one that needs to be changed,
because the system cannot determine which link in the cycle is incorrect.
\item[Redundant links] This is the situation where a type is
specified to be both an immediate and a non-immediate descendant of another type.
\begin{error}
Redundancy involving TV\_PRED
\end{error}
The assumption is that this would only happen because of a user error.
The condition is checked for because it could cause problems with the
greatest lower bound code (see \S\ref{glbgen}).
After finding this error, the system checks for any other redundancies
and reports them all.
\end{description}

\subsection{Uniqueness of greatest lower bound}
\label{glbgen}
  
The system requires that every set of types has a unique greatest lower bound
({\it glb})
as described in \S\ref{formthier}.  
This is necessary so that unification of typed feature structures
can give a single result, with one type per node in the feature structure.
Unlike the conditions listed in the previous section, however, the system can
fix this problem without user intervention by introducing new types.  These
types are given names such as {\bf glbtype1} etc.  They never have constraints.

For example, the toy-glb
grammar is almost exactly the same as the toy grammar, but the
type {\bf hd-grule} is omitted and the types which would inherit
from it instead inherit from its parents.  This type {\bf hd-grule}
was actually included in the toy grammar purely to
avoid a glb problem.  To illustrate what happens if the type hierarchy does not
meet the glb condition,
load the script file {\tt toy-glb/script}, which should give the following
messages:
\begin{verbatim}
Reading in type file /home/aac/toy/types.tdl
Checking type hierarchy
Checking for unique greatest lower bounds
Partition size 15
Elements in *interesting-types* 10
Checked
Expanding constraints
Making constraints well formed
Type file checked successfully
Computing display ordering
\end{verbatim}
The messages 
\begin{verbatim}
Partition size 15
Elements in *interesting-types* 10
Checked
\end{verbatim}
are a side effect of the glb checking.  They are displayed as a way
of giving the user some feedback on what the system is doing:
fixing glbs can be time-consuming in a large type system with
complex multiple inheritance.

In this particular case, just one glbtype is introduced.
By default, the type hierarchy is displayed
without glbtypes, but a hierarchy with glb types
can be shown by choosing
{\bf View} {\bf Type Hierarchy} and setting the {\tt Show all types}
checkbox.
If you view the type definition of a type which has a glb type as a parent,
it will show the glb type, but show the `real' parents after it 
in parentheses.
Apart from their user-interface properties, 
glbtypes behave exactly like user-defined types.

\subsection{Constraints}
Once the type hierarchy is successfully computed, the constraints
associated with types are checked, and inheritance
and typing are performed to give expanded constraints on types
(see \S\ref{formcons}).
\begin{description}
\item[Valid constraint description]
The check for syntactic well-formedness of the constraint description
is performed as the files are loaded, but errors such as 
missing types which prevent a valid feature structure being constructed
are detected when the constraint is expanded.
For example, suppose the following was in the 
`toy' type file, but the type 
{\bf foobar} was not defined.
\begin{verbatim}
agr-cat := feat-struc &
 [ PER foobar,
   NUM num,
   GEND gend ].
\end{verbatim}
The error messages would be as follows:
\begin{verbatim}
Invalid types (FOOBAR)
Unifications specified are invalid or do not unify
Type AGR-CAT has an invalid constraint specification
Type NON-3SING's constraint specification clashes with its parents'
Type 3SING's constraint specification clashes with its parents'
\end{verbatim}
Note that the error propagates because the descendants'
constraints cannot
be constructed either.

Another similar error is to declare two nodes to be reentrant which
have incompatible values.  For example:
\begin{verbatim}
agr-cat := feat-struc &
 [ PER #1 & per,
   NUM #1 & num,
   GEND gend ].
\end{verbatim}
The error messages would be very similar to the case above:
\begin{verbatim}
Unifications specified are invalid or do not unify
Type AGR-CAT has an invalid constraint specification
Type NON-3SING's constraint specification clashes with its parents'
Type 3SING's constraint specification clashes with its parents'
\end{verbatim}
\item[No Cycles] 
Feature structures are required to be acyclic
in the LKB system (see \S\ref{extvsint} and
\S\ref{formtfs}): if a cycle is constructed
during unification, then unification fails.  In the case
of construction of constraints, this sort of 
failure is indicated explicitly.
For example, suppose the following is a type definition
\begin{verbatim}
birule-headfirst := binary-headed-rule &
 [ H #1,
   NH1 #2,
   ARGS < #1 & [ H #1 ], #2 > ] .
\end{verbatim}
The following error is generated:
\begin{error}
Unification failed: copy found cycle at < ARGS : FIRST : H >\\
Type BIRULE-HEADFIRST has an invalid constraint specification
\end{error}
\item[Consistent inheritance]
Constraints are constructed by monotonic inheritance from 
the parents'.  If the parental constraints do not unify with
the constraint specification, or, in the case of multiple parents,
if the parents' feature structures are not compatible,
then the following error message is generated:
\begin{error}
Type X's constraint specification clashes with its parents
\end{error}
\item[Maximal introduction of features]
As described in \S\ref{formcons},
there is a condition on the type system that any feature
must be introduced at a single point in the hierarchy.
That is, if a feature, F, is mentioned at the top level of
a constraint on a type, t, and 
not on any of the constraints of
ancestors of t, then all constraints where
F is used must be constraints on descendants of t.
For example, the following would be an error because {\sc per}
is a top level feature on both the constraint for
{\bf agr-cat} and {\bf pos} but not on the constraints
of any of their ancestors:
\begin{verbatim}
feat-struc :< *top*.

agr-cat := feat-struc &
 [ PER per,
   NUM num,
   GEND gend ].

pos := feat-struc &
 [ PER per,
   FORM form-cat ].
\end{verbatim}
The error message is as follows:
\begin{error}
Feature PER is introduced at multiple types (POS AGR-CAT)
\end{error} 
To fix this, it is necessary to introduce another type on which to
locate the feature.  For example:
\begin{verbatim}
feat-struc :< *top*.

per-intro := feat-struc &
 [ PER *top* ].

agr-cat := per-intro &
 [ PER per,
   NUM num,
   GEND gend ].

pos := per-intro &
 [ PER per,
   FORM form-cat ].
\end{verbatim}

The reason for this condition is to allow deterministic 
typing of feature structures.                      
\item[No infinite structures]
Because strong typing is used, it is an error for a constraint on a type
to mention that type inside the constraint (see Carpenter, 1992).  
For example, the following is
invalid.
\begin{verbatim}
ne-list := *list* &
 [ FIRST *top*,
   REST ne-list ].
\end{verbatim}
The reason for this is that expansion of the constraint 
would create an infinite 
structure:
\begin{verbatim}
 [ ne-list
   FIRST *top*,
   REST [ ne-list
          FIRST *top*,
          REST [ ne-list
                 FIRST *top*,
                 REST .....
\end{verbatim}
The following error message is produced:
\begin{error}
Error in NE-LIST: 
  Type NE-LIST occurs in constraint for type NE-LIST at (REST)
\end{error}
Similarly it is an error to mention a daughter of a type in its
constraint.  It is also an error to make two types mutually
recursive:
\begin{verbatim}
foo := *top* &
[ F bar ].

bar := *top* &
[ G foo ].
\end{verbatim}
The error message in this case is:
\begin{error}
FOO is used in expanding its own constraint 
                    expansion sequence: (BAR FOO)
\end{error}


Note that it {\em is} 
possible to define recursive constraints on
types as long as they specify an ancestor of their type.
For example, a correct
definition of {\bf list} is:
\begin{verbatim} 
*list* :< *top*.

ne-list := *list* &
 [ FIRST *top*,
   REST *list* ].

elist :< *list*.
\end{verbatim}
With this definition, the previously impossible feature structure 
can be strongly typed without causing an infinite structure:
\begin{verbatim}
[ ne-list 
  FIRST *top*,
  REST ne-list ].
\end{verbatim}
is expanded to:
\begin{verbatim}
[ ne-list 
  FIRST *top*,
  REST  [ ne-list
          FIRST *top*,
          REST *list* ] ]
\end{verbatim}
\item[Type inference --- features]
There are two cases where typing may fail due to
the feature introduction condition.  The first is illustrated
by the following example:
\begin{verbatim}
noun-lxm := lexeme &
 [ SYN [ HEAD noun,
         INDEX *top* ],
   SEM [ INDEX ref-index ] ].
\end{verbatim}
Here, the feature {\sc index} is only defined for structures of type
{\bf sem-struc}.  This type clashes with {\bf gram-cat} which is
the value of {\sc syn} specified
higher in the hierarchy.
This example generates the following error messages:
\begin{verbatim}
Error in NOUN-LXM:
   No possible type for features (INDEX HEAD) at 
   path (SYN)
Error in PN-LXM:
   No possible type for features (COMPS SPR HEAD INDEX) at 
   path (SYN)
\end{verbatim}

A slightly different error message
is generated when a type is specified at a node which is incompatible
with the node's features.  For instance:
\begin{verbatim}
synsem-struc := feat-struc &
 [ ORTH list-of-orths,
   SYN gram-cat & 
       [ MODE mode-cat ],
   SEM sem-struc ].
\end{verbatim}
\begin{error}
Error in SYNSEM-STRUC:
  Type of fs GRAM-CAT at path (SYN) is incompatible with features (MODE) which have maximal type SEM-STRUC
\end{error}
Note that, because the system attempts to expand other
type constraints after detecting this error,
there are a lot of similar error messages from the types for which
{\bf synsem-struc} is an ancestor.

\item[Type inference --- type constraints]
The final class of error is caused when type inference causes a
type to be determined for a node which then clashes with an existing
specification on that node.  
\begin{error}
Unification with constraint of GRAM-CAT failed at path (SYN)
\end{error}
\end{description}

\section{Lexical entries}

When lexicon files are loaded, they are only checked for syntactic correctness 
(as defined in \S\ref{lexsyntax})
--- entries are only fully expanded when they are needed
during parsing or generation
or because of a user request to view an entry.  Thus when loading
the lexicon, you may get syntactic errors similar to those discussed
in \ref{synwf} above, but not content errors since
the feature structures are not expanded
at load time. 
The lexicon is
stored in a temporary file (called by default {\tt templex}) when it
is read in, not in main memory.  

Incorrect lexical entries will therefore only be detected when you view a 
lexical entry or try to parse with it.  The error messages
that are obtained are very similar to some of those discussed for the
type loading above, specifically: 
\begin{description}
\item[Valid constraint description] 
\item[No Cycles]
\item[Consistent inheritance]
\item[All types must be defined]
\item[Strong typing --- features]
\item[Strong typing --- type constraints]
\end{description}

There is a menu option to 
do a complete check on a loaded lexicon for correctness:
{\bf Check lexicon} under {\bf Debug}.  See \ref{debug}, below.

\section{Grammar rules}

Unlike lexical entries, grammar and lexical rules are expanded at load time.
Therefore you may get error messages similar to those listed
above for lexical entries when the rules are loaded.
Rules must expand out into feature structures which have identifiable paths
for the mother and daughters of the rule (\S\ref{rules}).  For the `toy'
grammar, the mother path is the empty path and the daughter
paths are defined in terms of the {\sc args} feature.  For example:
\begin{center}
{\tiny
   $\avmplus{\att{binary-rule}\\
             \attval{ARGS}{\avmplus{\attvaltyp{FIRST}{sign}\\
                                    \attval{REST}{\avmplus{\attvaltyp{FIRST}{sign}}}}}}$}
\end{center}
Here the mother is given by the empty path, one daughter 
by the path {\sc args}.{\sc first} and another by the path
{\sc args}.{\sc rest}.{\sc first}.  
The mother path and a function which gives the
daughters in the correct linear order must be specified as system parameters
in the globals and user-fns files respectively (see \S\ref{mpglob} and
\S\ref{userfns}).


\section{Lexical and morphological rules}
\label{lrules}

Lexical rules are defined very similarly to grammar rules, but 
unlike grammar rules can have associated affixation information,
in which case we refer to them as morphological rules.
Lexical rules in general can be applied in any order that is
licensed by the affixation: thus lexical rules that don't
affect spelling can be interleaved with morphological rules.
The syntax for lexical rules without associated affixation
is identical to that for grammar rules.
In principle
lexical rules which do not affect affixation
may also be applied to phrases ---
all constraints on lexical rule application must be explicitly encoded,
e.g., by stipulating that lexical rules may only apply
to {\bf word}s rather than {\bf phrase}s.
Normally this is done by defining type(s) for lexical rules.

For example, if you look at the {\tt rattle} grammar, you will see that
there is a three-way distinction between the types {\bf lexeme},
{\bf word} and {\bf phrase} (the latter two being subtypes of
{\bf gram-cat}).
All the inflectional lexical rules
are either of type {\bf lrule-infl} or {\bf lrule-no-aff}.  In either
case they are defined to convert between a {\bf lexeme} and a {\bf word}.
In contrast, all the grammar rules are defined so that their daughters
are of type {\bf gram-cat}, so that they cannot apply to lexemes.
However, note that in the {\tt enddayfour} grammar, two other classes
of lexical rule are defined, one which converts {\bf lexeme}s to 
{\bf lexeme}s (dative shift) and one which converts {\bf word}s to
{\bf word}s.  

Morphological rules are lexical
rules with added orthographic
information describing affixation.
The morphology system expects that the
orthographic specification comes immediately after the 
{\tt :=} or {\tt :<} in the rule.
For example:\footnote{Note: the linebreak 
in the orthographemic specification below is just for readability,
it is not allowed in the actual file.}
\begin{verbatim}
plur-noun_infl_rule :=
%suffix (!s !ss) (!ss !ssses) (ss sses) 
(!ty !ties) (ch ches) (sh shes) (x xes) (z zes)
lex_rule_infl_affixed &
[ NEEDS-AFFIX true,
  ARGS [ FIRST [ AFFIX  plur-noun] ] ].
\end{verbatim}

Note that the file is read in in two passes, with information beginning
with \% being interpreted as part of the orthographemics specification,
and everything else being treated as feature structure descriptions,
just like ordinary lexical rules.
Lexical rules which don't affect affixation may be in a file
with morphological rules: the
former are simply ignored by the orthographemic
reader.
A description of the operation of the orthographemic component is
given in \S\ref{morphology}.

For discussion of how the rules are treated during parsing, see 
\S\ref{parse}.

\section{Irregular morphology}
\label{irregs}

Irregular morphology may be specified in association with a
particular lexical rule, but it is often more convenient
to have a separate file for irregular morphemes.  For compatibility
with the PAGE system, this file should take the form of a string
containing irregular form entries, one per line, each consisting of
a triplet: 
\begin{enumerate}
\item inflected form 
\item rule
\item stem
\end{enumerate}
For example:
\begin{verbatim}
"
fell PAST-VERB fall
felt PAST-VERB feel
"
\end{verbatim}
The interpretation of irregular forms is similar to the operation of the
regular morphology: that is, the irregular form is
assumed to correspond to the spelling obtained when the specified
rule is applied to a lexical entry with an orthography
corresponding to the stem.
If the value of 
{\tt *lex-rule-suffix*} (see \S\ref{mpglob}) is specified,
the actual rule name is constructed by concatenating this suffix
onto the supplied name (e.g. if *lex-rule-suffix* is {\tt "-lex-rule"}
then {\tt past-verb} will be interpreted as 
the rule {\tt past-verb-lex-rule}).

The default operation of the irregular morphology system is 
one of the very few cases where there may be an asymmetry in system
behaviour between parsing and generation: when parsing, 
a regularly derived form will be accepted even if there
is also an irregular form if the value of 
{\tt *irregular-forms-only-p*} (see \S\ref{mpglob}) is 
{\tt nil}.\footnote{Note that throughout the
LKB, Lisp conventions are used
for the specification of boolean parameters, so {\tt t} indicates
true and {\tt nil} indicates false.}
Thus, for example, {\it gived} will be accepted as well as {\it gave},
and {\it dreamed} as well as {\it dreamt}.  If the value is {\tt t},
an irregular form will block acceptance of a regular form.
Generation will always produce the irregular form if there is one:
currently it will simply produce the first form for a particular rule
even if there are alternative spellings.  So assume the following
is included in the irregulars file:
\begin{verbatim}
dreamed PAST-VERB dream
dreamt PAST-VERB dream
\end{verbatim}
Including both forms would be desirable for parsing if
{\tt *irregular-forms-only-p*} is true, because it would allow 
both variants to be accepted, but for generation, only {\it dreamed}
would be produced.


\section{Rule application during parsing}
\label{parse}

At this point, it is useful to bring together a few details
about how rules are applied during parsing.
It's important to notice that there is
considerable flexibility in the way rules are applied, 
since as far as possible
we try not to build in any unnecessary assumptions about processing.  
As far as
the parser is concerned, the primary distinction is between rules that affect
spelling changes (i.e. morphological rules) and those that don't.  The system
makes the assumption that all morphological processing happens at the level of
an individual word (although see \S\ref{multiword}).  Thus, when parsing, all
affixation operations apply before grammar rules.\footnote{There are some
examples in English morphology which arguably make this assumption problematic.
For example, the suffix {\it -ed}, which forms adjectives such as {\it
broken-hearted}, could be described as applying to an adjective noun phrase to
form an adjective.  Note that the adjective is obligatory (*{\it a hearted
lover}), the result is sometimes but not always hyphenated, and that the
process is productive (e.g., {\it rattan chaired terrace}).  We leave this
problem as an exercise for the reader.}
This means that it is the grammar writer's responsibility to
stop rules applying in unwanted ways.  On the whole, this is done
via the type system, as was discussed above in \S\ref{lrules}.
However there are two situations for which this mechanism 
is insufficient, or undesirably clunky.
The first case is the marking of rules which have associated spelling 
effects.  This is necessary to prevent these rules being applied
when they are not licensed by the spelling.  This is done by means
of a user-definable function, {\tt spelling-change-rule-p},
which specifies whether or not a rule
affects spelling (see \S\ref{userfns}).  In the 
version of the file {\tt userfns} supplied with the toy grammar,
this checks the rule to see if its type is {\bf lrule-infl}, or
a subtype of that.
The other case in which a rule should not be applied by the 
parser is when it is a redundancy rule which
is not intended to be applied productively, but we leave discussion of
redundancy rules 
to \S\ref{userfns}, since they are not used in any of the grammars
supplied.

The other complication in parsing is the notion of a root phrase,
discussed in the next section.


\section{Root file}
\label{root}

A valid parse is defined as a well-formed phrase that spans the entire
input string and which also satisfies any {\it root} conditions
that the grammar writer may impose.  For instance,  {\it Kim sleep},
with non-finite {\it sleep},
might be a valid phrase (because
of sentences such as {\it Sandy prefers that Kim sleep}), but blocked
from being a sentence because of a root condition that requires a finite
main verb.  

Root conditions are implemented in the LKB via a
parameter *start-symbol* which points to feature structure specifications
which must be compatible with all parses.  If the value of {\tt *start-symbol*}
is {\tt nil}, then any phrase which spans the input is accepted.
Otherwise,
the value of {\tt *start-symbol*}
may be a single identifier or a list of identifiers of feature
structures: the feature structure associated with a phrase must unify
with at least one of the start symbol feature structures in order
to be accepted as a valid parse.  The identifiers may be types, in which
case the associated feature structure is the constraint on the
type.  Root types are treated like any other type with respect to loading,
viewing etc.
 
In the sample {\tt toy} grammar, the root symbols are 
entries.  For convenience, root entries
can be stored in a separate file.
Root entries may be viewed via {\bf View} 
{\bf Lex Entry}.

\section{Parse tree node labels}
\label{treenodes}

The parse node file allows feature structures which control the
labelling of the nodes in the
parse tree to be defined (for parse tree display, see \ref{parsetree}).
We will refer to these structures as templates, but this use 
shouldn't be confused with any other use of the term.
In very general terms,
a node will be labelled with a template, iff the feature
structure associated with the node is matched by the feature
structure(s) associated with that template.
The format for parse node descriptions is
identical to that of lexical entries.  Parse nodes may be viewed via 
{\bf Lex Entry} on the {\bf View} menu.

The precise behaviour of the node labelling depends on
the value of {\tt *lkb-system-version*}.  

\subsection{Original version}

If {\tt *lkb-system-version*} is not set to 
{\tt :page}, then the templates consist of identifiers 
associated with single feature
structures.  If the template feature structure subsumes the structure
constructed for a node in a parse, the node is labelled with the identifier
of the template.
The first matching template is used, and if no template matches,
then the node is labelled with the type of its own feature structure.

\subsection{PAGE emulation version}
If {\tt *lkb-system-version*} is set to 
{\tt :page}, the parse tree labelling is more complex.
There are two classes of templates: {\it label} and {\it meta}
structures (see \S\ref{ptreeglob} for the parameters).
Each label template feature structure specifies 
a single string at a fixed path in
its feature structure ({\sc label-name} in the sample grammars).
For instance, the following is a label from the `textbook' grammar.
\begin{verbatim}
np :=  label &
 [ SYN [ HEAD noun,
         SPR < > ],
   LABEL-NAME "NP" ].
\end{verbatim}
Meta templates are used for things such as {\tt /} (e.g. to produce
the label {\tt S/NP}).
If meta templates are specified,
each meta template feature structure must
specify a prefix string and a suffix string
(by default, at the paths {\sc meta-prefix} and {\sc meta-suffix}).  
For instance, the following is specified as a meta template in the textbook
grammar:
\begin{verbatim}
slash := meta &
  [ SYN [ GAP [ LIST ne-list ] ],
    META-PREFIX "/",
    META-SUFFIX "" ].
\end{verbatim}

To calculate the label for a 
node, the label templates are first checked to find a match.
Matching is tested by unification of the template feature
structure, excluding the label path, with the feature structure
on the parse tree node.
For instance, if a parse tree node had the following feature structure:
\begin{verbatim}
[ SYN [ HEAD *top*,
        SPR < > ]].
\end{verbatim}
it would unify with
\begin{verbatim}
 [ SYN [ HEAD noun,
         SPR < > ]].
\end{verbatim}
and could thus be labelled {\tt NP}, given the structure above.
There is a parameter, {\tt *label-fs-path*}, which allows
templates to be checked on only the substructure of the node
feature structure which follows that path, but
this is parameter is set to the empty path in the textbook grammar. 

If meta templates are specified, the feature structure at the
end of the path specified by the parameter {\tt *recursive-path*}
(({\sc syn gap list first}) in the textbook grammar)
is checked to see whether it matches a meta template.
If it does, the meta structure is checked against the label templates 
(the parameter {\tt *local-path*}
allows them to be checked against a substructure of the meta structure).
The final node label is constructed as the concatenation
of the first label name, the meta prefix, the label of
the meta structure, and the meta suffix.  


\section{Ancillary files}
\label{ancil}

The following are grammar specific files which parameterize the behaviour of
the system.  These should not be relevant to users who are working on an
existing grammar, so details are left to Chapter~\ref{advanced} and the
appendices.

\subsection{Globals}

Contains parameters that may be changed by the user.  
Details are given in Appendix~\ref{glob}.

\subsection{User preferences}

Some of the parameter settings can be controlled interactively
via {\bf Set options} on the {\bf Options} menu 
(see \S\ref{display}).
Specifying this file in the script means that any preferences 
which are set in one session will be saved for a subsequent session.
The user should not need to look at this file and 
should not edit it, since any changes may be overwritten.
Details of the individual
parameters are given in Appendix~\ref{glob}.

\subsection{User defined functions}

There are a few functions which control parameters such as rule
ordering which may differ in different grammars.  These are
in the user-fns file (written in Lisp).
Details are given in Appendix~\ref{glob}.


\chapter{LKB user interface}
\label{ui}

In this chapter, we explain the details of the menu commands
and other aspects of the graphical user interface.
Note that most of this is not relevant for people using the tty version
of the system.

Apart from the LKB top window in the ACL/CLIM interface, there are
a number of other types of LKB window which are described in this chapter,
which display the following classes of object:
\begin{enumerate}
\item Type hierarchy
\item Feature structure 
\item Parse output results
\item Individual parse trees
\item Parse chart
\end{enumerate}
Note that in the ACL/CLIM version, all windows for these structures
have two buttons: {\bf Close} and {\bf Close all}.
{\bf Close} will close the individual window, {\bf Close all}
will close all windows of that class --- e.g., all type hierarchy windows
etc.

\section{Top level commands}

The top level command window is displayed when the LKB is loaded
in the ACL/CLIM interface.
Note that, in the ACL version, it is possible for the interaction
window to be closed inadvertently.
It may be reopened by evaluating
(clim-user::restart-lkb-window) in Lisp (i.e., the
{\tt *common-lisp*} buffer in emacs).

In the MCL interface, the LKB menu is added to the top menu.

In this section the LKB menu
commands will be briefly described in the order in
which they appear in the `mini' interface.
Commands which are only in the `advanced' version are indicated by
an asterisk.  To switch between versions, use the {\bf Shrink menu}
or {\bf Expand menu} commands under {\bf Options}.

\subsection{Quit}

ACL/CLIM only.  Prompts the user to check if they really do want to
quit.  There are three options when running from source files:
\begin{description}
\item[Lisp] Ends the Lisp session.
\item[LKB] This just closes the LKB top window.  It may be reopened by
evaluating (clim-user::restart-lkb-window).
\item[Cancel] Cancels the quit.
\end{description}
Only the first and the last option are present when running from the distributed
image.

\subsection{Load}
\label{load}

The basic commands allow the loading of a script file
and the reloading of the same file (normally after some editing).
The advanced 
commands allow the user to reload various types of file using the
menu interface.   
A script
is used initially to load a set of files, and then individual files
can be reloaded as necessary after editing.
Reloading files is not as safe as reloading the script, since there
are various circumstances which may cause an individually reloaded
file to cause a grammar to behave differently from the expected behaviour
(some of which are mentioned below).
The reload options are therefore only on the `advanced' menu.

\paragraph{Complete grammar \ldots}

This prompts for a script file to load, and then loads the 
grammar as discussed above.

\paragraph{Reload grammar}

This reloads the last loaded script file.

\paragraph{Reload constraints*}

\paragraph{Reload leaf types*}

\paragraph{Reload lexicon*}

\paragraph{Reload grammar rules*}

\paragraph{Reload lexical rules*}
includes morphological rules

\paragraph{Reload tree nodes*}

\paragraph{Reload other entries*}


\subsection{View}
\label{view}

These commands all concern the display of various entities in the grammar.

\paragraph{Type hierarchy}
Displays a type hierarchy window.

Prompts for the highest node to be displayed.
The check box allows `invisible' types, such as glbtypes, to be
displayed if set.  Details of the type hierarchy window are in 
\S\ref{thier}.

\paragraph{Type definition}
\label{type-def}
Shows the definition of a type constraint plus the type's 
parents.

Prompts for the name of a type. 
If the type hierarchy window is displayed, scrolls the
type hierarchy window so that the chosen type is centered and
highlighted.  
Displays the type's
parents and the constraint specification in a feature
structure window:
details of feature structure windows are in \S\ref{activefs}.


\paragraph{Expanded type}
\label{type-exp}
Shows the fully expanded constraint of a type.

Prompts for the name of a type. 
If the type hierarchy window is displayed, scrolls the
type hierarchy window so that the chosen type is centered and
highlighted.  
Displays the type's
parents and the full constraint on the type.

\paragraph{Lex entry}
\label{psort-exp}
The expanded feature structure associated with a lexical entry
(or parse node label or root structure etc).

Prompts for the identifier of a lexical entry (or a parse node
label or root structure).
Displays the associated feature structure.

\paragraph{Word entries}
All the expanded feature structures associated with a particular
orthographic form.

Prompts for a word stem.
Displays the feature structures corresponding to lexical entries
which have this stem.

\paragraph{Grammar rule}
Displays a grammar rule.

Prompts for the name of a grammar rule, displays it in a
feature structure window.

\paragraph{Lexical rule}
Displays a lexical or morphological rule.

Prompts for the name of a lexical rule, displays its feature structure in a
window.

\subsection{Parse}
\label{parsecommands}

\paragraph{Parse input}

This command prompts the user for a sentence (or any string), and calls the
parser 
(possibly modifying the input according to the
user-defined function {\tt preprocess-sentence-string}, see \ref{userfns}).
A valid parse is defined as a structure which spans the entire input
and which will unify with the feature structure(s) 
identified by the value of the parameter {\tt *start-symbol*},
if specified
(i.e., the root(s), see \S\ref{root} and \S\ref{parse}).
If there is a valid parse,
the parse tree(s) are displayed.  Currently, in the Mac version,
all the parse trees are displayed in separate windows, while in
the ACL/CLIM version a single window with very small parse trees 
is displayed (see \S\ref{parseout}).

Note that it is sometimes more useful in ACL/CLIM
to run the parser from the Lisp
command line interface, since this means that
any results generated
by post-processing (e.g. by MRS functions, \S\ref{mrs}) will appear in an 
emacs buffer and can be searched, edited and so on.
It may also be useful to do this
if you have to use emacs to enter diacritics.
The command {\tt do-parse-tty} is therefore available in
non-tty mode as well as tty-mode --- it takes a string as an argument.
For example:
\begin{verbatim}
(do-parse-tty "Kim sleeps")
\end{verbatim}
In the tty version of the system, this outputs a bracketed
structure --- otherwise the normal graphical parse output is produced.

\paragraph{Show parse}
Shows the tree(s) from the last parse again.  

\paragraph{Show chart}
Shows the parse chart (see \S\ref{chart}).

\paragraph{Batch parse}
This prompts for the name of a file which contains sentences
on which you wish to check the operation of the parser,
one sentence per line (see the file {\tt test.items} in the
``toy'' grammar).  It then prompts for the name of a new file
to which the results will be output.  The output simply tells you
the number of parses found (if any) for each sentence in the
input file, and gives a time for the whole set at the end.
This is a very simple form of test suite: vastly 
more functionality is available from the \itsdb\ machinery which
can be run in conjunction with the LKB (see \ref{tsdb}).

\paragraph{Compare*}
FIX - add description

\subsection{Debug}
\label{debug}

\paragraph{Check lexicon} Prompts for a file to which any
error messages will be output, then expands all entries in the lexicon.

\paragraph{Print chart}
Displays the chart (crudely) in the LKB interaction
window.  This can be useful if the parse has
failed, in order to select an appropriate edge 
(the number in \verb+[]+s) to display with {\bf
Show edge}.

\subsection{Options}
\label{display}

\paragraph{Expand/Shrink menu} Changes the LKB top menu so that the
asterisked commands are added/removed.

\paragraph{Set options} Allows interactive setting of some system
parameters.  
FIX --- more details.
See Appendix~\ref{glob} for full details of all parameters.

Note that the values of the boolean parameters are specified 
in the standard way for Common Lisp: that is, {\tt t} indicates
true and {\tt nil} indicates false.

\paragraph{Save display settings} Save shrunkenness of feature structures

\paragraph{Load display settings} Load pre-saved display setting file.

\subsection{Generation*}
\label{gencommands}

The generator is described in more detail in \S\ref{generator},
but is currently in a fairly early stage of development.
It operates in a very similar manner to the parser
but relies on the use of an MRS style semantics,
thus it will only work with grammars that produce 
such semantics.  
% UPDATE POINT
The textbook grammar distributed with
the system will work with some sentences.
At the moment,
there is no way of entering an MRS input other than by parsing
a sentence which produces that MRS.

\paragraph{Generate \ldots}
Only usable if a sentence has been parsed.  Prompts the user for
the number of an edge with the desired MRS associated with it
(usually the top node in the parse tree for the relevant parse).
Outputs a list of sentences which can be clicked on to
obtain the associated parse trees.

\paragraph{Redisplay result}
Redisplays the results from the last sentence generated.

\paragraph{Show chart}
Displays a chart from the generation process (see \S\ref{chart}).  Note that 
the ordering of items on the chart is controlled by their semantic
indices.  

\paragraph{Index}
Indexes the lexicon and the rules for the generator.  Has to
be run before anything can be generated.

\subsection{Tidy up*}
\label{tidy}
This command clears expanded lexical entries which are cached.  
If accessed again they will be read
from file and expanded again.

Expansion of a large number of word senses will tend to fill up memory
with a large number of feature structures.  Most commands which are
likely to do this actually clear the cached feature structures
themselves, but if memory is becoming restricted this option can be
used.  However often more memory will be reclaimed by closing any open
feature structure windows, and especially the Type Hierarchy window.

\subsection{Output*}

\paragraph{Dump system}
\label{dumplkb}
This creates an image file, which will essentially save the state of
the current system.  It can be used before a grammar is loaded, in which
case the image may be used by multiple people, or to save state after a 
grammar has been loaded, though because of the handling of the temporary 
lexicon files (see \S\ref{sysglob}), it really only makes sense to do this 
for a single user.  
The dumped system 
relies on the cached lexicon, so if the templex
or templex-index files are deleted or modified, the saved image will
not work correctly.

The behaviour is quite different in MCL and ACL:

In MCL, choosing {\bf Dump system} will prompt for a file name for the saved
image and then save the image, finishing the Lisp session.  
The session can then be restarted by double-clicking on the new image file.

In earlier versions of ACL, standalone images were possible.  In ACL 5.0, the
file saved is not an executable, but relies on the standard executable.  Thus,
when restarting Lisp, you specify the saved file when emacs prompts you for a
saved image (see e.g., \S\ref{acl-emacs}).  If you are not using emacs, the
saved file is specified as a command line argument (see the ACL documentation).
Note that the file is required to be of type {\tt dxl}.


\section{Type hierarchy display}
\label{thier}

By default, a type hierarchy
is displayed automatically after a grammar is loaded 
(though this default must be turned off for grammars that use very
large numbers of types, see \S\ref{sysbeh})).
The type hierarchy
can also be accessed via the top level command {\bf Type hierarchy}
in the {\bf View} menu, as discussed above in \ref{view}.

The top of the hierarchy, that is the most
general type, is displayed at the left of the window.  The window is
scrollable by the user and is automatically scrolled by various {\bf
View} options.  Nodes in the window are active; clicking on a type
node will give a menu with the following options:
\begin{description}
\item[Shrink/Expand] Shrinking a type node results in the type
hierarchy being redisplayed without the part of the hierarchy which
appears under that type being shown.  The shrunk type is indicated by
an outline box.  Any subtypes of a shrunk type which are also subtypes
of an unshrunk type will still be displayed.  Selecting this option on
a shrunk type reverses the process.
\item[Type definition] 
Display the definition for the constraint on that 
type (see Section~\ref{view},
above).
\item[Expanded type]
Display the expanded constraint for that type (see
Section~\ref{view}, above).
\item[New hierarchy]
Displays the type hierarchy under the clicked-on node in a new
window, via the same dialog as the top-level menu command.
This is useful for complex hierarchies.
\end{description}

\section{Feature structure display}
\label{activefs}

Most of the view options display feature structures in a window.  
Our usual orthographic conventions for
drawing feature structures are followed; types are lowercased bold,
features are uppercased.  Feature structure windows
are active - currently the following operations are supported:
\begin{enumerate}
\item Clicking on the window identifier (i.e. the first item in the
window) will display a menu of options which apply to the whole
window.
\begin{description}
\item [Output TeX] Outputs the FS as TeX macros to a file selected by
the user.  
\item[Apply lexical rule]
Only available if the identifier points to something that might
be a lexical entry.
It prompts for a lexical or morphological rule
and applies the rule to the entry.  The result is displayed if
application succeeds.  
\item[Apply all lex rules]
Only available if the identifier points to something that might
be a lexical entry.
This tries to apply all the defined
lexical and morphological rules to the entry, and to any results of
the application and so on.  (To prevent infinite recursion on
inappropriately specified rules the number of applications is limited.)
The results are displayed in summary form,
see Figure~\ref{lexruleapp}.  Clicking on one of
these summaries will display the resulting feature structure.
% FIX - {lexruleapp}
\item [Show source] Shows the source code for this structure 
if ACL/CLIM is being used with emacs --- see \S\ref{emacs}
(not available with all structures)
\end{description}
\item Clicking on a type (either a parent, or a type in the feature
structure itself) will give a sub-menu with the following options:
\begin{description}
\item[Hierarchy] Scroll the type hierarchy window so
that the type is centered.  If the type hierarchy window is not visible,
use {\bf View Type Hierarchy} to redisplay it in order to make
this command available.
%\item[Help] This menu option is only available if a comment has been
%specified for a type.  The type name and comment are printed in a
%separate window.
\item[Shrink/Expand] 
Shrinking means that the feature structure will be redisplayed without
the feature structure which follows the type being shown.  The
existence of further undisplayed structure is indicated by a box round
the type.  Atomic feature structures may not be shrunk.  Shrinking
modifies the actual feature structure being displayed, so that if the
window is closed, and subsequently a new window opened onto that
feature structure, the shrunken status will be retained.  Furthermore,
any feature structures which are formed by unification with the
shrunken feature structure, will also be displayed with equivalent
parts hidden.  Thus if the {\em expanded} constraint on a type has
parts shrunk, any lexical entry which involves that type, which is
expanded subsequent to any shrinking on the type, will also be
displayed with parts hidden.  Since type constaints 
are evaluated when they are
read in, shrinking parts of the constraint on a type will not affect
the daughters of that type.

If this option is chosen on an already shrunken feature structure then
the feature structure will be expanded.  Again this can affect the
display of other structures.

The shrunkenness state may be saved via and loaded via the
{\bf Save/Load display settings} commands on the {\bf Options} menu
(see \S\ref{display}).
\item [Show source] Shows the source code for this structure 
if ACL/CLIM is being used with emacs --- see \S\ref{emacs} (not available
with all structures)
\item[Type definition] 
Display the definition for that type.
\item[Expanded type]
Display the expanded definition for that type.
\item[Select]  Selects the
feature structure rooted at the clicked node in order to test unification.
\item[Unify] Attempts to unify the previously selected feature structure
with the selected node.  Success or (detailed) failure messages
are shown in the LKB Top window.  See \ref{unifcheck} for further
details.
\end{description}
Clicking on a type which is in fact a string, and thus has no
definition etc, will result in the warning beep, and no display.
\end{enumerate}

The order in which features are displayed is determined according to
their order when introduced in the type specification file.  For
example, assume we have the following fragment of a type file:
\begin{verbatim}
sign := feat-struc &
 [ SYN *top*, 
   SEM *top* ].

word := sign &
 [ ORTH string ].
\end{verbatim}
then when a feature structure of type {\bf sign} is displayed, the
features will be displayed in the order {\sc syn}, {\sc sem}; when a {\bf
word} is displayed the order will be {\sc syn}, {\sc sem}, {\sc orth}.
This ordering can be changed or further specified by
means of the parameter {\tt *feature-ordering*}, which consists of
a list of features in the desired order (see \S\ref{disglob}).

\subsection{Unification checks}
\label{unifcheck}

The unification check mechanism operates on feature
structures that are displayed in windows.  One can
temporarily select any feature structure or part of a feature structure
by clicking on the relevant node in a displayed window and choosing
{\bf Select} from the menu.  Then to check whether this structure
unifies with another, and to get detailed messages if unification fails,
find the node corresponding to the second
structure, click on that, and choose {\bf Unify}.
If the unification fails, failure messages will be shown in the
top level LKB window.  If it succeeds, a new feature structure
window will be displayed.  This can in turn be used to check further
unifications.

For example, consider why ``Kim sleep'' does not parse in the buggy
grammar.  
In fact, the parse is blocked by the root mechanism
because we will want to license the
phrase ``Kim sleep'' in some contexts, e.g., 
{\it Sandy prefers that Kim sleep}.
By looking at the 
parse chart for the sentence, one can determine that
the hd-spec-rule will apply to combine the VP for
the infinitival form of {\it sleep} and the NP for {\it Kim}.
However this is not a valid parse, because the result is not unifiable with the
feature structure named by {\tt root}.

You can verify this by the following procedure:
\begin{enumerate}
\item Load the {\tt buggy} grammar.
\item {\bf Parse input} on ``Kim sleep''.  The parse will fail.
\item Do {\bf Show chart}.  A chart window will appear.
\item Click on the node in the chart that says hd-spec-rule and 
select {\bf Feature structure}. A feature structure window will
appear.
\item 
Click on the root node of the feature structure (i.e., the one
that says {\bf birule-hdfinal} in the 
new feature structure
window and 
choose {\bf Select}.
\item Display the feature structure for {\tt root} by choosing
{\bf Lex entry} from the {\bf View} menu ({\tt root} is not actually a
lexical entry, but it is structurally similar, and we wanted to avoid putting
too many items on the {\bf View} menu.)
\item Click on the root node of {\tt root} (i.e., the node
with the type {\bf phrase}) and choose {\bf Unify} from the pop-up menu.
\item Look at the Lkb Top window.  You should see:
\begin{verbatim}
Unification of (INF) and (FIN) failed at path < SYN : HEAD : FORM >
\end{verbatim}
\end{enumerate}

To check why a rule does not work, a more complex
procedure is sometimes necessary, because of the need to see whether
three (or more) feature structures can be unified.  
Suppose we want to check to see why the head-specifier-rule
does not apply to ``the book sleep'' using the
the finite form of {\it sleep}. This is 
complex, since both the feature structures for the NP for ``the book'' and
the VP for 3pl {\it sleep} will individually unify with the
daughter slots of the hd-spec-rule feature structure.
So we need to use the intermediate results 
from the unification test mechanism.

The following description details one way to do this.
\begin{enumerate}
\item Parse ``the book sleep''.  The parse will fail.
\item Select {\bf Show chart} to show the edges produced
during the failed parse.
\item Bring up the feature structure window for 
the uninstantiated hd-spec-rule 
(either via the {\bf View Rule} command in the top window menu
or from the chart).
\item Find the node in the hd-spec-rule window corresponding
to the specifier (i.e. the node labelled {\bf gram-cat} at
the end of the path {\tt (ARGS FIRST)}).  Because this is
coindexed with {\tt (NH1)} you may have to follow the coindexations
to get to the node itself.
Click on it and choose {\bf Select}.
\item Find the
node for {\tt the book} in the parse chart window
and choose {\bf Unify}.
(Note that this is a shortcut which is equivalent to displaying the 
feature structure for the
phrase via the {\bf Feature Structure} option in the 
chart menu and then selecting {\bf Unify} from the menu on 
the top node of the feature structure displayed.)
Unification should succeed and a new feature structure
window (titled Unification Result) will be displayed.  
\item Find the node in the new Unification Result window corresponding
to the head in the rule, i.e. the node at
the end of the path {\tt (ARGS REST FIRST)}, click on it and choose {\bf Select}.
\item Click on the
node in the parse chart for the finite form of
{\tt sleep} (i.e., the one labelled
{\tt non3sg-v\_irule}) and choose {\bf Unify}.
\item This time unification should fail, with the following message
in the LKB Top window:
\begin{verbatim}
Unification of 3SING and NON-3SING failed at path 
< SYN : SPR : FIRST : SYN : HEAD : AGR >
\end{verbatim}
\end{enumerate}

Note that it is important to 
{\bf Select} the node in the rule and {\bf Unify} the daughter nodes
because the result shown always corresponds to the 
initially selected feature structure.  
We want this to be the rule so we can try
unifying the other daughter into the partially instantiated result.


\section{Parse output display}
\label{parseout}

The parse output display is intended to give an easily readable
overview of the results of a parse, even if there are several
analyses.  The display shows a parse tree for each separate
parse, using a very small font to get as many trees as possible on
the screen.  Clicking on a tree gives two options:
\begin{description}
\item[Show enlarged tree] produces a full size parse tree window, as
in \S\ref{parsetree}, with clickable nodes etc
\item[Highlight chart nodes] will highlight the nodes on the 
parse chart corresponding to this tree.  If the parse chart is not currently
displayed, this option will bring up a new window (see \S\ref{chart}
for details of the chart display).
\end{description}
The details of the parse tree display are discussed in the next section.

The parse output display is currently only available on the ACL/CLIM
version of the interface, the MCL version produces a parse tree
window for each parse.

Besides the standard {\bf Close} and {\bf Close all} buttons,
the window has a button for {\bf Show chart} for convenience.
It has the same effect as the top-level menu command.


\section{Parse tree display}
\label{parsetree}

Parse trees are convenient abbreviations for feature structures 
representing phrases and their
daughters.  When a sentence is successfully
parsed, the trees which display valid
parses are shown, but parse trees may also be displayed for any edge in 
a parse chart (see \S\ref{chart}).
The nodes in the parse tree are labelled with the name of the 
(first) parse node label which has a feature structure which matches 
the feature structure associated with the node, if such a label 
is present.  The matching criteria are detailed in \ref{treenodes}.  

Clicking on a node in the parse tree will
give the following options:
\begin{description}
\item [Feature structure - Edge X]  (where X is the edge number in the
parse chart) displays the feature structure associated
with a node.
\item [Rule X] (where X is the name of the
rule used to form the node) 
displays the feature structure associated with the rule.
\end{description}

The input words are 
indicated in bold below the terminal parse tree nodes --- if any morphological
rules have been applied, these are indicated by nodes beneath the words
if the parameter {\tt *dont-show-morphology*} is nil, but not shown otherwise.
Similarly the parameter {\tt *dont-show-lex-rules*} controls whether or
not the lexical rule applications are displayed.

\section{Chart display}
\label{chart}

The chart is a record of the structures that the LKB system has built in
the course of attempting to find a valid parse or parses.\footnote{Chart
parsing is a technique
described in many introductory NLP and AI texts.  We don't assume any
detailed knowledge of it here.}
The parser works bottom-up: that is, it does not use any
information about the global goal of constructing a sentence (or
whatever the roots are) in order to constrain the structures that it
builds.  A structure built by the parser and put on the
chart is called an {\it edge}: edges are identified by an
integer ({\it edge number}).
All edges that are displayed on the chart 
represent complete rule applications.\footnote{Note for people
who know about chart parsers: the LKB system does not
use active edges.  We have found that an active chart parser 
is actually less efficient overall for the large scale LinGO ERG,
because of its greater space utilization.}

The chart window shows the words of the sentence to the left,
with lines indicating how the structures corresponding to these words
are combined to form phrases.  Each node in the chart display proper
corresponds to an edge in the chart.
Its label shows the following information:
\begin{enumerate}
\item The nodes of the input that this edge covers (where the first node
is notionally to the left of the first word and
is numbered 0, just to show we're doing real computer science here).
\item The edge number (in square brackets)
\item The name of the rule used to construct the edge
(or the orthography of the lexical item)
\end{enumerate}
For instance, in the chart for the sentence {\it Kim does like Sandy},
the nodes for the input are numbered
\begin{quote}
.$_{0}$ {\it Kim} .$_{1}$ {\it does} .$_{2}$ {\it like} 
.$_{3}$ {\it Sandy} .$_{4}$  
\end{quote}
In the chart display resulting from parsing this sentence 
in the textbook grammar, one edge is specified as:
\begin{verbatim}
2-4 [25] HEAD-COMPLEMENT-RULE
\end{verbatim}
Thus this edge is edge number 25, it covers {\it like Sandy}, 
and was formed by applying the {\tt head-complement-rule}.

Clicking on an edge node results in the following menu:
\begin{description}
\item[Feature structure] Shows the feature structure for the
edge
\item[Tree] Shows the tree headed by the phrase corresponding to
this edge
\item[Rule X]  Shows the feature structure for the rule that was used
to create this edge
\item[Highlight nodes] Highlights all the nodes
in the chart for which the chosen node is an ancestor or a descendant.
\item[New chart] Displays a new chart which only contains nodes
for which the chosen node is an ancestor or a descendant (i.e. those that
would be highlighted).  This is useful for isolating structures
when the chart contains
hundreds of edges.
\item[Unify] This is only shown if a feature structure
is currently {\bf Select}ed for the unification test --- see 
\S\ref{unifcheck}.
\end{description}

\chapter{Advanced features}
\label{advanced}

The previous chapters have described the main features of the LKB 
system which are utilized in the distributed grammars.
There are a range of other features which are in some sense
advanced: for instance because they concern facilities for using the
LKB with grammars in frameworks other than
the variety of HPSG assumed here, 
or because they cover functionality which is less well
tested (in particular defaults and generation), or because 
the features are primarily used for efficiency (some of these are
rather specific to the LinGO ERG).
These features are described in this chapter.
It should probably not be read by anyone who hasn't worked through
the earlier material in some detail.
On the whole, this chapter assumes rather more knowledge of 
NLP and of Lisp programming than we have in the earlier chapters.

The chapter starts with a section which gives some hints on 
starting to write an entirely new grammar.

\section{Defining a new grammar}
\label{own}

If possible, it is best to start from one of the sample grammars
rather than to build a new grammar completely from scratch.
However, when building a grammar in a framework other than 
HPSG, the existing source may not be of much use.
These notes are primarily intended for someone
trying to build a grammar almost from scratch.

The first step is to try and decide whether the LKB system is
going to be adequate for your needs.  The system is not designed for
building full NLP applications (though it could form part of such a 
system for teaching or research purposes, and might have some
utility for prototyping).  
For research, there are some
limitations imposed by the typed-feature structure 
formalism.\footnote{The system can be used to 
build grammars which are Turing equivalent, so 
these comments aren't about formal power.}
There's no way of describing any form of transformation
or movement directly, though, as is well known, feature structure
formalisms have alternative ways of achieving the same effect.

Even with respect to other typed feature structure formalisms,
the LKB has some self-imposed limitations.  There is
no way of writing a disjunctive or negated
feature structure (see \ref{disj}).
In many cases, grammars can be reformulated to eliminate disjunction
in favour of the use of types which express generalisations.
Occasionally it may be better to have multiple lexical entries
or grammar rules.  
The LKB does not support negation,
although we do intend to release a version which incorporates inequalities
(Carpenter, 1992).  More fundamentally for HPSG, the system
does not support set operations.  Alternative formalisations are
possible for most of the standard uses of sets.  For example,
operations which are described as set union in Pollard and Sag (1994)
can be reformulated as a list append.  We think we have good reasons
for adopting these limitations, and that the LinGO ERG
shows that an HPSG grammar can be built without these devices,
so the system is unlikely to change.

There are other limitations which are less fundamental,
though they might require considerable reimplementation.
In these cases, it may be worth considering using the LKB system if
you have some programming experience, or can persuade
someone else to do some programming for you.  For instance,
the current system for encoding orthographemics is very restricted.  
However, the interface to the rest of the system is fairly well-defined,
so it might be relatively easy to replace.  

If you've decided you want to use the LKB system, then you should start by
defining a very simple grammar.  
It will make life simpler 
if you copy the definitions for basic types like lists and difference
lists from the existing grammars, so you do not have to redefine the
global parameters unnecessarily.  
Similarly, if you are happy to use a list feature {\sc args} to
indicate the order of daughters in a grammar rule, you will not
have to change the parameters which specify daughters or the
ordering function.
There are some basic architectural decisions
which have to be taken early on.  For instance, if you decide
on a {\bf lexeme}, {\bf word}, {\bf phrase} distinction, this will
affect how you write lexical and grammar rules.  

As described at the beginning of Chapter~\ref{srcfiles},
you need to have distinct files for each class of object. 
You will have to write a script 
file to load your grammar files --- the full details of how to
do this are given below, but the easiest technique is to copy
an existing script and to change the file names, then to look at the
documentation if you find the behaviour surprising.  To start off
with, aim at a grammar which is comparable in scope to the `toy' grammar
supplied with the system: i.e., use fully inflected forms, 
one or two grammar rules, a very small number
of lexical entries and just enough types to make the structures well-formed
(the `toy' grammar actually has many more types than are strictly 
speaking needed, because it was defined 
to make it relatively straightforward
to extend using a predefined feature structure architecture).

There are many practical aspects to grammar engineering, many of which
are similar to good practise in other forms of programming.
One aspect which is to some extent peculiar to grammar writing is the
use of test suites (see e.g., Oepen and Flickinger, 1998).
The LKB system is compatible with the \itsdb\  system, as discussed
in \S\ref{tsdb}.
You should use a test-suite of sentences as soon as you have got
anything to parse, to
help you tell quickly when something breaks.  You should also
adopt a convention with respect to format of description files right
from the start: e.g., with respect to upper/lower case distinctions,
indentation etc.  Needless to say, comments 
and documentation are very important.  
You may find the strong typing annoying at first, but we have found that
it catches a large number of bugs quickly which can otherwise go undetected
or be very hard to track down.  

At some point, if you develop a moderate size grammar, or have a slow machine,
you will probably start to worry about processing efficiency.
Although this is partly a matter of the implementation of the LKB
system, it is very heavily dependent on the grammar.  For instance,
the textbook grammar is not a particularly
good model for anyone concerned with efficient processing, partly 
because it makes use
of large numbers of non-branching rules.
The LKB system code has been optimized with respect to the LinGO
ERG, so it may well have considerable inefficiencies for other
styles of grammar.  For instance, the ERG has about 40 rules (constructions)
--- grammars with hundreds of rules
might benefit for an improvement in the rule lookup mechanism.
We have put a lot of effort in making processing efficient
with type hierarchies like the ERG's
which contains thousands of types, with a reasonably
high degree of multiple inheritance.  However, there are
cases of multiple inheritance which the algorithm
for determining greatest lower bounds will not handle gracefully.

\section{Script files}
\label{script-adv}

Here is an example of a complex script file, as used for
the CSLI LinGO ERG:
\begin{verbatim}
(defparameter *grammar-version* "LinGO (jul-98)")

(time
(progn
(setf *grammar-directory* (parent-directory)) ; needed for MRS
(lkb-load-lisp (this-directory) "globals.lsp")
(lkb-load-lisp (this-directory) "user-fns.lsp")
(load-lkb-preferences (this-directory) "user-prefs.lsp")
(lkb-load-lisp (this-directory) "templates.lsp")
(lkb-load-lisp (this-directory) "checkpaths.lsp" t)
(load-irregular-spellings 
     (lkb-pathname (parent-directory) "irregs.tab"))
(read-tdl-type-files-aux
     (list (lkb-pathname (this-directory) "extra.tdl")
           (lkb-pathname (parent-directory) "fundamentals.tdl")
           (lkb-pathname (parent-directory) "lextypes.tdl")
           (lkb-pathname (parent-directory) "syntax.tdl")
           (lkb-pathname (parent-directory) "lexrules.tdl")
           (lkb-pathname (parent-directory) "auxverbs.tdl")
           (lkb-pathname (parent-directory) "multiletypes.tdl")
           (lkb-pathname (this-directory) "lkbpatches.tdl")
           (lkb-pathname (this-directory) "mrsmunge.tdl"))
     (lkb-pathname (this-directory) "settings.lsp"))
(read-tdl-leaf-type-file-aux
           (lkb-pathname (parent-directory) "letypes.tdl"))
(read-tdl-leaf-type-file-aux
          (lkb-pathname (parent-directory) "semrels.tdl"))
(read-cached-lex-if-available 
          (lkb-pathname (parent-directory) "lexicon.tdl"))
#|
(read-tdl-lex-file-aux 
          (lkb-pathname (parent-directory) "lexicon.tdl"))
|#
(read-tdl-grammar-file-aux 
          (lkb-pathname (parent-directory) "constructions.tdl"))
(read-morph-file-aux (lkb-pathname (this-directory) "inflr.tdl"))
(read-tdl-psort-file-aux 
          (lkb-pathname (parent-directory) "roots.tdl"))
(read-tdl-lex-rule-file-aux 
          (lkb-pathname (parent-directory) "lexrinst.tdl"))
(read-tdl-parse-node-file-aux 
          (lkb-pathname (parent-directory) "parse-nodes.tdl"))
(lkb-load-lisp (this-directory) "mrs-initialization.lsp" t)
)
)
\end{verbatim}

We won't go through this in detail, but
note the following:
\begin{enumerate}
\item The command to read in the script file is specified
to carry out all the necessary initializations of grammar parameters
etc.  So although it might look as though a script file can
be read in via {\tt load} like a standard Lisp file, this will
cause various things to go wrong.
\item The userprefs file is loaded automatically.  Here we assume it's
kept in the same directory as the other globals file, which allows
a user to set up different preferences for different grammars.
\item The {\tt templates.lsp} file contains a list of 
parameterized templates as used in TDL, but here defined in Lisp
to avoid the necessity to reimplement the TDL reader.
Parameterized templates are only supported for PAGE compatibility,
and should not be used when developing a new grammar.
\item The {\tt checkpaths} file is loaded to improve efficiency,
as discussed in \S\ref{checkpaths}.  The third argument to 
{\tt lkb-load-lisp} is {\tt t}, to indicate that the file is optional.
\item There is a list of type files read in by {\tt read-tdl-type-files-aux}
--- the second argument to this function is a file for
display settings (see \S\ref{display}).
\item Two type files are specified as leaf types (see \S\ref{leaftypes}).
\item The lexicon is cached so that it can be read in quickly if it is
unaltered: see \S\ref{cache}.
\end{enumerate}

\subsection{Loading functions}
\label{loadfns}

The following is a full list of available functions for loading
in LKB source files written using the TDL syntax.  All files
are specified as full pathnames.  Unless otherwise
specified, details of file format etc are 
specified in Chapter~\ref{formal} and error messages
etc are in Chapter~\ref{srcfiles}.

\lispcommand{read-tdl-type-files-aux {\it file-names} \&optional 
{\it settings-file}}

Reads in a list of type files and processes
them.  An optional settings file 
controls shrunkenness (see \S\ref{display}).
If you wish to split types into more than one file, they 
must all be specified in the file name list, since processing 
assumes it has all the types (apart from leaf types).

\lispcommand{read-tdl-leaf-type-file-aux {\it file-name}}

Reads in a leaf type file.  There may be more than one such command
in a script.
See \S\ref{leaftypes}.

\lispcommand{read-tdl-lex-file-aux {\it file-name}}

Reads in a lexicon file. There may be more than one such command
in a script.

\lispcommand{read-cached-lex-if-available {\it file-name(s)}}

Takes a file or a list of files.
Reads in a cached lexicon if available (WARNING, there is no
guarantee that it will correspond to the file(s) specified).
If there is no existing cached lexicon, or it is out of date,
reads in the specified files using {\tt read-tdl-lex-file-aux}.
See \S\ref{cache}.

\lispcommand{read-tdl-grammar-file-aux {\it file-name}}

Reads in a grammar file. There may be more than one such command
in a script.

\lispcommand{read-morph-file-aux {\it file-name}}

Reads in a lexical rule file with associated orthographemic information.
Note that the orthographemic system assumes there will only
be one such file in a grammar, so this command
may not occur more than once in a script, even 
though it takes a single file as argument.

\lispcommand{read-tdl-lex-rule-file-aux {\it file-name}}

Reads in a lexical rule file where the rules do not have
associated orthographemic information.
There may be more than one such command
in a script.

\lispcommand{load-irregular-spellings {\it file-name}}

Reads in a file of irregular forms.  It is assumed there is
only one such file in a grammar.

\lispcommand{read-tdl-parse-node-file-aux {\it file-name}}

Reads in a file containing entries which define parse nodes.

\lispcommand{read-tdl-psort-file-aux {\it file-name}}

Reads in a file containing any other sort of entry.  
This file simply defines the entries, without giving them
any particular functionality.
This command is used
in the example grammars for the
roots file, but note that this is possible only because root entry
identifiers
are enumerated in the globals file.

\subsection{Utility functions}

The following functions are defined as utilities for people
who want to write script files and don't know much
lisp  (the definitions are in the source file {\tt io-general/utils}).

\lispcommand{this-directory}

Returns the directory which contains the script file (only
usable inside the script file).

\lispcommand{parent-directory}

Returns the directory which is contains the directory containing 
the script file (only
usable inside the script file).

\lispcommand{lkb-pathname {\it directory} {\it name}}

Takes a directory as specified by the commands above, and combines it
with a file name to produce a valid full name for the other commands.

\lispcommand{lkb-load-lisp {\it directory} {\it name} 
\&optional {\it boolean}}

Constructs a file name as with {\tt lkb-pathname} and loads it
as a lisp file.  If optional is {\tt t} (i.e., true),
ignore the file if it is missing, otherwise signals a (continuable) error.

\section{Check paths}
\label{checkpaths}

The check paths mechanism greatly increases the efficiency of
parsing and generation with large grammars like the LinGO ERG.
It relies on the fact that unification failure during rule application
is normally caused by type incompatibility on one of a relatively
small set of paths.  These paths can be checked very efficiently 
before full unification is attempted, thus providing a filtering
mechanism which considerably improves performance.  The paths
are constructed automatically by batch parsing a representative
range of sentences.

To construct a set of check-paths, the macro {\tt
with-check-path-list-collection} is used.  It takes two arguments: the first is
a file to which the set of paths will be written, the second
is a call to a batch parsing function.  For example:
\begin{verbatim}
(with-check-path-list-collection "~aac/checkpaths.lsp"
  (parse-sentences "~aac/grammar/lkb/test-sentences" 
    "~aac/grammar/lkb/results"))
\end{verbatim}
The file of checkpaths created in this way
is then read in as part of the script.
For instance:
\begin{verbatim}
(lkb-load-lisp (this-directory) "checkpaths.lsp" t)
\end{verbatim}
To maintain filtering efficiency, the checkpaths should be
recomputed when there is a major change to the 
architecture of the feature structures used in the grammar.

%FIX - remove/change when we sort the following out
Note that there is currently a function which converts the 
output of the macro into a form suitable for the style of
rules used in the textbook and LinGO grammars, i.e.
with the daughters given by an {\sc ARGS} list.  This is
at the end of {\tt main/check-unif.lsp}.  It would
have to be rewritten for grammars that use a different
mother/daughter encoding.

\section{Multiword lexemes}
\label{multiword}

The LKB system allows lexical items to be specified which
have a value for their orthography which is a list of 
more than one string.
These items are treated as multiword lexemes and the 
parser checks that all the strings are present before
putting the lexical entry on the chart.

Multiword lexemes may have at most one affixation site.
This is specified on a per-entry basis, via the
user-definable function {\tt find-infl-pos} (see \S\ref{userfns}).
By default, this allows affixation on the rightmost element:
e.g. {\it ice creams}.  It can be defined in a more complex
way, to allow for e.g., {\it attorneys general}.

\section{Parse ordering}
\label{first-only}

FIX - left for Rob to instantiate 

\section{Character sets}
\label{accents}

Because the LKB is implemented in Common Lisp, its ability 
to handle characters
which are not part of the basic ASCII character set is determined by the
version of Common Lisp used.  The following notes are very preliminary --- we
would welcome more information.

\subsection{MCL}

Old versions of MCL supported 8-bit characters, newer versions support Apple's
double-byte character encoding.  On the basis of rather limited testing, this
means support for non-English alphabets is reasonable --- if you can enter the
characters on your keyboard (via the standard Apple sequences), they appear to
behave correctly in the LKB.  Note however that there will be portability 
issues when using files constructed on a Mac in other environments.

\subsection{ACL/CLIM on Unix}

The situation with
ACL on Unix is more complicated.  Standard ACL supports 8-bit 
characters, which
means that you can use the LKB for development for most European languages,
provided you have the necessary fonts etc installed.  Note that emacs has an
iso-accents-mode, which allows one to enter accented characters from a US
keyboard.  This allows one to enter lexical entries etc, but it doesn't help
when interacting with the LKB dialogues.  
Doing this requires that the keyboard and X keyboard mappings are
set up appropriately for the desired character set.
It is possible to avoid using dialogues
by using the alternative Lisp commands
instead: in particular, {\tt do-parse-tty} (see
\S\ref{parsecommands}).  Correct display depends on the fonts available in
the Xresources file: ACL 4.3 doesn't check to ensure the font
is using the given encoding, so e.g., bold font display (as used for types)
may be wrong when normal face font display is correct.  ACL 5.0
apparently fixes this problem.

To support double-byte encoding, needed for Japanese, it is necessary to have
International ACL.  This uses a version of EUC.  See the ACL documentation for
more information.  We have not tested the LKB in International ACL,
though in principle it should work.

\section{Cached lexicon}
\label{cache}

The cached lexicon functionality is provided as a temporary expedient to speed
up reloading a grammar when the lexicon has not changed.  Whenever a lexicon is
read into the LKB, it is stored in an external file (specified by the
parameter \verb+*psorts-temp-file*+, see \S\ref{sysglob}) 
rather than in memory.  
Lexical entries
are pulled in from this file as required.  The caching facility saves this file
and an associated index file (specified by \verb+*psorts-temp-index-file*+, 
see \S\ref{sysglob}) so
that they do not need to be recreated when the grammar is read in again if the
lexicon has not changed.  The script command {\tt read-cached-lex-if-available}
(see \S\ref{loadfns}), takes a file or list of files as arguments.  If a cached
lexicon is available in the locations specified by \verb+*psorts-temp-file*+
and \verb+*psorts-temp-index-file*+, and is more recent than the file(s) given
as arguments to the function, then it will be used instead of reading in the
specified files.  The code does not check to see whether the cached lexicon was
created from the specified files, and there are other ways in which the system
can be fooled.  Thus you should definitely not use this unless you have
a sufficiently large lexicon that the reload time is annoying.

The permanent solution to this problem is to use a proper database for 
lexical entries.

\section{Leaf types}
\label{leaftypes}

The notion of a leaf type is defined for efficiency.  A leaf type
is a type which is not required for the valid definition or expansion of
any other type or type constraint.  
Specifically this means that a leaf type
must meet the following criteria:
\begin{enumerate}
\item A leaf type has no daughters.
\item A leaf type may not introduce any new features on its constraint.
\item A leaf type may not be used in the constraint of another type.
\item A leaf type only has one `real' parent type --- it may have one or more
template parents (see \ref{thierglob}).
\end{enumerate}
Under these conditions, much more efficient type checking is possible,
and the type constraint can be expanded on demand rather than being expanded 
when the type file is read in.

In the LinGO ERG, most of the terminal lexical types (i.e. those from 
which lexical entries inherit directly) are leaf types, as are most of
the relation types.  The latter class is particularly important,
since it means that a lexicon can be of indefinite size and expanded on
demand, while still using distinct types to represent semantic relations.

Various checks are performed on leaf types to ensure they do meet
the above criteria.  However the tests cannot be completely comprehensive
if the efficiency benefits of leaf types are to be maintained.
If a type is treated as a leaf type when it does not fulfill the
criteria above, unexpected results will occur.  Thus
the leaf type facility has the potential for causing 
problems which are difficult to diagnose and it should therefore be
used with caution.


\section{Emacs interface}
\label{emacs}

FIX (though I'm not sure whether we need this section) - maybe Rob?

\section{YADU}
\label{defaults}

The structures used in the LKB system are not actually 
ordinary typed feature structures, but typed default feature
structures (TDFSs).  So far in this document we have assumed
non-default structures and monotonic unification, but this is actually
just a special case of typed default unification.
The full description of TDFSs and
default unification is
given in 
\href{http://www-csli.stanford.edu/~aac/papers/yadu.pdf}{Lascarides and
Copestake (in press)} (henceforth L+C).
The following notes are intended to supplement that paper.

Default information is introduced into the description
language by {\tt /}, followed by an indication of the persistence
of the default.  In terms of the current implementation,
the supported distinctions in persistence are between defaults
which are fully persistent and those which become non-default
when an entry feature structure is constructed (referred to as description
persistence).  The value of the
description persistence indicator is given by the global
{\tt *description-persistence*} --- the default is {\tt l}.

The modification to the BNF given for the TDL
syntax in \S\ref{bnftype} is as follows:
\begin{list}{}
   {\leftmargin 5em
    \itemindent -\leftmargin
    \itemsep 0pt plus 1pt
    \parsep 0pt plus 1pt}
\bnfitem{{\it Conjunction} $\rightarrow$ {\it DefTerm} $|$ 
                                      {\it DefTerm} \& {\it Conjunction}}
\bnfitem{{\it DefTerm} $\rightarrow$ {\it Term} $|$ {\it Term} 
/{\it identifier} {\it Term} $|$ /{\it identifier} {\it Term}}
\end{list}
It is not legal to have a default inside a default.

For example, the following definition of {\bf verb} makes
the features {\sc past}, {\sc pastp} and {\sc passp} indefeasibly
coindexed, and {\sc pastp} and {\sc passp} defeasibly coindexed.
The definition for {\bf regverb} makes the value of {\sc past} be
{\tt ed}, by default.
\begin{verbatim}
verb := *top* &
[ PAST *top* /l #pp,
  PASTP #p /l #pp,
  PASSP #p /l #pp ].

regverb := verb &
[ PAST /l "ed" ] .
\end{verbatim}
Any entry using these types will be completely indefeasible,
since the defaults have description persistence.
Further examples of the use of defaults, following the examples in
L+C, are given in the files in {\tt data/yadu\_test}.

You should note that the description language specifies
dual feature structures from which
BasicTDFSs are constructed
as defined in \S 3.5 of L+C.  See also the discussion of the encoding
of VALP in \S 4.2 of L+C.

If you view a feature structure which contains defaults,
you will see three parts.  The first is the indefeasible structure,
the second the `winning' defeasible structure, and the third 
the tail.  (Unfortunately this is very verbose: a more concise 
representation will be implemented at some point.)

\section{MRS}
\label{mrs}

FIX
% FIX - though this should maybe be just a pointer
% to the paper plus a distinct document describing
% munging, vitrification etc

\section{Generation}
\label{generator}

The generator requires a grammar which is using MRS or a relatively
similar formalism.  It will work in a somewhat rudimentary way on the
textbook grammar.  The following notes are VERY sketchy.
% FIX - do this properly ...
\begin{enumerate}
\item Instead of loading the lkb source, the mrs source must be loaded.  This
is done by replacing {\tt "lkb"} with {\tt "mrs"} as the argument to
{\tt load-system} and {\tt compile-system}.  The distributed image 
contains the appropriate code.  
\item The textbook grammar contains a file defining some necessary
global parameters which is loaded if the mrs code is available.
\item Before generating, the lexicon and lexical and grammar rules
must be indexed.  The function which does this, {\tt index-for-generator},
can be also be run with the {\bf Index} command
from the {\bf Generate} menu.
\item To generate, first parse a sentence.  
Then use the parse tree display to find the number associated with the topmost
edge in the tree for the analysis you are interested in
(or a lower node if you like, though the results may be
highly counterintuitive if you do this).  Then
click {\bf Generate \ldots} in the {\bf Generate} menu and give this
number as an argument.  (Warning: the default value supplied will NOT
always be what you want.)
\end{enumerate}

% FIX - whatever ...

\section{Testing and Diagnosis}
\label{tsdb}

The batch parsing support which is distributed with the
LKB is very simple.  For extensive grammar development,
more sophisticated tools
are available through the \itsdb\ package that was recently
integrated with the LKB.  The following description is by Stephan Oepen
(to whom all comments should be directed: {\tt oe@coli.uni-sb.de}).

\itsdb\ builds on the following components and modules
\begin{itemize}

\item test and reference data stored with annotations in a structured database;
annotations can range from minimal information (unique test item
identifier, item origin, length et al.) to fine-grained linguistic
classifications (e.g., regarding grammaticality and linguistic
phenomena presented in an item) as represented by the TSNLP test
suites (Oepen et al, 1997).

\item tools to browse the available data, identify suitable subsets and feed
them through the analysis component of a processing system like
the LKB system or PAGE;

\item the ability to gather a multitude of precise and fine-grained system
performance measures (like the number of readings obtained per test
item, various time and memory usage metrics, ambiguity and
non-determinism parameters, and salient properties of the result
structures) and store them as a {\em competence and performance
profile\/} into the database;

\item graphical facilities to inspect the resulting profiles, analyze
system competence (i.e., grammatical coverage and
overgeneration) and performance at highly variable
granularities, aggregate, correlate, and visualize the data,
and compare profiles obtained from previous grammar or system
versions or other platforms.

\item a universal pronounciation rule:\ {\em tee ess dee bee plus 
plus\/} is the name of the game.
\end{itemize}

While \itsdb\ is not yet available for public download, the LKB
distribution contains a bundled beta version of the complete
package.  See the draft user manual (`{\tt manual.ps}') in the
`{\tt src/tsdb/doc/}' subdirectory of the LKB source tree.
PostScript copies of Oepen et al, 1997 and Oepen and Flickinger, 1998, in
the same directory provide additional background reading.

\appendix
\chapter{The tty interface}
\label{tty}

\section{What is the tty mode?}

The LKB tty mode is provided to allow people to use the LKB system
even if they do not have access to a machine configuration which we
currently support,
as long as they have some version of Common Lisp.\footnote{We
believe that tty mode only uses generic Common Lisp commands (assuming
CLTL II) but we haven't tested it on an extensive range of Lisps.
If it doesn't work with your system, please let us know.}
tty mode is also useful if you are accessing a machine over a slow network
where the graphical display would take too long.
In this mode, commands are entered by typing rather than menu selection,
and display commands result in ASCII output.  Some commands
have no tty equivalent.

tty mode allows you to read in grammars, parse sentences
and generate sentences.  It is less convenient than the graphical
mode for grammar development and debugging because it is more difficult
to view feature structures, type hierarchies, charts and so on.
In this appendix, we describe the commands which are available in tty mode.
All commands are entered as Lisp
commands at the Lisp prompt.
For example:
\begin{verbatim}
USER(3): (read-script-file-aux "~aac/grammar/lkb/script")
\end{verbatim}
Here \verb+USER(3):+ is the system provided prompt (it may not look like
this on your system) and the user has typed the rest of the line
followed by the return or enter key.  \verb+read-script-file-aux+
is the LKB command, \verb+"~aac/grammar/lkb/script"+ is the argument to
the command (in this case a filename).
In what follows, we show the generic command and
its arguments in the style used in Steele.

\section{Loading the LKB in tty mode}

We do not provide a tty only image, so at least initially the tty
version of the LKB must be loaded from the source files.
The procedure is essentially the same as that described in \S\ref{acl-src}.
If you are using a version of Common Lisp which does not have supported
graphics (i.e., a version other than ACL with CLIM or MCL), the
tty version of the system will be loaded automatically.  If you
want a tty version for some reason in a platform where the graphics 
is supported, evaluate the following command before the load-system:
\begin{verbatim}
(pushnew :tty *features*)
\end{verbatim}
You will see a lot of warning messages about undefined functions 
when loading the system in tty mode, but these can be ignored.


\section{Loading grammars}

See also \S\ref{script-adv}.

\lispcommand{read-script-file-aux {\it filename}}

Loads the given file as a script file.
For example: 
\begin{verbatim}
(read-script-file-aux "~aac/grammar/lkb/script")
\end{verbatim}
Note that the file has to be expressed as a string
(i.e., starting and ending with {\tt "}).
For MCL, pathnames are expressed differently, for example:
\begin{verbatim}
(read-script-file-aux "Macintosh HD:grammar:lkb:script")
\end{verbatim}
See {\bf Load} / {\bf Complete grammar} (\S\ref{load})

\lispcommand{reload-script-file}

Reloads the previously loaded script file.
See {\bf Load} / {\bf Reload grammar} (\S\ref{load})

\lispcommand{read-type-patch-files}

Reloads the type constraints.
See {\bf Load} / {\bf Reload constraints} (\S\ref{load})

\lispcommand{reload-leaf-files}

Reloads the leaf type files.
See {\bf Load} / {\bf Reload leaf types} (\S\ref{load})

\lispcommand{reload-lex-files}
See {\bf Load} / {\bf Reload lexicon} (\S\ref{load})

\lispcommand{reload-grammar-rules}
See {\bf Load} / {\bf Reload grammar rules} (\S\ref{load})

\lispcommand{reload-lexical-rules}
See {\bf Load} / {\bf Reload lexical rules} (\S\ref{load})
As with the menu command, this also loads morphological rules.

\lispcommand{reload-template-files}
See {\bf Load} / {\bf Reload tree nodes} (\S\ref{load})

\lispcommand{reload-psort-files}
See {\bf Load} / {\bf Reload other entries} (\S\ref{load})


\section{View commands}

All the view commands print out an ASCII version of the 
feature structures etc in a way analogous to the
graphical display described in the main body of the manual.

\lispcommand{show-type-spec-tty {\it type}}

Displays a type's parents 
and the specification of its constraint (if any).
For example,
\begin{verbatim}
(show-type-spec-tty 'synsem-struc)
\end{verbatim}
Note that the type name has to be preceded by a quote.
See {\bf View} / {\bf Type definition} (\S\ref{view}).

The following is an example of the output.
This shows the feature structure followed by the
parents.
\begin{verbatim}
 [SYNSEM-STRUC
  ORTH:  (*DIFF-LIST*)
  SYN:   (GRAM-CAT)
  SEM:   (SEM-STRUC)
  KEY-ARG:  (BOOLNUM)]
(FEAT-STRUC)
\end{verbatim}

\lispcommand{show-type-tty {\it type}}

Displays the expanded type constraint.
For example,
\begin{verbatim}
(show-type-tty 'synsem-struc)
\end{verbatim}
See {\bf View} / {\bf Expanded type} (\S\ref{view})

\lispcommand{show-lex-tty {\it lexidentifier}}

Displays the expanded lexical entry corresponding to the identifier.
For example,
\begin{verbatim}
(show-lex-tty 'Kim_1)
\end{verbatim}
See {\bf View} / {\bf Lex entry} (\S\ref{view})

\lispcommand{show-words-tty {\it string}}

Displays the expanded lexical entries corresponding to the 
orthography given by the string.
For example,
\begin{verbatim}
(show-words-tty "Kim")
\end{verbatim}
The argument has to be a Lisp string but the command is
not case-sensitive.
See {\bf View} / {\bf Word entries} (\S\ref{view})

\lispcommand{show-grammar-rule-tty {\it gruleidentifier}}

Displays the grammar rule corresponding to the identifier.
For example,
\begin{verbatim}
(show-grammar-rule-tty 'hd_spec_rule)
\end{verbatim}
See {\bf View} / {\bf Grammar rule} (\S\ref{view})

\lispcommand{show-lex-rule-tty {\it lruleidentifier}}

Displays the lexical or
morphological rule corresponding to the identifier.
For example,
\begin{verbatim}
(show-lex-rule-tty 'plur_noun)
\end{verbatim}
See {\bf View} / {\bf Lexical rule} (\S\ref{view})

\section{Parsing}

\lispcommand{do-parse-tty {\it string}}

Takes a string and runs the parser, returning a bracketed 
structure or structures corresponding to the parses.
For example, with the textbook grammar:
\begin{verbatim}
(do-parse-tty "Kim sleeps")
Edge 9 P:
("S" ("NP" ("NP" ("NP" ("Kim" "Kim"))))
 ("VP" ("VP" ("sleeps" 3RD-SING-VERB_INFL_RULE))))
\end{verbatim}
Note that the edge number for the parse is given so that it
can be identified in the ASCII chart (see below) or used
as input to generation.

See {\bf Parse} / {\bf Parse input} (\S\ref{parsecommands})

\lispcommand{show-parse}
Displays the results of the parse again.
See {\bf Parse} / {\bf Show parse} (\S\ref{parsecommands})

\lispcommand{print-chart}

This shows the results of the parse in an ASCII format.
Each line represents an edge:
the first field shows the vertices that the
edge spans (starting at 0).  The second field
gives the edge number: the edge numbers given for the
parse results will correspond with one of the edge numbers
shown in the chart.  The third field shows the type of the
feature structure corresponding to the edge.
The fourth field shows the words in the input string that the
edge covers, and the final field shows the daughter edges.
For example:
\begin{verbatim}
 > chart dump:

0-1 [1] WORD => (Kim)  []

0-2 [4] PHRASE => (Kim dies)  [1 2]
1-2 [2] WORD => (dies)  []
\end{verbatim}

See \S\ref{parsecommands}.

\lispcommand{parse-sentences {\it input-file} {\it output-file}}

Similar to the menu command {\bf Parse} / {\bf Batch parse}
 (\S\ref{parsecommands}) but requires an input file of test sentences
and an output file for the results.  For example:
\begin{verbatim}
(parse-sentences "~aac/grammar/lkb/test-sentences" 
    "~aac/grammar/lkb/results")
\end{verbatim}

\section{Lexical rules}
\lispcommand{apply-lex-tty {\it lexidentifier} {\it lruleidentifier}}

Similar to {\bf Apply lexical rule} (see \S\ref{activefs}),
but requires a lexical identifier and a rule identifier to be specified.
For example:
\begin{verbatim}
(apply-lex-tty 'sleep_1 '3RD-SING-VERB_INFL_RULE)
\end{verbatim}
A feature structure is printed corresponding to the result 
of the rule application (if it succeeds).

\lispcommand{apply-lex-rules-tty {\it lexidentifier} }

Similar to {\bf Apply all lex rules} (see \S\ref{activefs}),
but requires a lexical identifier to be specified.
For example:
\begin{verbatim}
(apply-lex-rules-tty 'sleep_1)
\end{verbatim}
All the resulting structures will be printed.  Note - this can be
tediously long.

\section{Generation}

\lispcommand{do-generate-tty \&optional {\it edge-number}}

\lispcommand{show-gen-result}

\lispcommand{print-gen-chart}

\section{Miscellaneous}

\lispcommand{clear-non-parents}



\chapter{Path syntax}
\label{pathbnf}

Earlier versions of the LKB used a syntax for descriptions
which was based on the path value syntax used in PATR.
This syntax is still supported, although the extensions to it
described in Copestake (1993) are currently not.
The BNFs for the path syntax are given here.

\section{Type descriptions}

The following BNF description gives the syntax of the type specification 
language using the path syntax:
\begin{list}{}
   {\leftmargin 5em
    \itemindent -\leftmargin
    \itemsep 0pt plus 1pt
    \parsep 0pt plus 1pt}
\bnfitem{Type specification -> \\
typename Parents [Comment][Constraint].}
\bnfitem{Comment -> " string "}
\bnfitem{Parents -> ( typename$^*$ )}
\bnfitem{Constraint -> Path\_spec\_list | Enumeration}
\bnfitem{Path\_spec\_list -> Path\_spec$^*$}
\bnfitem{Path\_spec -> EPath = Typevalue | Path = Path}
\bnfitem{Typevalue -> typename}
\bnfitem{Epath -> Path | <>}
\bnfitem{Path -> < Type\_F\_pair\_list >}
\bnfitem{Type\_F\_pair\_list -> Type\_Feature\_pair \\
| Type\_Feature\_pair : Type\_F\_pair\_list}
\bnfitem{Type\_Feature\_pair -> [typename] feature}
\bnfitem{Enumeration -> ( OR typename$^+$ )}
\end{list}
Enumeration is syntactic sugar for enumerated atomic types.  For example:
\begin{quote}
\tt
{\bf boolean} ({\bf top}) (OR {\bf true} {\bf false}).
\end{quote}
expands out into the equivalent of:
\begin{quote}
\bf
{\bf boolean} ({\bf top}).\\
{\bf true} ({\bf boolean}).\\
{\bf false} ({\bf boolean}).
\end{quote}

\section{Entry descriptions}

The syntactic description for lexical entries is as follows:
\begin{list}{}
   {\leftmargin 5em
    \itemindent -\leftmargin
    \itemsep 0pt plus 1pt
    \parsep 0pt plus 1pt}
\bnfitem{Lexentry -> LexID PPath\_spec$^+$ .}
\bnfitem{LexID -> orth sense-id}
\bnfitem{PPath\_spec -> Path\_spec | typename}
\end{list}
{\tt orth}, {\tt sense-id},
{\tt typename} and {\tt feature} are 
terminal symbols.

Grammar and morphological rule files 
and template files are identical to this, 
except that the identifier is a single element.
\begin{list}{}
   {\leftmargin 5em
    \itemindent -\leftmargin
    \itemsep 0pt plus 1pt
    \parsep 0pt plus 1pt}
\bnfitem{Rule/template -> id PPath\_spec$^+$ .}
\end{list}
where {\tt id} is a terminal symbol.

Morphological rule files may have one or more letter sets at the 
beginning of the file:
\begin{list}{}
   {\leftmargin 5em
    \itemindent -\leftmargin
    \itemsep 0pt plus 1pt
    \parsep 0pt plus 1pt}
\bnfitem{Letterset -> \%(letter-set (Macro letter$^+$))}
\bnfitem{Macro -> !letter}
\end{list}
where {\tt letter} is a terminal symbol corresponding to any character.

The rules themselves have the following form:
\begin{list}{}
   {\leftmargin 5em
    \itemindent -\leftmargin
    \itemsep 0pt plus 1pt
    \parsep 0pt plus 1pt}
\bnfitem{Mrule -> id morph-rule Mgraph\_spec$^+$ Path\_spec$^+$ .}
\bnfitem{Mgraph\_spec -> \%prefix | suffix SPair$^+$}
\bnfitem{SPair -> ( Mstring Char+ )}
\bnfitem{Mstring -> * | Char+}
\bnfitem{Char -> letter | Macro}
\end{list}

All files may have comments in between elements introduced by {\tt ;}.


\chapter{Details of system parameters}
\label{glob}

Various parameters control the operation of the LKB and some feature and
type names are regarded as special in various ways.
The system provides default settings
but typically, each grammar will have its own files which set these
parameters, redefining some of the default settings, though
obviously a set of closely related grammars can share parameter
files.  Each script must load the grammar-specific parameter files
before loading any of the other grammar files.
Other parameters control aspects of the system which
are not grammar specific, but which concern things such as 
font size, which are more a matter of an individual user's preferences.
This class of parameters can be set via the {\bf Options}
menu (described in \S\ref{display}).
Changing parameters in the {\bf Options} menu results in
a file being automatically generated with the preferences:
this shouldn't be manually edited, since any changes may be 
overridden.  There's nothing to prevent global variables which affect
user preferences being specified in the grammar-specific globals
files, but it is better to avoid doing this, since it could be confusing.

There are also some functions which can be (re)defined
by the grammar writer to control some aspects of system behavior.

This appendix describes all the parameters and functions
that the LKB system allows the
user/grammar writer to set, including both grammar specific and
user preference parameters.
We sometimes refer to the parameters as {\it globals}, since they
correspond to Common Lisp global variables. 
Note that the parameters in the grammar-specific globals files are 
all specified as
\begin{verbatim}
(defparameter global-variable value comment)
\end{verbatim}
where the comment is optional.
Thus only a very basic knowledge of Common Lisp is required to
edit a globals file: the main thing to remember is that symbols
have to be preceded by a quote, for instance:
\begin{verbatim}
(defparameter *string-type* 'string)
\end{verbatim}

The descriptions below gives the
default values for the variables and functions
(as they are set in the source file
{\tt main/globals} and {\tt main/userfns}). 
The description of the global variables is divided into sections
based on the function of the variables: these 
distinctions do not correspond to any difference in the implementation
with the exception that the globals which can be set via the
{\bf Options} menu are all ones that are specified as being
grammar independent.  (Not all grammar independent variables
are available under {\bf Options}, since some are complex to set
interactively, or rarely used.)

\section{Grammar independent global variables}
\label{indglob}

\subsection{System behaviour}
\label{sysbeh}
\begin{description}
\item[*lkb-system-version*, :page]
This parameter controls the behaviour of the system with respect to
such things as the expected syntax (TDL or path style).  The default
setting is intended to make the system as compatible as possible with
the PAGE system.  Currently this has the following main effects:
\begin{enumerate}
\item TDL syntax assumed by default for grammar files
\item Tree nodes labels are constructed according to the dual
label/meta system (see \S\ref{treenodes}) rather than by simple
matching with category feature structures.
\end{enumerate}
For behaviour which is more similar to earlier versions of the LKB,
this variable can be set to :lkb4.
\item[*gc-before-reload*, nil] This boolean parameter controls
whether or not 
a full garbage collection is triggered before a grammar is reloaded.
It is best set to t for large grammars, to avoid image size increasing,
but it is nil by default, since it slows down reloading.
\item[*sense-unif-fn*, nil]  If set, this must correspond to
a function.
See make-sense-unifications in \S\ref{userfns},
below.
\item[*maximum-number-of-edges*, 500] A limit on the number of edges that can
be created in a chart, to avoid runaway grammars taking over multi-user
machines.  This must be increased to at least 2000 for large scale grammars
with a lot of ambiguity.
\item[*chart-limit*, 100] The limit on the number of words in a sentence
which can be parsed.  Whether the system can actually parse sentences of
this length depends on the grammar \ldots
\item[*maximal-lex-rule-applications*, 7] 
The number of lexical rule applications which may be made
before it is assumed that some rules are applying circularly and the system
signals an error.
\item[*display-type-hierarchy-on-load*, t] A boolean variable, which
controls whether the type hierarchy is displayed on loading or not.
This must be set to nil for grammars with large numbers
of types, because the type hierarchy display becomes too slow (and if the
type hierarchy involves much multiple inheritance, the results are not
very readable anyway). 
\end{description}


\subsection{Display parameters}
\label{disglob}
\begin{description}
\item[*feature-ordering*, nil]
A list which is interpreted as 
a partial order of features for fixing or resolving the default 
display ordering when feature structures are displayed or printed
(see \S\ref{types}).
\item[ *dont-show-morphology*, nil]
A boolean variable.  If set, the morphological 
derivations are not shown in parse trees (see \S\ref{parsetree}).
\item[ *dont-show-lex-rules*, nil]
A boolean variable. If set, 
applications of lexical rules are not shown in parse trees (see \S\ref{parsetree}).
\item[ *substantive-roots-p*, nil]
A boolean variable which can
be set to allow the root edges to be regarded as real edges
for the purposes of chart display (see \S\ref{root}).
\item[*display-type-hierarchy-on-load*, t] see \S\ref{sysbeh} above.
\item[ *parse-tree-font-size*, 12] The font size for parse tree nodes.
\item[ *fs-type-font-size*, 12] The font size for nodes in AVM windows.
\item[ *fs-title-font-size*, 12] The font size for titles in AVM windows.
\item[ *type-tree-font-size*, 12] The font size for the nodes in the type hierarchy.
\item[ *dialog-font-size*, 12] The font size used in dialogue windows.
\end{description}


\subsection{Parse tree node labels}
\label{ptreeglob}
These parameters are only used when in PAGE compatibility mode.  For details
of how this operates, see \S\ref{treenodes}.
\begin{description}
\item[*label-path*, (LABEL-NAME)]
The path where the name string is stored.
\item[*prefix-path*, (META-PREFIX)]
The path for the meta prefix symbol.
\item[ *suffix-path*, (META-SUFFIX)]
The path for the meta suffix symbol.
\item[ *recursive-path* (NON-LOCAL SLASH LIST FIRST)]
The path for the recursive category.
\item[*local-path*, (LOCAL)]
The path inside the node to be unified with the recursive node.
\item[ *label-fs-path*, (SYNSEM)]
The path inside the node to be unified with the label node.
\item[*label-template-type*, label]
\end{description}

\subsection{Languages}
\label{langglob}
Earlier versions of the LKB offered support for development of
multilingual lexicons, but the core version currently
has very limited capabilities.  The following
parameters simply allow lexicons of different languages to be distinguished.
\begin{description}
\item[*current-language*, English]
This parameter specifies the default language.
\item[*possible-languages*, nil]
This specifies the possible languages for interactions
where a language has to be selected or specified.  If nil,
it is assumed that the language is always the default.
\end{description}

\subsection{Defaults}
\label{defglob}
See \S\ref{defaults}.
\begin{description}
\item[ *description-persistence*, l]
The symbol used to indicate that a default should be
made hard (if possible) when an entry is expanded into a complete
feature structure.
\end{description}

\section{Grammar specific parameters}
\label{gramglob}
\subsection{Type hierarchy}
\label{thierglob}
\begin{description}
\item [*templates*, nil] A list of types which may be treated as 
templates, where by template in this context we mean simply a named
feature structure used to abbreviate a description of another structure
(they are not parameterizable).
This is used in the LinGO ERG to reduce multiple inheritance
and to allow the bottommost lexical types to be treated as leaf types
(\S\ref{leaftypes}).  This facility has only been included to allow
compatibility with an existing grammar, in a context where
multiple type inheritance was being used for implementation-specific 
purposes rather than for any linguistically motivated reason.
It should not be used when developing a new grammar.
\item [*toptype*, top] This gives *toptype* a value before a type file
is read in, which is necessary for the TDL syntax or in the path syntax
if the definition of $\top$ does not
appear before all type descriptions which contain constraint specifications.
\item [*string-type*, string] The name of the type which is special, in that
all Lisp strings are recognised as valid subtypes of it.
\end{description}
\subsection{Orthography and lists}
\label{olglob}
\begin{description}
\item [*orth-path*, (orth lst)]  The path into a sign, specified 
as a list of features, which leads to the orthographic specification.
\item[*list-head*, (hd)]  The path for the first element of a list.
\item[*list-tail*, (tl)] The path for the rest of a list.
\item[*list-type*, *list*] The type of a list.
\item[*empty-list-type*, *null*] The type of an empty list --- 
it must be a subtype of {\bf *list-type*}.
\item[*diff-list-type*, *diff-list*]  The type of a difference list (see
\S\ref{listdesc}).
\item[*diff-list-list*, list] The feature for the list portion
of a difference list.
\item[*diff-list-last* last] The feature for the last element of a difference list.
\end{description}

\subsection{Morphology and parsing}
\label{mpglob}
\begin{description}
\item[*lex-rule-suffix*, nil]  If set, this is appended to
the string associated with an irregular form in the irregs file in
order to construct the appropriate inflectional rule name. 
Required for PAGE compatibility in the LinGO ERG.
\item[*mother-feature*, 0] The feature specifying the mother in a rule
\item[*head-marking-path*, nil] A feature path --- 
a head daughter in a rule may be identified by having the same 
value for this path as the mother.  If this path is set, parsing may be
(slightly) more efficient, since the parser will then check for the
unifiability of a rule with its
head daughter before checking the other daughters.
\item[*start-symbol*, sign] A type which specifies the type of any valid parse.
Can be used to allow relatively complex root conditions.
\item[*deleted-daughter-features*, nil]
A list of features which will not be passed from daughter to mother
when parsing.  This must be set if efficiency is a consideration in order
to avoid copying parts of the feature structure that can never be
(directly) referenced from rules pertaining to higher nodes in the
parse tree.  This will include daughter features in HPSG, since it is
never the case that a structure can be directly selected on the basis of
its daughters.
\item[*check-paths*, nil]
An alist in which the keys are feature paths that often fail ---
these are checked first before attempting unification to improve efficiency.
See \S\ref{checkpaths}.
The value of this parameter could be set in the globals file, but since the
list of paths is automatically generated, it is normally kept in a distinct
file.  The parameter should ideally 
be set to {\tt nil} in the globals file for
grammars which do not use check paths, in case the grammar is read in after
a previous grammar which does set the paths.
\item[*irregular-forms-only-p*, nil]
If set, the parser will not invoke the morphological analyzer on a
form which has an irregular spelling.  This prevents the system
analyzing words such as {\it mouses}.  Note that this means
that if the grammar writer wants to be able to analyze {\it dreamed} as
well as {\it dreamt}, both entries have to be in the irregular morphology
file.  Also note that we currently assume (incorrectly) that spelling is
not affected by sense. For instance, the system cannot handle
the usage where the plural
of {\it mouse} is {\it mouses} when referring to computers.
Note that this flag does not affect generation, which
always treats an irregular form as blocking a regular spelling.
\item[*first-only-p*] If set, only one parse will be returned,
where any preferences must be defined as specified
in \S\ref{first-only}. 
\end{description}


\section{User definable functions}
\label{userfns}

\begin{description}
\item [make-sense-unifications] If this
function is defined, it takes 
three arguments so that the orthographic form of a lexical
entry, its id and the language are recorded in an appropriate place in
the feature structure.  The value of *sense-unif-fn* must be
set to this function, if it is defined.
The function is not defined by default.

The idea is that this function can be used to specify paths 
(such as \verb+< ORTH : LST : HD >+) which will have their values set to
the orthographic form of a lexical entry.\footnote{Note that
this description is in the alternative path notation defined
in \S\ref{pathbnf}.}
This allows the lexicon files to be shorter.
It is assumed to be exactly equivalent to specifying that the paths take
particular atomic values in the lexical entry.
For example, instead of writing:
\begin{verbatim}
teacher 1 
<> = common-noun
< ORTH : LST : HD > = "teacher"
< SEM : KEY > = teacher1_rel .
\end{verbatim}
the function could be defined so it was only necessary to write:
\begin{verbatim}
teacher 1 
<> = common-noun.
\end{verbatim}
since the value of the orthography string and the semantic relation name are
predictable from the identifying material.


\item[make-orth-tdfs] A function used by the parser
which takes a string and returns a 
feature structure which corresponds to the orthographic part of a sign
corresponding to that string.  The default version assumes that the string may 
have spaces, and that the feature structure will contain a list
representation with each element corresponding to one word (i.e. sequence of
characters separated by a space).  For instance, the string 
\verb+"ice cream"+ would give rise to the structure
\begin{verbatim}
[ ORTH [ LST [ HD ice
               TL [ HD cream ]]]]
\end{verbatim}

\item [establish-linear-precedence] A function which takes a rule 
feature structure and returns the top level features in a list
structures so that the ordering corresponds to mother, daughter 1,
daughter 2 ... daughter n.  The default version assumes that the daughters
of a rule are indicated by the features 1 through n.

\item [spelling-change-rule-p] 
A function which takes a rule structure and checks whether the rule has an
effect on spelling. It
is used to prevent the parser 
trying to apply a rule which affects spelling and
which ought therefore only be applied by the morphology
The current value of this
function checks for \verb+< NEEDS-AFFIX > = true+.
If this matches a rule, the parser will not attempt to apply this rule.
(Note that the function will return false if the value of
NEEDS-AFFIX is anything other than {\bf true}, since the
test is equality, not unifiability.)
See \S\ref{morphology}.
\item [redundancy-rule-p]  Takes a rule as input and checks whether
it is a redundancy rule, defined as one which is only used in
descriptions and is not intended to be applied productively.
For instance, the prefix {\it step-}, as in {\it stepfather}, 
has a regular meaning, but only applies to a small, fixed set of words.
A redundancy rule for {\it step-} prefixation can be specified to capture the
regularity and avoid redundancy, but it can only be used in the
lexical descriptions of {\it stepfather} etc, and not applied productively.
(There is nothing to prevent productive lexical rules being used in
descriptions.)

The default value of this function checks for \verb+< PRODUCTIVE > = false+.
If this matches a rule, the parser will not attempt to apply that rule.
(Note that the function will return false if the value of
PRODUCTIVE is anything other than {\bf false}, since the
test is equality, not unifiability.)

\item[preprocess-sentence-string]  The function
takes an input string and preprocesses it for the parser.  
The result of this function should be a single string which
is passed to a simple word identifier
which splits the string into words (defined as things
with a space between them).
So minimally this function could be simply return its input string.
However, by default some more complex processing is carried
out here, in order to
strip punctuation and separate {\it 's}.

\item[find-infl-poss] This function is called when a lexical
item is read in, but only for lexical items which have more than one
item in their orth value (i.e., multiword lexemes).  
It
must be defined to take three arguments: a set of unifications
(in the internal data structures), an orthography value (a list of strings)
and a sense identifier.  It should return an integer, indicating 
which element in a multi-word lexeme may be inflected
(counting left to right, leftmost is 1) or {\tt nil},
which indicates that no item may be inflected.
The default value for the function allows inflection on the rightmost element.
See \S\ref{multiword}.

\item[hide-in-type-hierarchy-p] Can be defined so that certain types
are not shown when a hierarchy is displayed (useful for hierarchies
where there are a very large number of similar leaf types, for instance
representing semantic relations).

\item[rule-priority] 

FIX - Rob's

\end{description}

\subsection{System files}
\label{sysglob}

There are two user definable functions which control two system files.
The file names are associated with two global variables --- these are
initally set to {\bf nil} and are then instantiated by the functions.  The
global variables are described below, but should not be changed
by the user.  The functions which instantiate them may need to be changed
for different systems.

\begin{description}
\item[lkb-tmp-dir] This function attempts to find a sensible directory for
the temporary files needed by the LKB.  The default value for this
on Unix is a directory {\tt tmp} in the user's home directory: on a Macintosh
it is {\tt Macintosh HD:tmp}.  The function should be redefined as necessary
to give a valid path.  It is currently only called by 
{\tt set-temporary-lexicon-filenames} (below).

\item[set-temporary-lexicon-filenames] This function is called in order to
set the temporary files.  It uses lkb-tmp-dir, as defined above.
It is useful to change the file names in this function if one is
working with multiple grammars and using cacheing or dumping images to
ensure that the lexicon file associated with a grammar has a 
unique name which avoids overriding another lexicon
(see \S\ref{cache}).
\end{description}

\begin{description}
\item[*psorts-temp-file*, {\tt $\sim$/tmp/templex}]
This file is constructed by the system and used to store the unexpanded lexical
entries, in order to save memory.  Once a lexical entry is used, it will
be cached until either a new lexicon is read in, or until the {\bf Tidy up}
command is used (\S\ref{tidy}).  If the temporary
lexicon file is deleted or modified while the LKB is running, it will not
be possible to correctly access lexical entries.  The file is retained
after the LKB is exited so that it may be reused if the lexicon has not been 
modified (see \S\ref{cache}, \S\ref{loadfns} and the description of 
\verb+*psorts-temp-index-file*+, below).

The pathname is actually specified as:
\begin{verbatim}
  (make-pathname :name "templex" 
                 :directory (lkb-tmp-dir))
\end{verbatim}

\item[*psorts-temp-index-file*]
This file is used to store an index for the temporary lexicon file.
If the option is taken to read in a cached lexicon (see \S\ref{cache}
and \S\ref{loadfns}),
then the lexicon index is reconstructed from this file.
If this file has been deleted, or is apparently outdated, the lexicon
will be reconstructed from the source file.
\end{description}


\chapter*{References}

% first argument - i.e. #1 is always author
% #2 is title
% last is date
% agreed author format is, for example
% Alshawi, H., D. Carter, M. Rayner and B. Gamb\"ack
% 
% the only formatting commands that should appear in fields
% are those to mark idiosyncratic information like accents
\newcommand{\book}[4]{\item #1 (#4) {\it #2,} #3.}
% #3 is publisher
\newcommand{\unpub}[4]{\item #1 (#4) `#2', #3.}
% for tech reports etc
% #3 is information about institution etc
\newcommand{\bookart}[7]{\item #1 (#7) `#2' in #5 (ed.), {\it #4,} #6, pp.~#3.}
% published book article (old \barticle)
% #3 is page numbers 
% #4 book title
% #5 editors
% #6 publishers
\newcommand{\bookartnoed}[6]{\item #1 (#6) `#2' in {\it #4,} #5, pp.~#3.}
% book article where editors are unknown
% #3 is page numbers 
% #4 book title
% #5 publishers
\newcommand{\bookartnopp}[6]{\item #1 (#6) `#2' in #4 (ed.), {\it
#3,} #5.}
% book article where page numbers are unknown
% #3 book title
% #4 editors
% #5 publishers
\newcommand{\forthart}[6]{\item #1 (#6, forthcoming) `#2' in #4 (ed.), {\it #3,} #5.}
% article in forthcoming book (old \farticle}
% #3 book title
% #4 editors
% #5 publishers
\newcommand{\journart}[6]{\item #1 (#6) `#2', {\it #3,} {\bf #4},
#5.}
% article in journal (old \jarticle)
% #3 name of journal
% #4 volume
% #5 pages
\newcommand{\journartnopp}[5]{\item #1 (#5) `#2', {\it #3,} 
{\bf #4}.}
% article in journal no page numbers
% #3 name of journal
% #4 volume
\newcommand{\forthjournart}[4]{\item #1 (#4) `#2', {\it #3.}}
% article in journal no yet published
% #3 name of journal
\newcommand{\procart}[6]{\item #1 (#6) `#2', {\it Proceedings of the
#3,} #4, pp.~#5.}
% article in proceedings (old \particle)
% #3 name of conference
% #4 venue
% #5 pages
\newcommand{\pubprocart}[7]{\item #1 (#7) `#2', {\it Proceedings of the
#3,} #4, ,#5, pp.~#6.}
% #3 name of conference
% #4 venue
% #5 publishers
% #6 pages
\newcommand{\procartnopp}[5]{\item #1 (#5) `#2', {\it Proceedings of the
#3,} #4.}
% article in proceedings without known page numbers
% #3 name of conference
% #4 venue
\newcommand{\pubprocartnopp}[6]{\item #1 (#6) `#2', {\it Proceedings of the
#3,} #4, #5.}
% article in proceedings without known page numbers
% #3 name of conference
% #4 venue
% #5 publishers
\newcommand{\procartnoppnoplace}[4]{\item #1 (#4) `#2', {\it Proceedings of the
#3}.}
% article in proceedings without known page numbers without place
% where meeting happened
% #3 name of conference
\newcommand{\procartnoplace}[5]{\item #1 (#5) `#2', {\it Proceedings of the
#3,} pp.~#4.}
% article in proceedings where the venue is not known
% #3 name of conference
% #4 pages
%\newcommand{\acqwp}{ESPRIT BRA-3030 ACQUILEX WP NO.}
%\newcommand{\aunpub}[5]{\item #1 (#4) {\it #2.}\\ \acqwp #3}
%\begin{list}{}
%   {\leftmargin 1.6em
%    \itemindent -\leftmargin
%    \itemsep 0pt plus 1pt
%    \parsep 0pt plus 1pt}
%
% Put refs here
% 
%\end{list}


\begin{list}{}
   {\leftmargin 1.6em
    \itemindent -\leftmargin
    \itemsep 0pt plus 1pt
    \parsep 0pt plus 1pt}

\book{Aho, A. V.,  J.E.~Hopcroft and J.D.~Ullman}
{Data Structures and Algorithms}
{Addison Wesley, Reading, MA}
{1982}

\book{Briscoe, E.J., A.~Copestake and V.~de~Paiva}
{Inheritance, defaults and the lexicon}
{Cambridge University Press}
{1993}

\book{Carpenter, R.}
{The logic of typed feature structures}
{(Tracts in Theoretical Computer Science), Cambridge University Press,
  Cambridge, England} 
{1992}

%FIX carpenter 1993 in book

\procart{Copestake, A.}
{The ACQUILEX LKB: representation issues in semi-automatic
acquisition of large lexicons}
{3rd Conference on Applied Natural Language Processing (ANLP-92)}
{Trento, Italy}
{88--96}
{1992}

\unpub{Copestake, A.}
{The Compleat LKB}
{Technical report 316,
University of Cambridge Computer Laboratory}
{1993}

%FIX MRS semantics (Copestake et al, 1997)

\procart{Kasper, R.~T. and  W.~C. Rounds}
{A logical semantics for feature structures}
{24th Annual Conference of the Association for Computational
Linguistics (ACL-86)}
{Columbia University}
{235--242}
{1986}

\journart{Kasper, R.~T. and W.~C. Rounds}
{The logic of unification in grammar}
{Linguistics and Philosophy}
{13 (1)}
{35--58}
{1990}

\forthjournart{Lascarides, A. and A.~Copestake}
{Default representation in constraint-based frameworks}
{Computational Linguistics}
{in press}

\procart{Moens, M., J.~Calder, E.~Klein, M.~Reape, and H.~Zeevat}
  {Expressing generalizations in unification-based grammar formalisms}
  {4th Conference of the European Chapter of the Association 
for Computational Linguistics (EACL-89)}
  {Manchester, England}
  {66--71}
  {1989}

\bookart{Oepen, Stephan, Klaus Netter and Judith Klein}
{{\sc tsnlp} --- {T}est {S}uites for {N}atural {L}anguage {P}rocessing}
{13 -- 36}
{Linguistic Databases}
{John Nerbonne}
{CSLI Lecture Notes 77, CSLI Publications, Stanford CA}
{1997}

\journart{Oepen, Stephan and Daniel P. Flickinger}
{Towards Systematic Grammar Profiling.
               {T}est Suite Technology Ten Years After}
{Journal of Computer Speech and Language:
Special Issue on Evaluation}
{12 (4)}
{411-437}
{1998}

\procart{Pereira, F. C.~N. and S.~M. Shieber}
{The semantics of grammar formalisms seen as computer languages}
{10th International Conference on Computational Linguistics
(COLING-84)}
{Stanford, California}
{123--129}
{1984}

\book{Pollard,  C. and I.A.~Sag}
 {An information-based approach to syntax and semantics: 
Volume~1 fundamentals} 
 {CSLI Lecture Notes 13, CSLI Publications, Stanford CA}
 {1987}

\book{Pollard, C. and I.A.~Sag}
    {Head-driven phrase structure grammar}
    {Chicago University Press, Chicago}
    {1994}

\book{Sag, I.A. and T.~Wasow}
     {Syntactic Theory --- a formal introduction}
     {CSLI Publications, Stanford CA (also distributed by 
Cambridge University Press)}
     {1999}
    
\book{Shieber, S.~M.}
{An introduction to unification-based approaches to grammar}
{CSLI Lecture Notes 4, Stanford CA}
{1986}

\procartnopp{Uszkoreit, Hans, Rolf Backofen, Stephan Busemann,
              Abdel Kader Diagne,  Elizabeth A. Hinkelman,
               Walter Kasper, Bernd Kiefer, Hans-Ulrich Krieger, 
               Klaus Netter, G{\"u}nter Neumann, Stephan Oepen and
              Stephen P. Spackman}
{{DISCO} --- An {HPSG}-based {NLP} System and its Application for 
Appointment Scheduling}
{15th International Conference on Computational
Linguistics (COLING-94)}
{Kyoto, Japan}
{1994}

\end{list}



\end{document}

